{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54bf33e90f474048ad2e931024fb8805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a234a76201741419ce5ad5b8b6e2167",
              "IPY_MODEL_ecb4491804c944759a027bf6760723a2",
              "IPY_MODEL_f1155a36c27a4ba3a7a3e8919034fd04"
            ],
            "layout": "IPY_MODEL_1d32985b0d5e40299bdd31b13c48aa6a"
          }
        },
        "7a234a76201741419ce5ad5b8b6e2167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd385d4764cd4ffaa3bd4c43fd9749c7",
            "placeholder": "​",
            "style": "IPY_MODEL_d6f093d12590487faa41525de9e57c44",
            "value": ""
          }
        },
        "ecb4491804c944759a027bf6760723a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e4dc09ec4140dfab630c304c814d58",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5675ade3cc564a4facc7387614f480ce",
            "value": 1
          }
        },
        "f1155a36c27a4ba3a7a3e8919034fd04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e20fc304438646b6979f37cc1fab386a",
            "placeholder": "​",
            "style": "IPY_MODEL_83413b41ea534693af392dac84b8e38f",
            "value": " 60/? [04:48&lt;00:00,  5.17s/it]"
          }
        },
        "1d32985b0d5e40299bdd31b13c48aa6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd385d4764cd4ffaa3bd4c43fd9749c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6f093d12590487faa41525de9e57c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55e4dc09ec4140dfab630c304c814d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5675ade3cc564a4facc7387614f480ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e20fc304438646b6979f37cc1fab386a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83413b41ea534693af392dac84b8e38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install openai\n",
        "!pip install tqdm\n",
        "!pip install sounddevice\n",
        "!pip install whisper\n",
        "!pip install gtts\n",
        "!pip install beautifulsoup4\n",
        "!pip install tavily-python\n",
        "!pip install lxml\n",
        "!pip install pyttsx3\n",
        "!pip install langchain_openai\n",
        "!pip install langchain-community\n",
        "!pip install langchain-google-genai\n",
        "!pip install langsmith\n",
        "!pip install google-search-results\n",
        "\n",
        "!pip install lxml\n",
        "!pip install httpx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nZBqgN5unCwA",
        "outputId": "53c4412f-4803-4de9-e453-6eae55a9218a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m140.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=eedb854715437a44882b72b0cca6dac3faa39dbc7f35e286e5a6fac127ac505f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.36.0\n",
            "    Uninstalling opentelemetry-api-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.36.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.57b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.57b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.36.0\n",
            "    Uninstalling opentelemetry-sdk-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.36.0\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.1.0 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting sounddevice\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice) (2.23)\n",
            "Downloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice\n",
            "Successfully installed sounddevice-0.5.2\n",
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from whisper) (1.17.0)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=50a2fb6b1204eb2578c7209dfcb42f726a81209ef26ddaa65139f3108fd73aba\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/b8/4e/9c4c3351d670e06746a340fb4b7d854c76517eec225e5b32b1\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper\n",
            "Successfully installed whisper-1.1.10\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from gtts) (2.32.4)\n",
            "Collecting click<8.2,>=7.1 (from gtts)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (2025.8.3)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: click, gtts\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "Successfully installed click-8.1.8 gtts-2.5.4\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.7.12-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily-python) (2.32.4)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.11.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2025.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->tavily-python) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->tavily-python) (4.15.0)\n",
            "Downloading tavily_python-0.7.12-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: tavily-python\n",
            "Successfully installed tavily-python-0.7.12\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Collecting pyttsx3\n",
            "  Downloading pyttsx3-2.99-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading pyttsx3-2.99-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pyttsx3\n",
            "Successfully installed pyttsx3-2.99\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.76 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.107.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.27)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.75\n",
            "    Uninstalling langchain-core-0.3.75:\n",
            "      Successfully uninstalled langchain-core-0.3.75\n",
            "Successfully installed langchain-core-0.3.76 langchain_openai-0.3.33\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.76)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.27)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.29 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.76)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Collecting filetype<2,>=1.2 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (0.4.27)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.7.0 langchain-google-genai-2.1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4862c0bbfb504c40a81716b911df9707"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.27)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.3)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from google-search-results) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->google-search-results) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->google-search-results) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->google-search-results) (2025.8.3)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=6962e1d69cbc5cbe48c40128882f5cfa74fc1c3e3dd75cd2f451450d35c43c56\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/47/f5/89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y5YSKSqMJA-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420546aa-65e2-4fc3-8c30-c605f919e157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STEP - 1 : Imports\n",
        "# ============================================\n",
        "\n",
        "# Standard Library Imports\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Third-Party Library Imports\n",
        "import chromadb\n",
        "import openai\n",
        "import tqdm\n",
        "import pyttsx3\n",
        "import gtts\n",
        "from google.colab import drive\n",
        "from bs4 import BeautifulSoup\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# LangChain and related framework imports\n",
        "import langchain\n",
        "from langchain import hub\n",
        "from langchain.tools import Tool\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser # Corrected import path\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# STEP - 2 : Load API keys (Colab + Env)\n",
        "# =========================================\n",
        "import os\n",
        "\n",
        "# Attempt to fetch from Colab userdata if available\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    _colab_available = True\n",
        "except ImportError:\n",
        "    _colab_available = False\n",
        "\n",
        "def get_key(key_name: str) -> str:\n",
        "    \"\"\"Fetch API key from Colab userdata first, then environment variables.\"\"\"\n",
        "    key = None\n",
        "    if _colab_available:\n",
        "        key = userdata.get(key_name)\n",
        "    if not key:\n",
        "        key = os.environ.get(key_name)\n",
        "    if not key:\n",
        "        print(f\"❌ {key_name} not found! Please add it to Colab secrets or environment variables.\")\n",
        "    else:\n",
        "        print(f\"✅ {key_name} loaded successfully!\")\n",
        "    return key\n",
        "\n",
        "# Fetch all keys\n",
        "OPENAI_API_KEY = get_key(\"OPENAI_API_KEY\")\n",
        "TAVILY_API_KEY = get_key(\"TAVILY_API_KEY\")\n",
        "SERPAPI_API_KEY = get_key(\"SERPAPI_API_KEY\")\n",
        "LANGCHAIN_API_KEY = get_key(\"LANGCHAIN_API_KEY\")\n",
        "# LANGSMITH_API_KEY = get_key(\"LANGSMITH_API_KEY\")\n",
        "# HF_TOKEN = get_key(\"HF_TOKEN\")\n",
        "# Optional: Pinecone if you use it\n",
        "# PINECONE_API_KEY = get_key(\"PINECONE_API_KEY\")\n",
        "\n",
        "# Set OpenAI key for SDKs\n",
        "if OPENAI_API_KEY:\n",
        "    import openai\n",
        "    openai.api_key = OPENAI_API_KEY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gZdqyv_wMWc",
        "outputId": "f13f8b86-2523-45e9-b33a-8ff507221816"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OPENAI_API_KEY loaded successfully!\n",
            "✅ TAVILY_API_KEY loaded successfully!\n",
            "✅ SERPAPI_API_KEY loaded successfully!\n",
            "✅ LANGCHAIN_API_KEY loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# ---------- MOUNT THE GOOGLE DRIVE -------------\n",
        "# ================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(f\"✅ Google Drive mounted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdkadBgywvdD",
        "outputId": "56c097e7-f482-416c-da5b-72a483840c54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP - 3 : Importing all embedded chunks\n",
        "# ============================================\n",
        "\n",
        "all_embedded_file_path = \"/content/drive/MyDrive/Ironhack_final_project/all_embedded_chunks.json\"\n",
        "\n",
        "with open(all_embedded_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    all_embedded_chunks = json.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(all_embedded_chunks)} chunks\")\n",
        "print(all_embedded_chunks[0])  # preview first chunk"
      ],
      "metadata": {
        "id": "1CbpzVP9Jl2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8830998-6890-4784-9c91-a182fc022af2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 164 chunks\n",
            "{'video_id': 'tQ84XYcP-nA', 'chunk_index': 0, 'text': \"Every week there's a new AI tool making headlines, and right now there are more AI video generators than ever. But most of them don't work as well as you'd expect. Some generate great videos, but only if you stay within their style limits. If you try to get more creative, like detailed anime or wild fantasy worlds, they often mess up, and most of the time they're too expensive or just confusing to use. I've tested every major tool that's come out recently, and I've seen where they shine and where they completely fall apart. So, in this video, I'm going to show you the AI video tools that actually deliver the kind of quality you'd want to publish. The best way to use them without wasting hours learning clunky software, and the one platform that ties everything together, so you can create full videos without switching between five different sites. Let's break it down. All right, let's start with Seedance 1.0. Seance is by danceance's flagship AI video generator, the same company behind Tik Tok. And it's currently ranked number one on the leading video AI benchmark. And unlike most tools that glitch out when you get specific, Cance was built for precision and it actually follows your complex prompts. If you've tried video generators before and felt limited, this one changes the game. To use it, I'm heading into the text to video tab and selecting seed dance in the drop- down menu. By the way, if you want to know how I ax all of the AI models in just one tool, it's because I use open art. I'll leave a link to it down below so that you can follow along. Seedance supports both image and text generation. But for now, we're starting fresh with a prompt. Now, I'll type in my prompt. For example, a lone cyberpunk courier speeds through a neon lit city on a futuristic motorcycle. Rain falls in slow motion, bouncing off chrome surfaces and glowing street sign. The camera tracks from behind as the bike swerves between flying cars and under flickering holograms. Midway, the courier skids around a corner and removes their helmet, revealing a determined expression. In the distance, a tower explodes in a pulse of electric blue light. Moody, high contrast lighting, reflective puddles, 24 fps, cinematic motion blur. As you can see, I try to be as in-depth as I possibly can. This is very important for all models we are going to cover today. The more specific you are in your prompt, the better the result will come out. But I get that for a beginner, this can be challenging. So, I've come up with a simple solution to help you out. To create a very in-depth prompt, I head to chat GPT and paste in this prompt. Create a super in-depth prompt for an AI generated video using Seance Model. The prompt idea is a lone courier speeding through a neon lit city on\", 'embedding': [0.019356219097971916, 0.017160408198833466, -0.040577374398708344, -0.016167782247066498, 0.0028857612051069736, -0.057211387902498245, 0.0223641786724329, 0.02416895516216755, 0.044938914477825165, 0.05561717227101326, 0.036606866866350174, -0.06274603307247162, -0.02989911660552025, -0.0685814768075943, -0.004587138071656227, -0.020800039172172546, -0.05206777900457382, -0.03946442902088165, 0.006655110511928797, -0.0006570511613972485, -0.0026338445022702217, 0.011054251343011856, -0.03029015101492405, 0.04478851705789566, 0.04045705497264862, -0.04223174974322319, 0.006621270906180143, 0.06918306648731232, 0.01625801995396614, 0.00567376334220171, -0.0016148982103914022, -0.024695347994565964, -0.004222423303872347, -0.053992871195077896, -0.006839348003268242, 0.008971239440143108, 0.0020322524942457676, 0.0011890840250998735, -0.00865540374070406, 0.004162264056503773, 0.0003628350968938321, 0.00685438746586442, 0.0577528215944767, -0.006549831945449114, 0.001814175513572991, 0.022574735805392265, 0.013799013569951057, -0.01735592633485794, -0.01916070096194744, 0.022619854658842087, -0.0765826478600502, 0.09059973806142807, -0.021115874871611595, 0.04617217928171158, 0.0008036891813389957, -0.01917574182152748, -0.04214151203632355, 0.007211582735180855, -0.019265979528427124, -0.004647297319024801, 0.05023292452096939, -0.0011778040789067745, -0.0009860467398539186, 0.034140340983867645, 0.008610283955931664, -0.010430099442601204, -0.01949157752096653, 0.010430099442601204, 0.010212022811174393, -0.015017237514257431, 0.02601884864270687, 0.0022484497167170048, 0.00993378646671772, -0.004587138071656227, -0.07718423753976822, -0.015716588124632835, 0.003011719323694706, 0.02564285509288311, -0.005963279400020838, -0.031162459403276443, 0.02418399415910244, -0.0032767958473414183, -0.028981689363718033, -0.011129450052976608, -0.05513589829206467, -0.018634308129549026, -0.03218516707420349, -0.03744909539818764, -0.05549685284495354, -0.015348113141953945, -0.011264808475971222, 0.03781004995107651, -0.004760095849633217, 0.046713609248399734, 0.05014268308877945, 0.05525621399283409, -0.006944626569747925, 0.0013197421794757247, 0.056339081376791, 0.03465169295668602, 0.05600820481777191, -0.010933932848274708, -0.01630314067006111, 0.014212608337402344, 0.03549392148852348, 0.008918600156903267, -0.011648323386907578, -0.03260628134012222, -0.038110844790935516, 0.022529616951942444, -0.09553278982639313, -0.01693481206893921, -0.038471803069114685, 0.03140309825539589, 0.017476243898272514, -0.02057444304227829, 0.026530202478170395, 0.03507280722260475, 0.014693882316350937, 0.010046584531664848, -0.044968992471694946, 0.0375092551112175, 0.0030060794670134783, 0.02594364993274212, 0.017927438020706177, -0.04187079519033432, -0.00383326830342412, -0.03787020966410637, -0.0017897358629852533, -0.02200322411954403, 0.011994238011538982, 0.020033009350299835, 0.01989765092730522, -0.007256702054291964, -0.020108209922909737, -0.02640988491475582, -0.005959519650787115, 0.009978905320167542, 0.009256995283067226, 0.029177207499742508, -0.02522174082696438, -0.031282778829336166, 0.0042036231607198715, 0.006452072877436876, -0.053601838648319244, -0.04120904579758644, -0.027928903698921204, -0.011798720806837082, -0.024394551292061806, 0.02529693953692913, -0.018950143828988075, -0.025131501257419586, -0.04578114300966263, -0.0024195273872464895, 0.0012360833352431655, 0.008775721304118633, -0.044999074190855026, 0.030214952304959297, -0.03759949281811714, -0.015039796940982342, -0.0003167757240589708, -0.022123541682958603, -0.02455998957157135, -0.012159676291048527, -0.0605803020298481, -0.004248742945492268, -0.029342643916606903, 0.012783827260136604, -0.05522613599896431, 0.02961336076259613, -0.00076796964276582, -0.02594364993274212, -0.0022258900571614504, -0.0367271862924099, 0.04322437569499016, -0.03335827216506004, 0.001816055504605174, -0.0324859619140625, -0.010933932848274708, -0.015205235220491886, -0.013926852494478226, 0.05026300251483917, -0.0076514966785907745, 0.005884320475161076, 0.006617510691285133, -0.0040419455617666245, 0.012016798369586468, -0.0066964696161448956, -0.0024590068496763706, -0.011340007185935974, -0.0332680307328701, 0.018513990566134453, 0.031132379546761513, 0.017521364614367485, 0.04433732107281685, -0.0324859619140625, 0.07652249187231064, -0.021567068994045258, 0.07772567123174667, -0.03967498615384102, 0.07170975208282471, 0.033809464424848557, -0.015761706978082657, -0.023928318172693253, 0.02422911301255226, 0.01550603099167347, -0.00014205557818058878, -0.013806533999741077, -0.030741345137357712, 0.013468138873577118, 0.05167674273252487, -0.00833956804126501, 0.002842521760612726, -0.02016836777329445, -0.052849847823381424, -0.02125123329460621, -0.014144929125905037, 0.033689144998788834, -0.03110230155289173, 0.040908247232437134, -0.03176405280828476, -0.020393965765833855, 0.014295327477157116, -0.02520669996738434, 0.011362566612660885, -0.012498071417212486, 0.020348845049738884, -0.047796476632356644, 0.00847492553293705, 0.011159529909491539, 0.02021348848938942, 0.02567293308675289, -0.008023731410503387, -0.040306657552719116, 0.036997903138399124, -0.02822970040142536, 0.016363300383090973, -0.014716441743075848, 0.02666556090116501, 0.005083451513200998, 0.02636476419866085, -0.012513111345469952, 0.046803850680589676, 0.003970506601035595, 0.01737096533179283, 0.010783534497022629, -0.034862250089645386, -0.01499467808753252, -0.01352077815681696, 0.037388935685157776, 0.07904917746782303, -0.008076370693743229, 0.015430832281708717, -0.00029938595253042877, 0.00958787091076374, -0.0641898512840271, 0.00550832599401474, 0.014656282030045986, -0.026785878464579582, 0.0023819277994334698, 0.020709801465272903, 0.006467112805694342, -0.03862220048904419, 0.006527272053062916, 0.005899360403418541, 0.02055940218269825, -0.04827775061130524, -0.020679721608757973, 0.014859319664537907, 0.001661897636950016, 0.008083891123533249, 0.0237779188901186, 0.04322437569499016, -0.0015086797066032887, -0.029026808217167854, -0.028274819254875183, 0.03540368378162384, 0.0015866985777392983, 0.007474779151380062, 0.04364548996090889, -0.029523121193051338, -0.020078130066394806, -0.0026206846814602613, 0.02055940218269825, -0.010264662094414234, 0.0034159140195697546, -0.012227355502545834, -0.02344704419374466, 0.012941746041178703, -0.06912291049957275, 0.024981103837490082, -0.044547878205776215, -0.04806719347834587, -0.03290707617998123, 0.0072416625916957855, -0.01914566196501255, -0.0009254175238311291, 0.05820401385426521, -0.021100835874676704, 0.035884957760572433, 0.008602764457464218, -0.10082679986953735, 0.014347966760396957, 0.010708335787057877, 0.01080609392374754, 0.03711821883916855, -0.05017276480793953, 0.013588456436991692, -0.009941305965185165, -0.01191903930157423, 0.027943942695856094, 0.08229777216911316, -0.0009169576223939657, -0.018649348989129066, -0.01729576662182808, -0.016047462821006775, 0.06003887206315994, -0.02779354527592659, 0.029071928933262825, -0.07080736756324768, -0.026244446635246277, -0.014859319664537907, -0.007501098792999983, -0.00938483327627182, -0.05733170732855797, -0.027206992730498314, 0.01550603099167347, 0.00995634589344263, 0.01550603099167347, -0.028741052374243736, 0.011791201308369637, 0.004839054774492979, -0.010888813063502312, 0.012866546399891376, -0.0006885407492518425, -0.05889584496617317, -0.026470042765140533, -0.019296059384942055, 0.041720397770404816, 0.009851067326962948, -0.06196396425366402, -0.0237779188901186, 0.010715855285525322, 0.03254612162709236, -0.023928318172693253, 0.012513111345469952, -0.010234582237899303, -0.034140340983867645, 0.029132086783647537, 0.005151130724698305, -0.041359443217515945, -0.01009922381490469, -0.00012748577864840627, -0.03639630973339081, 0.03633615002036095, 0.022529616951942444, -0.04659329354763031, 0.008835881017148495, -0.0411188043653965, -0.039945702999830246, -0.016694175079464912, 0.019311100244522095, 0.018905024975538254, 0.014355486258864403, -0.005497045814990997, -0.004756336100399494, -0.03456145524978638, 0.027252113446593285, 0.004722496494650841, 0.0014898799126967788, -0.009490111842751503, 0.005805361550301313, 0.00722286244854331, 0.0006523511838167906, -0.015460911206901073, -0.010196982882916927, -0.009497632272541523, 0.017446164041757584, -0.013971971347928047, 0.04018633812665939, -0.00044719898141920567, -0.03224532678723335, -0.012099516578018665, -0.027507789433002472, 0.003891547443345189, 0.03600527346134186, 0.02665052004158497, -0.00307187857106328, -0.06346794217824936, 0.0169498510658741, 0.022890571504831314, -0.10437619686126709, -0.03832140192389488, 0.051857221871614456, 0.0032203965820372105, 0.02892152965068817, 0.0007886493694968522, 0.011452805250883102, 0.011467845179140568, 0.04869886487722397, 0.034832172095775604, 0.016092583537101746, -0.037358857691287994, 0.005102251190692186, 0.009249475784599781, 0.01979237236082554, 0.010174422524869442, 0.014603642746806145, 0.028680892661213875, 0.019070463255047798, 0.05778289958834648, -0.01044513937085867, 0.05486518144607544, -0.018619269132614136, 0.0009470372460782528, 0.016904732212424278, 0.04138952121138573, 0.030440550297498703, 0.029447922483086586, 0.04683392867445946, -0.007572537753731012, -0.09836027771234512, -0.020664680749177933, -0.03332819044589996, -0.03137301653623581, 0.034832172095775604, 0.0271468348801136, -0.0012097636936232448, 0.011227209120988846, -0.05333112180233002, -0.01206943765282631, 0.00958787091076374, 0.051797062158584595, -0.02525181882083416, -0.01767176203429699, 0.0030267592519521713, 0.007403340190649033, 0.001071585575118661, 0.030500708147883415, -0.005527125671505928, -0.026154207065701485, 0.02057444304227829, -0.024289272725582123, 0.007098784204572439, -0.01622794196009636, 0.004906733985990286, -0.02528189867734909, -0.0016515577444806695, -0.02162722870707512, -0.043465014547109604, -0.04373573139309883, -0.04298374056816101, -0.022168660536408424, 0.04355525225400925, -0.025357097387313843, -0.041720397770404816, 0.057241469621658325, -0.028951609507203102, 0.011467845179140568, 0.006008399184793234, 0.008489965461194515, 0.015731627121567726, -0.03465169295668602, 0.042382147163152695, 0.042412228882312775, 0.046382736414670944, -0.039163630455732346, 0.03027511201798916, 0.003359514754265547, 0.026184286922216415, -0.01373885478824377, -0.015445872209966183, 0.006064798217266798, 0.0028707212768495083, -0.020845159888267517, -0.017175449058413506, 0.06250539422035217, 0.029688559472560883, -0.007993652485311031, 0.0011073050554841757, -0.03033527173101902, -0.037659652531147, 0.02240929752588272, 0.07586073875427246, -0.014723961241543293, 0.025477416813373566, -0.011971678584814072, -0.04187079519033432, 0.01801767759025097, 0.004072024952620268, 0.007903413847088814, 0.00901635829359293, 0.0677994042634964, -0.018243273720145226, 0.023958398029208183, -0.02242433838546276, -0.023672640323638916, 0.003083158517256379, -0.0016421579057350755, 0.01407724991440773, 0.004944333340972662, -0.027688266709446907, -0.004229942802339792, 0.05158650502562523, -0.012986864894628525, 0.010031544603407383, 0.07134880125522614, 0.040246497839689255, -0.011445285752415657, 0.020288687199354172, 0.002966600004583597, -0.011881439946591854, -0.011302407830953598, 0.05348151922225952, -0.0019589336588978767, 0.022589774802327156, -0.019747253507375717, 0.03398994356393814, 0.007042385172098875, -0.020694760605692863, -0.021221153438091278, 0.007192783057689667, -0.00024275173200294375, -0.014475804753601551, -0.008016211912035942, -0.006888227071613073, -0.029673520475625992, -0.006997265852987766, -0.03242580220103264, -0.04337477684020996, 0.05528629571199417, 0.00550456577911973, 0.01497211866080761, 0.0033933543600142, -0.07441692054271698, -0.013099663890898228, -0.00015639039338566363, 0.023281605914235115, -0.05994863063097, 0.016754334792494774, -0.000250976620009169, 0.028710972517728806, 0.04232199117541313, 0.022304018959403038, -0.018153036013245583, 0.029793838039040565, -0.003594511654227972, 0.0019185141427442431, 0.005361687857657671, -0.019596856087446213, 0.008963719010353088, -0.05011260509490967, -0.004406660795211792, -0.02128131315112114, -0.016017384827136993, 0.031222619116306305, 0.006294155027717352, -0.03420050069689751, 0.0004963133251294494, -0.017521364614367485, 0.022484496235847473, 0.00729806162416935, 0.009001318365335464, 0.014235167764127254, -0.016137702390551567, 0.035945113748311996, -0.012152155861258507, 0.02819962054491043, -0.009723229333758354, 0.014611163176596165, -0.023612482473254204, -0.02782362513244152, 0.0270716343075037, -0.023236487060785294, 0.011475365608930588, 0.0006951206596568227, 0.02129635214805603, 0.01657385751605034, -0.02449982985854149, 0.033719226717948914, -0.009888666681945324, -0.03224532678723335, -0.01588202640414238, 0.01982245221734047, -0.018183114007115364, 0.0155361108481884, -0.0069671859964728355, 0.024770546704530716, 0.009129157289862633, 0.007591337896883488, -0.005809121765196323, -0.008933639153838158, -0.0331176333129406, 0.00865540374070406, -0.027297232300043106, 0.006275355350226164, 0.00021972204558551311, -0.029342643916606903, -0.010046584531664848, -0.02853049524128437, 0.0035606720484793186, 0.003974266350269318, -0.017882319167256355, -0.020484203472733498, -0.011836320161819458, -0.0011618243297562003, -0.009610430337488651, 0.011761121451854706, 0.023296646773815155, -0.008911079727113247, -0.002492846455425024, 0.007828214205801487, 0.020694760605692863, -0.05387255549430847, 0.010783534497022629, 0.013287660665810108, -0.03317779302597046, -0.013438059017062187, 0.04773631691932678, -0.012949265539646149, 0.00903891772031784, -0.00658367108553648, 0.01583690568804741, 0.00994882546365261, 0.03149333596229553, -0.00847492553293705, -0.00648967269808054, -0.03495248779654503, -0.05982831493020058, 0.0135734174400568, -0.006989745888859034, 0.001125164795666933, 0.027282191440463066, 0.01049025822430849, -0.01846887171268463, -0.009911226108670235, -0.021085795015096664, -0.001424080808646977, 0.0013676815433427691, -0.03417041897773743, 0.0124379126355052, 0.02304096892476082, -0.003630231134593487, -0.04391620680689812, -0.013656135648488998, -0.011911519803106785, -0.033809464424848557, 0.024815665557980537, -0.0038614680524915457, -0.04000585898756981, -0.017536403611302376, -0.009963865391910076, 0.05062395706772804, 0.003985546063631773, 0.02159714885056019, -0.014859319664537907, -0.008858440443873405, -0.029763758182525635, -0.00668518990278244, -0.003795668948441744, -0.014934518374502659, 0.03787020966410637, -0.02594364993274212, -0.021567068994045258, -0.022965770214796066, 0.012941746041178703, 0.006918306928128004, 0.02640988491475582, -0.012595830485224724, -0.015009718015789986, -0.035554081201553345, 0.0073469411581754684, -0.02159714885056019, 0.019070463255047798, -0.011152009479701519, -0.003534352406859398, 0.02890649065375328, -0.03931403160095215, 0.0012379633262753487, -0.019250940531492233, -0.027161873877048492, 0.01370877493172884, -0.03510288521647453, 0.018574150279164314, 0.035223204642534256, 0.007260462269186974, 0.03567440062761307, 0.029523121193051338, 0.027297232300043106, -0.0390733927488327, -0.017220567911863327, 0.0020510524045675993, -0.00976082868874073, 0.030560867860913277, 0.027898823842406273, 0.004417940508574247, -0.009610430337488651, 0.013031984679400921, -0.014671321958303452, -0.07778583467006683, -0.04042697697877884, 0.04078793153166771, -0.015686508268117905, -0.013423019088804722, 0.002667684108018875, -0.027282191440463066, -0.010061624459922314, 0.04361541196703911, 0.029447922483086586, -0.009888666681945324, -0.03510288521647453, -0.0022747693583369255, -0.0382913239300251, 0.00567752355709672, -0.0338997021317482, 0.006155036855489016, 0.03242580220103264, -0.016348259523510933, -0.01228751428425312, -0.047405440360307693, -0.03239572420716286, 0.010317301377654076, -0.03176405280828476, -0.04163016006350517, -0.02243937738239765, -0.06024942919611931, -0.0012398433173075318, 0.018258314579725266, -0.027507789433002472, 0.03167381137609482, -0.0061211977154016495, 0.002942160237580538, 0.003989306278526783, -0.005820401478558779, -0.009640510194003582, -0.014400606043636799, -0.0425325483083725, 0.01334029994904995, -0.03386962413787842, -0.04078793153166771, 0.004305141977965832, 0.0029008009005337954, 0.0023499683011323214, 0.007956053130328655, 0.0165588166564703, -0.025372138246893883, 0.016378339380025864, 0.04617217928171158, -0.01351325772702694, -0.037749890238046646, -0.002513526240363717, 0.016017384827136993, -0.025823332369327545, 0.012964305467903614, 0.012723668478429317, -0.0011815640609711409, 0.006726549472659826, -0.027989063411951065, 0.009174276143312454, 0.025417257100343704, -0.0009249475551769137, 0.017130328342318535, 0.0005776222096756101, 0.04698432609438896, -0.010242101736366749, -0.01043761894106865, 0.05381239578127861, 0.019957810640335083, -0.02961336076259613, -0.009813467971980572, -0.014911958947777748, 0.007895893417298794, -0.038411643356084824, -0.016423458233475685, 0.04484867677092552, -0.018243273720145226, 0.007895893417298794, -0.0011054250644519925, 0.009535231627523899, 0.010851213708519936, -0.013242541812360287, 0.006309194955974817, -0.0285605750977993, 0.030500708147883415, 0.022890571504831314, -0.016453538089990616, -0.016062503680586815, 0.0009916865965351462, -0.024680307134985924, -0.03793036937713623, -0.028004102408885956, -0.027868743985891342, 0.017897358164191246, 0.006448313128203154, 0.008971239440143108, 0.0022935690358281136, -0.013558377511799335, -0.09029894322156906, -0.029748719185590744, 0.012648469768464565, -0.0037618293426930904, -0.028756093233823776, -0.023341765627264977, 0.01138512697070837, 0.01009922381490469, -0.0198525320738554, -0.008971239440143108, 0.039915621280670166, -0.010941452346742153, 0.012505591847002506, -0.002992919646203518, 0.025522535666823387, -0.045329950749874115, -0.0003701200184877962, -0.04686400666832924, 0.014708921313285828, -0.026830999180674553, -0.008978758938610554, 0.014167489483952522, -0.016062503680586815, 0.007625177036970854, -0.009121636860072613, -0.02164226770401001, -0.014483325183391571, 0.01628809981048107, -0.012911666184663773, -0.017130328342318535, -0.01585194654762745, -0.01083617378026247, -0.005790322087705135, 0.012708628550171852, 0.013295181095600128, 0.0018310953164473176, -0.023386884480714798, -0.012317594140768051, 0.005350407678633928, -0.034140340983867645, 0.01876966655254364, -0.012107037007808685, 0.03693774342536926, -0.03209492564201355, -0.011249768547713757, 0.009685629047453403, 0.025417257100343704, -0.009302115067839622, -0.001627118093892932, 0.0035381123889237642, -0.00046599871711805463, 0.019025344401597977, 0.01807783544063568, -0.016513697803020477, -0.011543044820427895, 0.03221524506807327, -0.036245912313461304, 0.0118889594450593, 0.0055609652772545815, 0.044547878205776215, -0.012340153567492962, -0.0058504813350737095, 0.03573455661535263, -0.01621290110051632, -0.012362712994217873, -0.0007120404043234885, -0.004143464379012585, 0.020078130066394806, -0.01731080748140812, -0.02677083946764469, -0.0008882880210876465, 0.01388925313949585, 0.0331176333129406, 0.037659652531147, 0.002387567888945341, -0.02422911301255226, 0.028259778395295143, 0.017175449058413506, 0.023928318172693253, -0.006707749795168638, 0.0270716343075037, -0.03853195905685425, -0.002953440183773637, 0.029718639329075813, -0.0360955148935318, 0.026439962908625603, -0.026244446635246277, -0.037388935685157776, -0.012580790556967258, 0.03759949281811714, 0.039163630455732346, -0.03859211876988411, -0.015596269629895687, -0.029312564060091972, -0.03937418758869171, -0.039163630455732346, -0.015054836869239807, 0.02920728549361229, 0.04021641984581947, 0.010505298152565956, 0.06719781458377838, 0.019311100244522095, -0.01842375099658966, 0.008617803454399109, 0.01368621550500393, 0.013302700594067574, -0.025718053802847862, 0.00901635829359293, 0.008407246321439743, 0.011136969551444054, 0.002472166670486331, -0.040998488664627075, 0.016498656943440437, 0.012859026901423931, 0.006527272053062916, 0.008377167396247387, 0.037358857691287994, -0.012708628550171852, 0.007331901229918003, -0.001351701794192195, 0.026109088212251663, -0.02776346541941166, -0.026876118034124374, -0.02564285509288311, -0.00695590628311038, 0.042773183435201645, -0.004244982730597258, -0.011054251343011856, -0.029793838039040565, -0.03865227848291397, -0.009813467971980572, 0.010054104961454868, 0.032666441053152084, -0.02928248606622219, 0.017431125044822693, -0.02892152965068817, -0.006324234884232283, -0.012452952563762665, -0.02743259072303772, -0.02989911660552025, -0.0035982714034616947, 0.004993212874978781, -0.007753015495836735, 0.008723082020878792, 0.0015274793840944767, -0.03032023087143898, 0.01043761894106865, -0.005647443700581789, -0.013159822672605515, 0.017130328342318535, -0.019762294366955757, 0.007275501731783152, 0.00035508020664565265, 0.030771424993872643, 0.002126251347362995, 0.013844133354723454, -0.018544070422649384, -0.03549392148852348, -0.006831828039139509, -0.023191368207335472, 0.05011260509490967, 0.005688803270459175, 0.023973437026143074, 0.0346817709505558, 0.03435089811682701, -0.026981396600604057, 0.006339274346828461, -0.005951999686658382, -0.030891744419932365, 0.0005184030160307884, -0.03104214183986187, 8.865255222190171e-05, 0.04575106501579285, -0.0006020618602633476, 0.0324859619140625, 0.02634972520172596, 0.023537281900644302, -0.011866400018334389, 0.00882084108889103, 0.04731520265340805, -0.007151423487812281, 0.02269505336880684, -0.036636944860219955, 0.02344704419374466, -0.0026996436063200235, -0.02052932418882847, -0.00811397098004818, 0.02159714885056019, 0.014611163176596165, -0.03287699818611145, 0.01279886718839407, -0.027628106996417046, -0.0016017383895814419, -0.01624298095703125, 0.005215049721300602, 0.007956053130328655, -0.006873187143355608, -0.013656135648488998, -0.027206992730498314, -0.016453538089990616, -0.03203476965427399, 0.007151423487812281, 0.0077605354599654675, 0.008896039798855782, -0.009881147183477879, 0.052458811551332474, -0.00958787091076374, -0.036997903138399124, -0.010753454640507698, -0.011663362383842468, -0.014167489483952522, -0.019341180101037025, -0.029387764632701874, 0.003906587138772011, -0.010354900732636452, 0.025086382403969765, 0.01873958669602871, 0.035223204642534256, -0.026830999180674553, -0.033689144998788834, -0.00811397098004818, -0.021491870284080505, -0.013731335289776325, -0.0024590068496763706, -0.01624298095703125, 0.022093461826443672, 0.0029402803629636765, 0.012099516578018665, 0.01103921141475439, 0.004538259003311396, -0.020363885909318924, -0.03940426930785179, 0.019732214510440826, 0.004316421691328287, 0.0027974022086709738, 0.0017652962123975158, -0.019205821678042412, 0.020469164475798607, 0.01625801995396614, 0.0017981957644224167, 0.01622794196009636, 0.025823332369327545, 0.011565604247152805, -0.022258900105953217, -0.01428028754889965, 0.02955320104956627, 0.026830999180674553, -0.00594823993742466, 0.035915035754442215, 0.008880999870598316, -0.03423057869076729, 0.02099555730819702, 0.05170682445168495, -0.003786268876865506, 0.030064554885029793, -0.02021348848938942, 0.02316128835082054, 0.035614240914583206, 0.016167782247066498, -0.012828947044909, -0.004929293412715197, 0.0023650082293897867, 0.002966600004583597, -0.025071341544389725, -0.0027071635704487562, -0.01950661651790142, -0.01370877493172884, 0.013987011276185513, 0.00867044273763895, 0.0036095513496547937, 0.017130328342318535, 0.013039504177868366, 0.014949558302760124, 0.02892152965068817, -0.005760242231190205, -0.0006218015914782882, -0.03260628134012222, 0.006703989580273628, 0.023657601326704025, -0.03964490443468094, 0.01046769879758358, 0.009068997576832771, 0.004357781261205673, -0.035163044929504395, 0.03215508535504341, -0.009542751125991344, -0.01952165737748146, 0.007689096499234438, 0.029869036749005318, -0.0001250183122465387, 0.012385273352265358, 0.0036264711525291204, -0.0077605354599654675, -0.005192489828914404, 0.0026000048965215683, -0.005967039614915848, -0.0077680554240942, -0.007170223165303469, -0.00398178631439805, -0.004117144737392664, -0.004899214021861553, 0.0331176333129406, 0.013799013569951057, -0.013483177870512009, 0.04626241698861122, 0.005654963664710522, 0.005587284918874502, -0.03895307332277298, 0.0055572050623595715, -0.008745642378926277, -0.043164219707250595, 0.02848537638783455, 0.03973514586687088, 0.02455998957157135, -0.018874945119023323, -0.010715855285525322, -0.030515749007463455, -0.020845159888267517, 0.023567361757159233, 0.0049104937352240086, -0.004489379469305277, 0.00320723676122725, 0.030515749007463455, 0.02892152965068817, 0.015746667981147766, -0.02774842642247677, 0.02129635214805603, 0.023537281900644302, 0.03245588392019272, 0.014769081026315689, 0.041720397770404816, -0.01622794196009636, 0.027312271296977997, 0.005102251190692186, -0.010151863098144531, -0.021491870284080505, -0.0285605750977993, 0.02022852748632431, 0.015145075507462025, 0.0005748022813349962, -0.007410860154777765, -0.010588017292320728, 0.005215049721300602, 0.021897945553064346, 0.0033764345571398735, -0.006888227071613073, -0.005023292265832424, -0.006974705960601568, 0.0036565507762134075, -0.00229732901789248, 0.0918029248714447, -0.020815080031752586, 0.007301821373403072, 0.01807783544063568, 0.020649641752243042, 0.01100913155823946, -0.013272620737552643, 0.01621290110051632, -0.019025344401597977, -0.006113677751272917, -0.017521364614367485, 0.0070912642404437065, 0.008978758938610554, 0.021912984549999237, 0.025898531079292297, -0.02890649065375328, 2.2956253815209493e-05, -0.058324333280324936, 0.0008582084556110203, -0.009106596931815147, 0.012588310055434704, 0.022093461826443672, -0.012204795144498348, -0.008459885604679585, 0.0005273328861221671, 0.010227061808109283, 0.0016120782820507884, -0.005260169040411711, 0.029056888073682785, 0.007474779151380062, 0.013663656078279018, 0.01027218159288168, -0.013595976866781712, -0.02346208319067955, 0.01913062296807766, 0.02637980505824089, -0.020333806052803993, -0.03032023087143898, -0.006470873020589352, 0.008407246321439743, -0.029177207499742508, -0.020469164475798607, 0.003786268876865506, -0.02021348848938942, 0.04608193784952164, 0.01121216919273138, -0.017596563324332237, -0.03218516707420349, -0.006455833092331886, -0.01229503471404314, -0.035584159195423126, 0.029447922483086586, 0.010745935142040253, 0.02747770957648754, 0.013904293067753315, -0.02057444304227829, -0.02347712405025959, 0.038471803069114685, 0.028064262121915817, -0.00128402269911021, -0.0367271862924099, 0.039163630455732346, -0.02532701939344406, 0.013490698300302029, -0.0346817709505558, -0.01479916088283062, -0.00034756032982841134, 0.0097307488322258, -0.02274017408490181, 0.013656135648488998, -0.01627306081354618, -0.01261087041348219, 0.01535563264042139, -0.02125123329460621, 0.031974609941244125, 0.006632550619542599, 0.015318033285439014, -0.001188143971376121, 0.005940719973295927, 0.008196689188480377, -0.022860491648316383, 0.0006659810314886272, -0.01627306081354618, -0.006745349150151014, 0.002772962674498558, 0.017987597733736038, 0.02089027874171734, 0.008971239440143108, -0.002868841402232647, -0.0006880707223899662, -0.04078793153166771, -0.006132477428764105, 0.029793838039040565, 0.03669710457324982, 0.000871838245075196, -0.0062828753143548965, -0.038381561636924744, 0.011197129264473915, -0.025507496669888496, -0.006023438647389412, -0.0059933592565357685, 0.029808878898620605, -0.01658889651298523, -0.024289272725582123, -0.015318033285439014, -0.025116462260484695, -0.043465014547109604, 0.023973437026143074, -0.01513755600899458, 0.023988476023077965, -0.030094634741544724, 0.015002197585999966, 0.01410732977092266, -0.000659401121083647, 0.005662483628839254, -0.005978319328278303, -0.030440550297498703, 0.031583573669195175, -0.015701549127697945, -0.03943434730172157, 0.03576463833451271, 0.03726861625909805, -0.007519898470491171, 0.016724254935979843, -0.0025830850936472416, -0.01536315307021141, -0.004654817283153534, -0.03224532678723335, 0.004948093090206385, -0.011603203602135181, -0.024379512295126915, 0.025462375953793526, 0.04578114300966263, -0.013626056723296642, -0.006613750942051411, 0.008948679082095623, -0.03134293854236603, -0.01729576662182808, -0.04265286400914192, 0.011693442240357399, -0.005215049721300602, 0.006331754848361015, -0.001076285494491458, -0.027703305706381798, 0.02776346541941166, 0.002562405541539192, 0.004290102049708366, 0.0208752378821373, -0.025011183694005013, 0.006880707107484341, 0.011979199014604092, 0.001951413694769144, 0.04722496494650841, 0.0165588166564703, -0.04141960293054581, -0.020860198885202408, -0.001631818013265729, -0.0439462885260582, -0.0035794717259705067, -0.016694175079464912, 0.01773192174732685, -0.02848537638783455, -0.006779188755899668, 0.0022559696808457375, 0.009347233921289444, 0.005470726173371077, -0.043856047093868256, 0.005764002446085215, 0.00016285281162708998, 0.013874213211238384, -0.01212207693606615, -0.012791347689926624, -0.04650305211544037, 0.007689096499234438, 0.021070756018161774, -0.023341765627264977, 0.003496752819046378, -0.0010029664263129234, -0.01809287630021572, 0.020694760605692863, 0.016814492642879486, -0.014265247620642185, 0.0010349260410293937, 0.007696616463363171, -0.013979491777718067, -0.0032147567253559828, 0.012242395430803299, 0.007971092127263546, -0.01625801995396614, -0.006899506784975529, -0.023537281900644302, -0.02558269537985325, -0.03790028765797615, 0.007538698613643646, 0.014197568409144878, 0.016423458233475685, -0.021807705983519554, -0.02671067975461483, 0.008602764457464218, 0.0069709462113678455, -0.0027861224953085184, -0.003727989736944437, -0.02277025394141674, 0.018905024975538254, 0.006192636676132679, 0.013866692781448364, -0.00901635829359293, -0.025793252512812614, 0.02532701939344406, 0.005346647929400206, -0.017220567911863327, -0.025883492082357407, -0.01228751428425312, -0.014769081026315689, 0.005466966424137354, -0.044608037918806076, 0.020063089206814766, 0.01726568676531315, 0.018213193863630295, -0.02600380964577198, -0.0061174375005066395, 0.01991269178688526, 0.02310112863779068, -0.023928318172693253, -0.0009564370848238468, 0.005670003592967987, 0.026830999180674553, -0.0034159140195697546, 0.0184387918561697, 0.031523413956165314, 0.0031414376571774483, 0.03239572420716286, -0.0007989892037585378, -0.00029515603091567755, -0.0026206846814602613, 0.0009569071116857231, -0.025101421400904655, -0.008204209618270397, 0.003113238140940666, -0.014167489483952522, -0.01152048446238041, 0.04653313383460045, 0.00813653040677309, 0.013287660665810108, -0.01803271658718586, 0.0729730948805809, -0.0188298262655735, 0.004072024952620268, 0.019416378811001778, -0.014287807047367096, -0.0006561111658811569, 0.02304096892476082, -0.020739881321787834, 0.012362712994217873, -0.009632989764213562, -0.010512818582355976, 0.001888434519059956, -0.015400752425193787, -0.042081352323293686, 0.03260628134012222, -0.046773768961429596, -0.007730455603450537, 0.022183701395988464, -0.03579471632838249, 0.02419903315603733, -0.005662483628839254, -0.008144049905240536, 0.005767762195318937, -0.028425216674804688, 0.006572391372174025, 0.03600527346134186, -0.017972556874155998, 0.015002197585999966, -0.020785000175237656, 0.01411485020071268, 0.011558083817362785, 0.021446751430630684, -0.0060121589340269566, -0.006982225924730301, -0.034441135823726654, -0.034080181270837784, 0.007373260799795389, -0.01916070096194744, 0.0002634314587339759, 0.01591210626065731, -0.023552322760224342, -0.04036681726574898, -0.007610137574374676, -0.012039357796311378, -0.010384979657828808, -0.014490844681859016, -0.015017237514257431, 0.017912399023771286, 0.028590654954314232, -0.030771424993872643, 0.01618282124400139, 0.026861077174544334, 0.0013582817045971751, 0.0004439090262167156, 0.008377167396247387, -0.013648616150021553, 0.012941746041178703, 0.01667913608253002, 0.009339714422821999, 0.01583690568804741, 0.028605693951249123, 0.003795668948441744, 0.02307104878127575, 0.0029290004167705774, 0.021747546270489693, 0.004316421691328287, -0.03928394988179207, 0.042111434042453766, -0.013407979160547256, 0.011076810769736767, 0.03708814084529877, -0.02713179402053356, -0.02810938097536564, -0.054684702306985855, -0.021461790427565575, -0.013400459662079811, 0.027974022552371025, 0.04072777181863785, 0.025808291509747505, 0.012016798369586468, 0.010212022811174393, -0.033749304711818695, -1.8373815692029893e-05, 0.011114410124719143, 0.001454160432331264, 0.042081352323293686, 0.03332819044589996, 0.02204834297299385, -0.025462375953793526, 0.016844572499394417, -0.018619269132614136, -0.012550710700452328, -0.006000879220664501, 0.027898823842406273, -0.023958398029208183, 0.029733680188655853, -0.005090971477329731, 0.007155183702707291, 0.0010396259604021907, -0.008384686894714832, 0.006211436353623867, -0.014678842388093472, 0.0052376096136868, -0.012648469768464565, -0.005692563485354185, 0.020815080031752586, 0.019250940531492233, 0.05125562846660614, -0.018889985978603363, -0.027222033590078354, 0.0005517725367099047, -0.006625030655413866, 0.0242441538721323, 0.0159271452575922, 0.01807783544063568, -0.010896333493292332, -0.02308608964085579, -0.020454123616218567, -0.01373885478824377, -0.016904732212424278, 0.02419903315603733, -0.009595390409231186, -0.014543483965098858, -0.00604599853977561, -0.018544070422649384, -0.02455998957157135, 0.0018376752268522978, 0.025041261687874794, -0.00865540374070406, 0.0006490612286143005, -0.01910054311156273, -0.044217005372047424, -0.01765672117471695, -0.013400459662079811, -0.014017091132700443, 0.026259485632181168, -0.023567361757159233, 0.030515749007463455, 0.023582402616739273, 0.023928318172693253, -0.007858294062316418, 0.0009235375327989459, -0.02738747000694275, -0.00956531148403883]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP - 4 : RAG Pipeline with Memory\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# --------------------------\n",
        "# 1️ Initialize OpenAI client\n",
        "# --------------------------\n",
        "# OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")  # Load API key from Colab secrets\n",
        "# client = OpenAI(api_key=api_key) # No need to re-initialize client here\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 2️ Initialize OpenAIEmbeddings\n",
        "# --------------------------\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 3️ Create/Load vector store\n",
        "# --------------------------\n",
        "\n",
        "persist_dir = \"/content/drive/MyDrive/Ironhack_final_project/chromadb_store\" # file path to the vector database\n",
        "\n",
        "\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=persist_dir,  # folder where your vector DB is stored\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4️ Add embedded chunked data to Chromadb_store (vector DB or store)\n",
        "# ----------------------------------------------------------\n",
        "texts = [chunk[\"text\"] for chunk in all_embedded_chunks]\n",
        "metadatas = [\n",
        "    {\n",
        "        \"source\": chunk.get(\"video_id\") or chunk.get(\"article_id\"),\n",
        "        \"chunk_index\": chunk[\"chunk_index\"]\n",
        "    }\n",
        "    for chunk in all_embedded_chunks\n",
        "]\n",
        "\n",
        "# Add documents with automatic embedding generation\n",
        "vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
        "\n",
        "print(f\"🎉 Done! Chroma vector store is loaded\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 5️ Create retriever\n",
        "# --------------------------\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})  # higher k for better results\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 6️ LLMChain prompt and chain\n",
        "# --------------------------\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\", \"chat_history\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful AI assistant for content creators.\n",
        "Always answer concisely, clearly, and in a structured format.\n",
        "Prefer bullet points or numbered steps (3–6 items).\n",
        "Each bullet should be 1–2 sentences max.\n",
        "Do not repeat information.\n",
        "If the context is incomplete, use conversation history or your own knowledge.\n",
        "\n",
        "Conversation history:\n",
        "{chat_history}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (concise and structured):\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# prompt = PromptTemplate(\n",
        "#     input_variables=[\"context\", \"question\", \"chat_history\"],\n",
        "#     template=\"\"\"\n",
        "# You are a helpful AI assistant. Use the context below and the chat history to answer the question.\n",
        "# If the context is incomplete, rely on conversation history or your knowledge to give the best answer.\n",
        "\n",
        "# Conversation history:\n",
        "# {chat_history}\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Question: {question}\n",
        "\n",
        "# Answer:\n",
        "# \"\"\"\n",
        "# )\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, openai_api_key=api_key) # Pass the api_key here\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Add memory here\n",
        "# ----------------------\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "qa_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "\n",
        "# ----------------------------------\n",
        "#  Full RAG function with memory\n",
        "# ----------------------------------\n",
        "\n",
        "def ask_rag(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Full RAG pipeline: retrieves relevant chunks, uses memory, and generates answer.\n",
        "    \"\"\"\n",
        "    # --- Retrieve relevant chunks ---\n",
        "    results = retriever.get_relevant_documents(query)\n",
        "\n",
        "    if not results:\n",
        "        return \"❌ No relevant documents found.\"\n",
        "\n",
        "    # --- Combine retrieved chunks ---\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    print(\"\\n📝 Retrieved context preview:\\n\", context[:1000])\n",
        "\n",
        "    # --- Load memory ---\n",
        "    chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
        "\n",
        "    # --- Generate answer ---\n",
        "    answer = qa_chain.run({\n",
        "        \"context\": context,\n",
        "        \"question\": query,\n",
        "        \"chat_history\": chat_history\n",
        "    })\n",
        "\n",
        "    # --- Save interaction into memory ---\n",
        "    memory.save_context({\"input\": query}, {\"output\": answer})\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "BRh5r9OIJlye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65f6de9-c5c8-4db3-83da-be2be0997142"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Done! Chroma vector store is loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP - 5 :  Testing my RAG retrieval and its memory with multiple questions\n",
        "# ============================================\n",
        "\n",
        "# First question: sets some context\n",
        "print(ask_rag(\"What is an AI image generator?\"))\n",
        "\n",
        "# Second question: relies on memory\n",
        "print(ask_rag(\"And which ones are considered the best?\"))\n",
        "\n",
        "# Third question: continues from memory\n",
        "print(ask_rag(\"Pick one for me\"))\n",
        "\n",
        "\n",
        "# # -----------------------\n",
        "# # 8️⃣ Testing my RAG\n",
        "# # -----------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     user_query = input(\"❓ Enter your question: \")\n",
        "#     answer = ask_rag(user_query)\n",
        "#     print(\"\\n💡 Answer:\\n\", answer)\n",
        "\n",
        "\n",
        "# # -----------------------------------------------------------\n",
        "# # 8️⃣ Testing RAG with memory with continuous questioning\n",
        "# # -----------------------------------------------------------\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"📝 You can ask multiple questions. Type 'exit' to quit.\\n\")\n",
        "\n",
        "#     while True:\n",
        "#         user_query = input(\"❓ Enter your question: \")\n",
        "#         if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "#             print(\"👋 Exiting chat. Goodbye!\")\n",
        "#             break\n",
        "\n",
        "#         answer = ask_rag(user_query)\n",
        "#         print(\"\\n💡 Answer:\\n\", answer)\n",
        "#         print(\"-\" * 50)  # separator for readability\n",
        "\n"
      ],
      "metadata": {
        "id": "l242PpnfJlwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56a0676-b00a-41e7-fcfb-0a38da98df28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1001266257.py:137: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Retrieved context preview:\n",
            " AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1001266257.py:151: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = qa_chain.run({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Definition**: An AI image generator is a software tool that creates images based on textual prompts provided by the user.\n",
            "- **Functionality**: Users can input descriptions, and the AI interprets these to produce visuals that match the given criteria.\n",
            "- **Technology**: These generators utilize advanced machine learning models, often trained on vast datasets of images and text.\n",
            "- **Applications**: They are used in various fields, including art, marketing, design, and entertainment, to create unique visuals quickly.\n",
            "- **Examples**: Popular AI image generators include Dalle 3, Midjourney, and Stable Diffusion.\n",
            "\n",
            "📝 Retrieved context preview:\n",
            " pretty good, but what do I not like? It's very resource intensive, so you'll need a powerful computer to be able to run it, which could be a limitation. Last up at number seven, we have Generative AI by Getty Images, which was developed in partnership with NVIDIA. This is by far the most expensive of all the offerings, starting at $49 for just 25 image generations. So why would you use this? Well, all the images are legally secure and commercially safe. The AI was trained on Getty Images' vast library of images, but as large as their archive is, it's not as large as what many of the options we looked at used. Also, they offer fewer customization options compared to some of the open-source alternatives, which can restrict your creativity. So, there you have it, some of the best AI generators out there, each with its own strengths and also weaknesses. Which one are you most excited about to try? And did you learn about any new ones? Let me know in the comments. Please consider subscribin\n",
            "- **DALL-E 3**: Known for its high-quality image generation and versatility in interpreting prompts.\n",
            "- **Midjourney**: Popular for its artistic style and community-driven approach, often used for creative projects.\n",
            "- **Stable Diffusion**: An open-source option that allows for extensive customization and flexibility in image generation.\n",
            "- **DeepAI**: Offers a variety of tools for generating images, focusing on accessibility and ease of use.\n",
            "- **Generative AI by Getty Images**: Provides legally secure images, though it's more expensive and has fewer customization options compared to others.\n",
            "- **Artbreeder**: Allows users to blend images and create unique visuals collaboratively, emphasizing user creativity.\n",
            "\n",
            "📝 Retrieved context preview:\n",
            " mid Journey cost $10 per month Chachi BT cost $20 per month which can become quite expensive luckily I've reviewed seven of the best free AI image generators to give you my favorite free mid Journey alternative I'll show you the quality it produces what makes it different and answer how free is it really I'm not only going to tell you which image generator is the best for the next 5 hours I'm going to test them and rank them so that by the end of this video you have a new favorite free AI image generator so let's go to image generator number one cart. a so as you can see I'm now on the homepage of C art and what makes this special is that you can run multiple different models that generate a wildly different response one thing that makes cart specifically different compared to others is that you can open any image on the gallery and then you can click on the remix button what happens then is that we get the entire prompt that they have used as well as we see which model they used which\n",
            "- **CART.AI**: This generator allows you to run multiple models for diverse outputs and features a remix option to see prompts and models used.\n",
            "- **Free Usage**: You start with 150 stamina points, allowing approximately 15 free generations daily.\n",
            "- **Quality**: It produces high-quality images, such as a fashionable blonde woman in NYC, with options for enhancing image quality.\n",
            "- **Customization**: Offers various models, including the popular Flux model, and settings to refine prompts automatically. \n",
            "\n",
            "**Recommendation**: Try CART.AI for its versatility and free usage!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**==================>>> T O O L S <<<========================**"
      ],
      "metadata": {
        "id": "5ZHn7ePsqqC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP - 6 : Retriever tool (Chroma retriever) with metadata\n",
        "# ============================================\n",
        "\n",
        "from langchain.tools import Tool\n",
        "\n",
        "retriever_tool = Tool(\n",
        "    name=\"Chroma Retriever\",\n",
        "    func=vectorstore.as_retriever().get_relevant_documents,\n",
        "    description=(\n",
        "        \"This is the PRIMARY tool to use first. \"\n",
        "        \"Always try this before using any other tool. \"\n",
        "        \"It retrieves relevant document chunks from the Chroma database \"\n",
        "        \"to answer questions when the context exists.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"✅ Retriever tool is loaded\")"
      ],
      "metadata": {
        "id": "vBAeCTMwJlt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed15fe6-db5e-4922-f472-8ba1033f66dd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Retriever tool is loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP - 7 : Google Search tool (Tavily)\n",
        "# ============================================\n",
        "\n",
        "\n",
        "# from langchain.tools import Tool\n",
        "\n",
        "\n",
        "# TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "# if not TAVILY_API_KEY:\n",
        "#     raise ValueError(\"❌ No Tavily API key found! Please add it in Colab secrets.\")\n",
        "\n",
        "from tavily import TavilyClient\n",
        "tavily = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "def search_tavily(query: str):\n",
        "    \"\"\"Perform a web search using Tavily and return top results as a list.\"\"\"\n",
        "    results = tavily.search(query, max_results=3)\n",
        "    # Return a list of result content\n",
        "    return [r[\"content\"] for r in results[\"results\"]]\n",
        "\n",
        "search_tool = Tool(\n",
        "    name=\"Google Search (Tavily)\",\n",
        "    func=search_tavily,\n",
        "    description=\"Use this when the question cannot be answered from the context. Returns top 3 web search results.\"\n",
        ")\n",
        "\n",
        "print(\"✅ Search tool is loaded\")\n",
        "\n",
        "# ------------------------------------\n",
        "# Testing the search tool (Tavily)\n",
        "# ------------------------------------\n",
        "\n",
        "results = search_tavily(\"AI tools for writing blogs\")\n",
        "print(f\"🔍 Tavily Search Results:\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\" {i}. {r}\")"
      ],
      "metadata": {
        "id": "CflNew8NYuWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6a2018-14e8-4bc3-aec2-33ffaabdd2bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Search tool is loaded\n",
            "🔍 Tavily Search Results:\n",
            " 1. My top 7 picks for the best AI for writing SEO rich blog content tools –. Koala AI. WriteHuman. Copymatic. Jenni AI.\n",
            " 2. With RightBlogger, you can write SEO-friendly articles in minutes, turn any videos & audio recordings into blog posts, and create interactive AI lead magnets to grow your email list—all using simple, powerful AI tools. | Feature | RightBlogger | Other AI Tools | ChatGPT | | AI Blogging Tools |  |  |  | RightBlogger Tool Studio lets you create custom AI-powered tools for your website that engage readers, capture leads, and drive traffic. That said, we do believe it's important for bloggers and content creators to use these AI tools responsibly. Powerful AI tools like the 80+ inside RightBlogger are designed to give you a great starting point to all of your content creation needs. Can AI-generated content be detected by tools?\n",
            " 3. Go from blank page to polished blog post in seconds with Grammarly, your AI writing partner that makes crafting compelling blog posts as easy as a few clicks. Sign up for Grammarly Free and get access to AI-powered writing assistance that makes it easy to create high-quality blog posts in seconds. These blog post samples were created with Grammarly's AI writing assistance. **Prompt:**Write a blog post about how business students can use infographics to bring data to life. **Prompt:**Write a blog post on five ways companies can use social media to reach their customers. **Prompt:**Write a blog post about how business students can use infographics to bring data to life. **Prompt:**Write a blog post on five ways companies can use social media to reach their customers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# STEP 8 - W H I S P E R  T O O L - for Speech-to-Text\n",
        "# ========================================================\n",
        "\n",
        "# import whisper # Removed this import\n",
        "# import sounddevice as sd # Keep this import if local recording is desired, but address PortAudio issue separately\n",
        "from scipy.io.wavfile import write\n",
        "from langchain.tools import Tool\n",
        "from openai import OpenAI # Ensure OpenAI client is imported\n",
        "\n",
        "# Load Whisper model once # Removed this line\n",
        "\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "def record_audio(filename=\"input.wav\", duration=5, fs=16000):\n",
        "    print(f\"🎤 Recording for {duration} seconds...\")\n",
        "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
        "    sd.wait()\n",
        "    write(filename, fs, audio)\n",
        "    print(\"✅ Recording finished\")\n",
        "    return filename\n",
        "\n",
        "def transcribe_audio_openai(audio_file_path):\n",
        "    \"\"\"Transcribes audio using OpenAI's Whisper model.\"\"\"\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        # Use the existing 'client' object from cell oz4C828MYuR5\n",
        "        # Make sure the client is initialized in a preceding cell\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\", # OpenAI's Whisper model\n",
        "            file=audio_file\n",
        "        )\n",
        "    return transcription.text\n",
        "\n",
        "# --- Wrap in a Tool ---\n",
        "def whisper_speech_to_text(audio_file_path):\n",
        "    \"\"\"Transcribes a given audio file into text using OpenAI's Whisper.\"\"\"\n",
        "    # Note: This now expects a file path as input, not live recording due to PortAudio issue\n",
        "    # If local recording is resolved, you can add logic here to record first\n",
        "    if not os.path.exists(audio_file_path):\n",
        "         return f\"❌ Error: Audio file not found at {audio_file_path}\"\n",
        "\n",
        "    text = transcribe_audio_openai(audio_file_path)\n",
        "    return text\n",
        "\n",
        "whisper_tool = Tool(\n",
        "    name=\"Whisper Speech-to-Text\",\n",
        "    func=whisper_speech_to_text,\n",
        "    description=(\n",
        "        \"Transcribes an audio file (provide file path) into text using OpenAI's Whisper model. \"\n",
        "        # Removed the part about recording live audio due to environment limitations\n",
        "        \"Useful for converting spoken content from a file into text for analysis or response generation.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"✅ Whisper tool is loaded (using OpenAI)\")"
      ],
      "metadata": {
        "id": "Gans4It3YuUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d96013c-0597-4457-e801-3840b65568e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Whisper tool is loaded (using OpenAI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# STEP - 9 : TTS (Text-to-Speech) tool with OpenAI\n",
        "# =====================================================\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def text_to_speech(text, filename=\"response.mp3\"):\n",
        "    \"\"\"\n",
        "    Convert agent's text response to natural speech using OpenAI TTS\n",
        "    \"\"\"\n",
        "    with client.audio.speech.with_streaming_response.create(\n",
        "        model=\"gpt-4o-mini-tts\",   # You can also try \"gpt-4o-tts\" for higher quality\n",
        "        voice=\"alloy\",             # Options: alloy, verse, sage, etc.\n",
        "        input=text,\n",
        "    ) as response:\n",
        "        response.stream_to_file(filename)\n",
        "\n",
        "    print(f\"🔊 Saved speech to {filename}\")\n",
        "    return filename\n",
        "\n",
        "# Wrap text-to-speech function as a Tool\n",
        "tts_tool = Tool(\n",
        "    name=\"Text-to-Speech\",\n",
        "    func=text_to_speech,\n",
        "    description=\"Converts text into natural-sounding speech using OpenAI TTS and saves as MP3.\"\n",
        ")\n",
        "\n",
        "print(\"✅ Text-to-Speech tool is loaded\")"
      ],
      "metadata": {
        "id": "oz4C828MYuR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729a26fe-4ef9-4670-9834-dbfdd9c2f594"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Text-to-Speech tool is loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =================================================\n",
        "# STEP - 10 : N E W S  T O O L - Powered by Google\n",
        "# =================================================\n",
        "\n",
        "import os\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.agents import Tool\n",
        "\n",
        "# --------------------------\n",
        "# 1️⃣ Load SerpAPI key\n",
        "# --------------------------\n",
        "# For Colab, you can use userdata.get() if available\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    serpapi_api_key = userdata.get('SERPAPI_API_KEY')\n",
        "except ImportError:\n",
        "    serpapi_api_key = os.getenv('SERPAPI_API_KEY')\n",
        "\n",
        "if not serpapi_api_key:\n",
        "    raise ValueError(\"❌ No SerpAPI API key found! Please set it in environment variables or Colab secrets.\")\n",
        "\n",
        "os.environ['SERPAPI_API_KEY'] = serpapi_api_key\n",
        "\n",
        "# --------------------------\n",
        "# Initialize SerpAPI wrapper\n",
        "# --------------------------\n",
        "search = SerpAPIWrapper()  # automatically uses SERPAPI_API_KEY\n",
        "\n",
        "# --------------------------\n",
        "# Safe search function\n",
        "# --------------------------\n",
        "def safe_news_search(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the web using SerpAPI and returns a concise summary of the top results.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query. Must not be empty.\n",
        "\n",
        "    Returns:\n",
        "        str: Concise summary of the search results or an error message if input is invalid.\n",
        "    \"\"\"\n",
        "    if not query or query.strip() == \"\":\n",
        "        return \"❌ Cannot perform search: the query is empty.\"\n",
        "\n",
        "    # Get top 5 results for brevity\n",
        "    results = search.results(query)\n",
        "    organic = results.get(\"organic_results\", [])\n",
        "\n",
        "    if not organic:\n",
        "        return \"No results found for your query.\"\n",
        "\n",
        "    summary_lines = []\n",
        "    for i, r in enumerate(organic[:5], 1):\n",
        "        title = r.get(\"title\", \"No title\")\n",
        "        link = r.get(\"link\", \"No link\")\n",
        "        snippet = r.get(\"snippet\", \"\")\n",
        "        summary_lines.append(f\"{i}. {title}\\n   {snippet}\\n   🔗 {link}\")\n",
        "\n",
        "    return \"\\n\\n\".join(summary_lines)\n",
        "\n",
        "# --------------------------\n",
        "# Wrap as a LangChain tool\n",
        "# --------------------------\n",
        "news_tool = Tool(\n",
        "    name=\"Latest AI News\",\n",
        "    func=safe_news_search,\n",
        "    description=(\n",
        "        \"Use this tool to search the web for factual information, news, or updates on a specific topic. \"\n",
        "        \"Only call it when the user asks a factual question or requests the latest news. \"\n",
        "        \"Do NOT use it for greetings, casual conversation, or personal opinions. \"\n",
        "        \"The input must be a valid search query; empty queries will return an error.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"✅ Latest AI News tool is loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHbtslgLJreG",
        "outputId": "1db1cc0f-7298-4432-f717-74e845ad087a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Latest AI News tool is loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b1dd1bc",
        "outputId": "705c559c-b531-46cb-babb-fcbd41b52797"
      },
      "source": [
        "!pip install fpdf"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=66b22d2ecb68e03b59d6396d7d3c84f4cb3c20d737012bd583a57205c72c7b81\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/11/dc73d78e40a218ad52e7451f30166e94491be013a7850b5d75\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "# =================================================\n",
        "# STEP - 10 :  SAVE CHAT AS PDF  T O O L\n",
        "# =================================================\n",
        "\n",
        "def save_chat_as_pdf(chat_text: str, filename=\"chat.pdf\"):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, chat_text)\n",
        "    pdf.output(filename)\n",
        "    return f\"Chat saved to {filename}\"\n",
        "\n",
        "from langchain.agents import Tool\n",
        "\n",
        "save_pdf_tool = Tool(\n",
        "    name=\"SaveChatPDF\",\n",
        "    func=save_chat_as_pdf,\n",
        "    description=\"Saves the current chat as a PDF file.\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "pLr0LWHFAOU1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================\n",
        "# STEP - 10 :  S U M M A R I S E   T O O L\n",
        "# =================================================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def summarize_url(url: str):\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    # Use your LLM to summarize\n",
        "    summary = llm.predict(f\"Summarize this text:\\n{text}\")\n",
        "    return summary\n",
        "\n",
        "url_summary_tool = Tool(\n",
        "    name=\"URLSummary\",\n",
        "    func=summarize_url,\n",
        "    description=\"Reads a URL and returns a summarized version of its content.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "tRkdIVaQAvrz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP - 11 : G R E E T I N G   T O O L\n",
        "# ============================================\n",
        "\n",
        "import random # Import the random module\n",
        "\n",
        "def greet_user(user_input: str = None) -> str:\n",
        "    greetings_general = [\n",
        "        \"Hello! 👋 I’m your AI Content Coach. How can I assist you today?\",\n",
        "        \"Hi there! I’m here to help you create amazing content. What would you like to work on?\",\n",
        "        \"Hey! Ready to improve your content? Let’s get started.\",\n",
        "        \"Hello! I can guide you through content creation, AI tips, or signature design. What’s first?\"\n",
        "    ]\n",
        "\n",
        "    if user_input:\n",
        "        user_input_lower = user_input.lower()\n",
        "        if any(word in user_input_lower for word in [\"morning\", \"afternoon\", \"evening\"]):\n",
        "            time_word = next((w for w in [\"morning\", \"afternoon\", \"evening\"] if w in user_input_lower), \"day\")\n",
        "            return f\"Good {time_word.capitalize()}! I’m your AI Content Coach. How can I help you today?\"\n",
        "        if any(word in user_input_lower for word in [\"hi\", \"hello\", \"hey\"]):\n",
        "            return random.choice(greetings_general)\n",
        "\n",
        "    return random.choice(greetings_general)\n",
        "\n",
        "greeting_tool = Tool(\n",
        "    name=\"Greeting\",\n",
        "    func=greet_user,\n",
        "    description=\"Responds naturally to greetings like 'hi', 'hello', or 'good morning'.\"\n",
        ")\n",
        "\n",
        "\n",
        "print(\"✅ Greeting tool is loaded\")"
      ],
      "metadata": {
        "id": "y5AkdBTAYuNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bae3388-72be-44d7-8651-b0ead8a704a5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Greeting tool is loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**================  AGENT  ===================**"
      ],
      "metadata": {
        "id": "eqS_H3qF5Ndr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP - 12 : Conversational A G E N T\n",
        "# ============================================\n",
        "\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.tools import Tool\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "import openai\n",
        "# import sounddevice as sd\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import wave\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Integrating list of tools to the Agent\n",
        "# ----------------------------------------\n",
        "\n",
        "tools = [search_tool, greeting_tool, retriever_tool,  whisper_tool, news_tool, tts_tool, save_pdf_tool, url_summary_tool]\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# 3️⃣ Initialize Agent with Memory\n",
        "# ------------------------------------\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "print(\"🤖 AI Content Coach is ready! Type 'exit' to quit.\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 4️⃣ Run Conversation Loop\n",
        "# --------------------------\n",
        "\n",
        "voice_enabled = False  # 🔇 Default OFF\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Toggle voice manually\n",
        "    if user_input.lower() == \"voice on\":\n",
        "        voice_enabled = True\n",
        "        print(\"🔊 Voice enabled.\")\n",
        "        continue\n",
        "    elif user_input.lower() == \"voice off\":\n",
        "        voice_enabled = False\n",
        "        print(\"🔇 Voice disabled.\")\n",
        "        continue\n",
        "\n",
        "    # Agent responds\n",
        "    response = agent.run(user_input)\n",
        "    print(\"\\nAI:\", response)\n",
        "\n",
        "    # Play audio only if enabled\n",
        "    if voice_enabled:\n",
        "        text_to_speech(response)"
      ],
      "metadata": {
        "id": "G2Pkc23sYuK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3c58e2-6381-4b22-8d20-c7e675a46e13"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-87304444.py:27: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 AI Content Coach is ready! Type 'exit' to quit.\n",
            "\n",
            "You: Hello\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Greeting\",\n",
            "    \"action_input\": \"Hello!\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mHey! Ready to improve your content? Let’s get started.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Hey! Ready to improve your content? Let’s get started.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "AI: Hey! Ready to improve your content? Let’s get started.\n",
            "\n",
            "You: I want to become an AI content creator\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To become an AI content creator, start by learning about AI technologies, tools, and platforms. Familiarize yourself with machine learning concepts, natural language processing, and content generation techniques. You can take online courses, read books, and follow industry news to stay updated. Additionally, practice creating content using AI tools, and consider building a portfolio to showcase your work. Engaging with communities and networking with other creators can also be beneficial.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "AI: To become an AI content creator, start by learning about AI technologies, tools, and platforms. Familiarize yourself with machine learning concepts, natural language processing, and content generation techniques. You can take online courses, read books, and follow industry news to stay updated. Additionally, practice creating content using AI tools, and consider building a portfolio to showcase your work. Engaging with communities and networking with other creators can also be beneficial.\n",
            "\n",
            "You: No I want to create content using ai tools\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To create content using AI tools, you can explore various platforms and software designed for content generation. Some popular AI content creation tools include:\\n\\n1. **OpenAI's GPT-3**: You can use this for generating text-based content, such as articles, stories, and more.\\n2. **Canva**: Offers AI-powered design suggestions for creating visuals and graphics.\\n3. **Copy.ai**: Helps in generating marketing copy, blog posts, and social media content.\\n4. **Jasper**: An AI writing assistant that helps with content creation for various purposes.\\n5. **Lumen5**: Converts text content into engaging videos using AI.\\n\\nStart by experimenting with these tools to see which ones best fit your content creation needs.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "AI: To create content using AI tools, you can explore various platforms and software designed for content generation. Some popular AI content creation tools include:\n",
            "\n",
            "1. **OpenAI's GPT-3**: You can use this for generating text-based content, such as articles, stories, and more.\n",
            "2. **Canva**: Offers AI-powered design suggestions for creating visuals and graphics.\n",
            "3. **Copy.ai**: Helps in generating marketing copy, blog posts, and social media content.\n",
            "4. **Jasper**: An AI writing assistant that helps with content creation for various purposes.\n",
            "5. **Lumen5**: Converts text content into engaging videos using AI.\n",
            "\n",
            "Start by experimenting with these tools to see which ones best fit your content creation needs.\n",
            "\n",
            "You: what was previous question?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The previous question was about creating content using AI tools.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "AI: The previous question was about creating content using AI tools.\n",
            "\n",
            "You: quit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP 13 : S A V E  T H E  A G E N T\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import dill\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# --- Paths ---\n",
        "agent_dir = \"/content/drive/MyDrive/Ironhack_final_project/agent_save\"\n",
        "os.makedirs(agent_dir, exist_ok=True)\n",
        "\n",
        "# --- 1. Save Tools  ---\n",
        "def save_tools_config(tools, agent_dir):\n",
        "    tools_config = []\n",
        "    for t in tools:\n",
        "        tools_config.append({\n",
        "            \"name\": t.name,\n",
        "            \"description\": t.description,\n",
        "            \"func_name\": t.func.__name__  # just save function name\n",
        "        })\n",
        "    with open(os.path.join(agent_dir, \"tools_config.json\"), \"w\") as f:\n",
        "        json.dump(tools_config, f, indent=2)\n",
        "    print(f\"✅ Tools config saved to {os.path.join(agent_dir, 'tools_config.json')}\")\n",
        "\n",
        "# 🔹 Call it here (assuming you have a `tools` list in your notebook)\n",
        "if \"tools\" in globals():\n",
        "    save_tools_config(tools, agent_dir)\n",
        "else:\n",
        "    print(\"⚠️ No tools found. Tools config not saved.\")\n",
        "\n",
        "# --- 2. Save LLM config ---\n",
        "llm_config = {\n",
        "    \"model_name\": \"gpt-4o-mini\",\n",
        "    \"temperature\": 0.3\n",
        "}\n",
        "with open(os.path.join(agent_dir, \"llm_config.json\"), \"w\") as f:\n",
        "    json.dump(llm_config, f, indent=2)\n",
        "print(f\"✅ LLM configuration saved to {os.path.join(agent_dir, 'llm_config.json')}\")\n",
        "\n",
        "# --- 3. Save Memory ---\n",
        "if \"memory\" in globals() and isinstance(memory, ConversationBufferMemory):\n",
        "    try:\n",
        "        chat_history_state = memory.load_memory_variables({})\n",
        "        with open(os.path.join(agent_dir, \"chat_history.pkl\"), \"wb\") as f:\n",
        "            dill.dump(chat_history_state, f)\n",
        "        print(f\"✅ Chat history saved to {os.path.join(agent_dir, 'chat_history.pkl')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving chat history: {e}\")\n",
        "else:\n",
        "    print(\"⚠️ Memory object not found or invalid. Chat history not saved.\")\n",
        "\n",
        "# --- 4. Persist Chroma Vectorstore ---\n",
        "if \"vectorstore\" in globals() and isinstance(vectorstore, Chroma):\n",
        "    try:\n",
        "        vectorstore.persist()\n",
        "        print(\"✅ Chroma vector store persisted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error persisting Chroma vector store: {e}\")\n",
        "else:\n",
        "    print(\"⚠️ Vector store not found or invalid. Not persisted.\")\n",
        "\n",
        "print(\"🎉 Agent saved successfully!\")\n"
      ],
      "metadata": {
        "id": "iNxvmdY2YuIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f96b9a9-021f-4a58-ae3d-6507803aba23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tools config saved to /content/drive/MyDrive/Ironhack_final_project/agent_save/tools_config.json\n",
            "✅ LLM configuration saved to /content/drive/MyDrive/Ironhack_final_project/agent_save/llm_config.json\n",
            "✅ Chat history saved to /content/drive/MyDrive/Ironhack_final_project/agent_save/chat_history.pkl\n",
            "✅ Chroma vector store persisted.\n",
            "🎉 Agent saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2839597089.py:58: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=================== > > > LANGSMITH EVALUATION < < < ======================**"
      ],
      "metadata": {
        "id": "sdWdFyfiTEDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VezSfFa4HJvr",
        "outputId": "3db7425e-13d0-4a1c-9539-26e3a39c07a2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langsmith\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzdUelN-MIFx",
        "outputId": "cc1e5a52-cc33-4e67-d375-4acba605fda1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.27)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.3)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "import os\n",
        "\n",
        "# LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")      #uncomment and run either one according to notebook if not already\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "client = Client(api_key=LANGCHAIN_API_KEY)\n"
      ],
      "metadata": {
        "id": "hpKOyI7mMIDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now set the environment for LangSmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"      # enable tracing\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"AI_Content_Coach\"  # custom project name\n",
        "\n",
        "print(\"✅ LangSmith tracing enabled and API key loaded securely.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_A17pIvO3qs",
        "outputId": "3cadcb1e-742a-4b3b-f4ce-b19bf207c3fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LangSmith tracing enabled and API key loaded securely.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "langsmith_eval_manual.py\n",
        "------------------------\n",
        "Evaluate your agent with LangSmith using manually defined Q–A pairs.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "from langsmith import Client\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from openevals.llm import create_llm_as_judge\n",
        "from openevals.prompts import (\n",
        "    CORRECTNESS_PROMPT,\n",
        "    HALLUCINATION_PROMPT,\n",
        "    CONCISENESS_PROMPT,\n",
        ")\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "DATASET_NAME = \"Agent Evaluation Dataset\"\n",
        "LLM_MODEL = \"gpt-4o\"          # Model used to generate answers if no agent provided\n",
        "JUDGE_MODEL = \"openai:o3-mini\"  # Model used for evaluation\n",
        "MAX_OUTPUT_CHARS = 600\n",
        "MAX_COMPLETION_TOKENS = 256\n",
        "# ----------------------------------------\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --------- Define your examples manually ---------\n",
        "examples = [\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What is the capital of France?\"},\n",
        "        \"outputs\": {\"answer\": \"Paris\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"Who wrote Hamlet?\"},\n",
        "        \"outputs\": {\"answer\": \"William Shakespeare\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\"question\": \"What is 2 + 2?\"},\n",
        "        \"outputs\": {\"answer\": \"4\"}\n",
        "    },\n",
        "    # 👉 Add more Q–A pairs here\n",
        "]\n",
        "\n",
        "# -------- Target function --------\n",
        "def truncate_text(s: str, max_chars: int = MAX_OUTPUT_CHARS) -> str:\n",
        "    return s if len(s) <= max_chars else s[:max_chars] + \"...[TRUNCATED]\"\n",
        "\n",
        "def make_target(agent=None):\n",
        "    \"\"\"Return a function that LangSmith will evaluate against the dataset.\"\"\"\n",
        "    llm = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
        "\n",
        "    def target(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        question = inputs.get(\"question\", \"\")\n",
        "        answer = \"\"\n",
        "\n",
        "        if agent:\n",
        "            try:\n",
        "                answer = agent.run(question)\n",
        "            except Exception:\n",
        "                try:\n",
        "                    answer = agent.predict(question)\n",
        "                except Exception:\n",
        "                    logger.warning(\"Agent call failed, falling back to LLM.\")\n",
        "                    answer = \"\"\n",
        "\n",
        "        if not answer:\n",
        "            prompt = f\"Answer concisely:\\n\\nQ: {question}\\nA:\"\n",
        "            try:\n",
        "                answer = llm.predict(prompt, max_tokens=MAX_COMPLETION_TOKENS)\n",
        "            except TypeError:\n",
        "                answer = llm.predict(prompt)\n",
        "\n",
        "        return {\"answer\": truncate_text(answer), \"contexts\": []}\n",
        "\n",
        "    return target\n",
        "\n",
        "# -------- Evaluators --------\n",
        "def make_evaluators():\n",
        "    correctness = create_llm_as_judge(\n",
        "        prompt=CORRECTNESS_PROMPT, feedback_key=\"correctness\", model=JUDGE_MODEL\n",
        "    )\n",
        "    hallucination = create_llm_as_judge(\n",
        "        prompt=HALLUCINATION_PROMPT, feedback_key=\"hallucination\", model=JUDGE_MODEL\n",
        "    )\n",
        "    conciseness = create_llm_as_judge(\n",
        "        prompt=CONCISENESS_PROMPT, feedback_key=\"conciseness\", model=JUDGE_MODEL\n",
        "    )\n",
        "\n",
        "    def correctness_eval(run, example):\n",
        "        return correctness(\n",
        "            inputs=example.inputs[\"question\"],\n",
        "            outputs=run.outputs[\"answer\"],\n",
        "            reference_outputs=example.outputs[\"answer\"]\n",
        "        )\n",
        "\n",
        "    def hallucination_eval(run, example):\n",
        "        return hallucination(\n",
        "            inputs=example.inputs[\"question\"],\n",
        "            outputs=run.outputs[\"answer\"],\n",
        "            context=\"\\n\".join(run.outputs.get(\"contexts\", []))\n",
        "        )\n",
        "\n",
        "    def conciseness_eval(run, example):\n",
        "        return conciseness(\n",
        "            inputs=example.inputs[\"question\"],\n",
        "            outputs=run.outputs[\"answer\"]\n",
        "        )\n",
        "\n",
        "    return [correctness_eval, hallucination_eval, conciseness_eval]\n",
        "\n",
        "# -------- Main --------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Initialize client\n",
        "    client = Client()\n",
        "\n",
        "    # 2. Create or update dataset\n",
        "    if not client.has_dataset(dataset_name=DATASET_NAME):\n",
        "        client.create_dataset(dataset_name=DATASET_NAME, description=\"Manual QA pairs\")\n",
        "    client.create_examples(dataset_name=DATASET_NAME, examples=examples)\n",
        "\n",
        "    # 3. Build target function (replace agent=None with your agent if available)\n",
        "    target = make_target(agent=None)\n",
        "\n",
        "    # 4. Get evaluators\n",
        "    evaluators = make_evaluators()\n",
        "\n",
        "    # 5. Run evaluation\n",
        "    results = client.evaluate(\n",
        "        target,\n",
        "        data=DATASET_NAME,\n",
        "        evaluators=evaluators,\n",
        "        description=\"Evaluation with correctness, hallucination, conciseness\",\n",
        "        blocking=True\n",
        "    )\n",
        "\n",
        "    # 6. Show results\n",
        "    print(\"Evaluation complete. View results in LangSmith UI:\")\n",
        "    try:\n",
        "        print(results.url)\n",
        "    except:\n",
        "        print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "0-UvkP3rLZOj",
        "outputId": "3ff5d658-e62b-4f78-f434-48b82aee11ad"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openevals'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3814265873.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenevals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_llm_as_judge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m from openevals.prompts import (\n\u001b[1;32m     15\u001b[0m     \u001b[0mCORRECTNESS_PROMPT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openevals'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXsS3zMALZMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEFcec2bLZKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjf59lFTLZII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETGjg4HELZGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_67zaWixLZD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcUQdeY6LZB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKLFpd4vLY_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hM1OyPwsLY8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GRJ64gTLY6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcAY8NXjLY3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE ONE THAT WORKS"
      ],
      "metadata": {
        "id": "vmag87JJLaK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Module Docstring ---\n",
        "\"\"\"\n",
        "This script creates a question-answer dataset for AI content creation in LangSmith,\n",
        "fetches the API key from Colab secrets, sets environment variables, defines specific\n",
        "Q&A pairs, uploads them to LangSmith, and evaluates agent responses using multiple\n",
        "custom evaluators (accuracy, hallucination, document relevance).\n",
        "\"\"\"\n",
        "\n",
        "# --- Standard library imports ---\n",
        "import logging\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "\n",
        "# --- Third-party imports ---\n",
        "from langsmith import Client\n",
        "from langsmith.evaluation import aevaluate\n",
        "from langchain import hub\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.documents import Document\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Colab secret handling ---\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Load environment variables from .env file (optional for local dev) ---\n",
        "# load_dotenv()\n",
        "\n",
        "# --- Configure logging ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Load LangSmith API key from Colab secrets ---\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "if not LANGCHAIN_API_KEY:\n",
        "    raise ValueError(\"❌ No LangSmith API key found in Colab secrets!\")\n",
        "\n",
        "# --- Set environment variables for LangSmith ---\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "logging.info(\"✅ LangSmith API key loaded and environment configured.\")\n",
        "\n",
        "# --- Initialize LangSmith Client ---\n",
        "# Explicitly initialize the client here to ensure it's available in this cell's scope\n",
        "try:\n",
        "    client = Client(api_key=LANGCHAIN_API_KEY)\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to initialize LangSmith client: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# --- Dataset Creation with Conflict Handling ---\n",
        "# --------------------------------------------------\n",
        "\n",
        "\n",
        "DATASET_NAME = \"rag_eval_dataset\"\n",
        "\n",
        "try:\n",
        "    # Use the langsmith client initialized in this cell\n",
        "    dataset = client.read_dataset(dataset_name=DATASET_NAME)\n",
        "    logging.info(f\"⚠️ Dataset '{DATASET_NAME}' already exists. Using existing dataset.\")\n",
        "except Exception as e:\n",
        "    logging.info(f\"Dataset '{DATASET_NAME}' not found. Creating a new one...\")\n",
        "    # Use the langsmith client initialized in this cell\n",
        "    dataset = client.create_dataset(\n",
        "        dataset_name=DATASET_NAME,\n",
        "        description=\"RAG evaluation dataset for AI content creation\"\n",
        "    )\n",
        "    logging.info(f\"✅ Dataset '{DATASET_NAME}' created successfully.\")"
      ],
      "metadata": {
        "id": "pCBGeVLMVnsK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVoobAW1K7M4",
        "outputId": "8035f0d9-9b09-426c-bbeb-025101ed73fd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(name='rag_eval_dataset', description='A dataset for evaluating a chatbot on creating an AI content creation and the tools required with questions and ideal answers.', data_type=<DataType.kv: 'kv'>, id=UUID('0c368c5b-f89b-47f6-b951-0836d5a806f3'), created_at=datetime.datetime(2025, 9, 12, 17, 54, 47, 395828, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 9, 12, 17, 54, 47, 395828, tzinfo=datetime.timezone.utc), example_count=55, session_count=14, last_session_start_time=datetime.datetime(2025, 9, 18, 18, 57, 38, 512980), inputs_schema=None, outputs_schema=None, transformations=None, metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Q&A pairs ---\n",
        "qa_pairs: List[Dict[str, Dict[str, str]]] = [\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": (\n",
        "                \"Suggest me AI tools for content creation in 2025\"\n",
        "            )\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": (\n",
        "                \"Here’s a list of essential AI tools in 2025 for a beginner as AI content creator:\\n\"\n",
        "                \"Text Generation:** GPT-4/5, Claude 3 - Write articles, scripts, and social media posts.\\n\"\n",
        "                \"Image & Video Creation:** MidJourney v6, DALL·E 3, Runway - Generate visuals, videos, and thumbnails.\\n\"\n",
        "                \"Audio & Voice:** ElevenLabs, Descript - Create voiceovers, podcasts, and edit audio.\\n\"\n",
        "                \"Video Editing:** Runway, CapCut AI - Auto-edit videos, add effects, captions, and summaries.\"\n",
        "                \"Social Media & Marketing:** Jasper.ai, Buffer AI - Automate posts, optimize engagement, track trends.\\n\"\n",
        "                \"Research & Idea Generation:** Elicit, Perplexity AI - Quick topic research and summaries.\\n\"\n",
        "                \"Analytics & Optimization:** VidIQ, Google Analytics AI - Track performance, improve reach and SEO.\\n\"\n",
        "                \"Workflow & Collaboration:** Notion AI, ClickUp AI - Organize content pipelines and automate tasks.\\n\"\n",
        "            )\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": (\n",
        "                \"Which AI tools can help improve SEO for blogs in 2025?\"\n",
        "            )\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": (\n",
        "                \"These tools help optimize content for better search engine ranking and visibility:\\n\"\n",
        "                \"-Surfer AI: Analyzes keywords and SEO structure to optimize blog content.\\n\"\n",
        "                \"-Frase: Suggests topic ideas, improves readability, and identifies content gaps.\\n\"\n",
        "                \"-Writesonic: Generates SEO-friendly content and meta descriptions.\\n\"\n",
        "                \"-Jasper: Supports keyword integration and content optimization for blogs.\\n\"\n",
        "\n",
        "            )\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": (\n",
        "                \"Which AI tools are good at summarizing long documents or PDFs?\"\n",
        "            )\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": (\n",
        "                \"These AI tools extract key points and produce concise summaries:\\n\"\n",
        "                \"-ChatGPT: Summarizes PDFs, Word documents, and long text efficiently.\\n\"\n",
        "                \"-Claude: Provides safe and accurate summaries in readable form.\\n\"\n",
        "                \"-Jasper: Highlights essential information and generates concise overviews.\\n\"\n",
        "\n",
        "            )\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": (\n",
        "                \"What AI tools are useful for social media post creation?\"\n",
        "            )\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": (\n",
        "                \"These tools help design graphics, captions, and formatted posts quickly:\\n\"\n",
        "                \"-Canva Magic Studio: Generates social media graphics and layouts automatically.\\n\"\n",
        "                \"-Microsoft Designer: Offers AI-assisted design for posts and banners.\\n\"\n",
        "                \"-Copy.ai: Writes captions, ad copy, and short-form content tailored for social platforms.\\n\"\n",
        "                            )\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": (\n",
        "                \"What AI tools can assist in image generation in 2025?\"\n",
        "            )\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": (\n",
        "                \"Answer: These tools create custom visuals, illustrations, and designs from text prompts:\\n\"\n",
        "                \"-MidJourney: Generates artistic and highly creative images.\\n\"\n",
        "                \"-DALL·E: Converts text descriptions into realistic or stylized images.\\n\"\n",
        "                \"-Stable Diffusion: Produces high-quality images for various styles and themes.\\n\"\n",
        "                \"-Canva Magic Studio: Combines AI image generation with easy-to-use templates for marketing and social media.\\n\"\n",
        "            )\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Add Q&A pairs to dataset ---\n",
        "for example in qa_pairs:\n",
        "    client.create_example(inputs=example[\"inputs\"], outputs=example[\"outputs\"], dataset_id=dataset.id)\n",
        "logging.info(f\"✅ Added {len(qa_pairs)} examples to dataset '{DATASET_NAME}'.\")"
      ],
      "metadata": {
        "id": "Z8HosHK5VotP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Define Q&A pairs ---\n",
        "# qa_pairs: List[Dict[str, Dict[str, str]]] = [\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"Here’s a list of essential AI tools in 2025 for a beginner as AI content creator:\\n\"\n",
        "#                 \"Text Generation:** GPT-4/5, Claude 3 - Write articles, scripts, and social media posts.\"\n",
        "#                 \"Image & Video Creation:** MidJourney v6, DALL·E 3, Runway - Generate visuals, videos, and thumbnails.\"\n",
        "#                 \"Audio & Voice:** ElevenLabs, Descript - Create voiceovers, podcasts, and edit audio.\"\n",
        "#                 \"Video Editing:** Runway, CapCut AI - Auto-edit videos, add effects, captions, and summaries.\"\n",
        "#                 \"Social Media & Marketing:** Jasper.ai, Buffer AI - Automate posts, optimize engagement, track trends.\"\n",
        "#                 \"Research & Idea Generation:** Elicit, Perplexity AI - Quick topic research and summaries.\"\n",
        "#                 \"Analytics & Optimization:** VidIQ, Google Analytics AI - Track performance, improve reach and SEO.\"\n",
        "#                 \"Workflow & Collaboration:** Notion AI, ClickUp AI - Organize content pipelines and automate tasks.\"\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"AI can help transform long blog articles into engaging short-form content for social media by automatically summarizing key points.\"\n",
        "#                 \"It can generate platform-specific captions, scripts, or hooks for Instagram Reels, TikTok videos, and LinkedIn posts.\"\n",
        "#                 \"AI tools can suggest relevant hashtags, keywords, and trending topics to boost visibility and engagement.\"\n",
        "#                 \"They can also create visual elements like graphics, carousels, or infographics from your blog content.\"\n",
        "#                 \"This allows you to efficiently repurpose content, save time, and maintain a consistent posting schedule across multiple platforms.\"\n",
        "\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"AI can help you create video content by streamlining scriptwriting, video generation, and editing in a seamless workflow.\\n\"\n",
        "#                 \"-For scriptwriting, tools like GPT-4/5 or Jasper.ai can generate engaging scripts, dialogue, or storyboards from your ideas.\"\n",
        "#                 \"-For video generation, platforms like Runway, Pictory, or Synthesia can turn scripts into AI-generated videos with animations, avatars, or stock footage.\"\n",
        "#                 \"-For editing, tools like Descript, CapCut AI, or Adobe Firefly can automatically trim, add captions, transitions, and enhance visuals.\" \"A typical workflow\"\n",
        "#                 \"starts with AI-generated scripts, moves to video creation, and finishes with AI-assisted editing before publishing on platforms like YouTube, TikTok, or Instagram.\"\n",
        "\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"AI-powered SEO tools to boost rankings:\\n\"\n",
        "#                 \"- SurferSEO: Recommends keywords, structure, and length based on competitors.\\n\"\n",
        "#                 \"- Clearscope: Uses NLP to include semantically related terms.\\n\"\n",
        "#                 \"- SEMRush AI Assistant: Tracks keyword rankings and suggests updates.\\n\"\n",
        "#                 \"- MarketMuse: Identifies content gaps for untapped opportunities.\\n\"\n",
        "#                 \"- ChatGPT (SEO plugins): Drafts keyword-rich meta descriptions and FAQ sections.\"\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"AI can generate fresh, trend-driven ideas:\\n\"\n",
        "#                 \"- ChatGPT: Brainstorms article titles, podcast themes, and outlines.\\n\"\n",
        "#                 \"- Jasper: Creates ad campaign slogans and A/B test variants.\\n\"\n",
        "#                 \"- Copy.ai: Produces multiple versions of headlines and hooks.\\n\"\n",
        "#                 \"- BuzzSumo + AI: Finds trending topics, viral posts, and competitor insights.\\n\"\n",
        "#                 \"- Notion AI: Organizes brainstormed ideas into campaigns, schedules, and task lists.\"\n",
        "#             )\n",
        "#         }\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# # --- Add Q&A pairs to dataset ---\n",
        "# for example in qa_pairs:\n",
        "#     client.create_example(inputs=example[\"inputs\"], outputs=example[\"outputs\"], dataset_id=dataset.id)\n",
        "# logging.info(f\"✅ Added {len(qa_pairs)} examples to dataset '{DATASET_NAME}'.\")"
      ],
      "metadata": {
        "id": "YElIQEA2uXoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWl760qGIROt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "langsmith_agent_eval.py\n",
        "\n",
        "Drop this into the same environment where you have your agent/retriever\n",
        "(or run it as a standalone evaluation). Update CONFIG at top for your\n",
        "paths/model choices.\n",
        "\n",
        "Dependencies:\n",
        "    pip install -U langsmith langchain openevals openai pandas\n",
        "Environment variables:\n",
        "    LANGSMITH_TRACING=true\n",
        "    LANGSMITH_API_KEY=...\n",
        "    OPENAI_API_KEY=...\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import textwrap\n",
        "import logging\n",
        "from functools import partial\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# LangSmith client + wrappers\n",
        "from langsmith import Client\n",
        "# openevals -> LLM-as-judge utilities\n",
        "from openevals.llm import create_llm_as_judge\n",
        "from openevals.prompts import CORRECTNESS_PROMPT, HALLUCINATION_PROMPT, CONCISENESS_PROMPT\n",
        "\n",
        "# Optional LangChain chat model (fallback path)\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import Document\n",
        "\n",
        "# ------------- CONFIG ----------------\n",
        "DATA_PATH = \"examples.csv\"   # or examples.jsonl; if None you can set `examples_list` below\n",
        "DATASET_NAME = \"My Agent Eval Dataset\"\n",
        "MAX_OUTPUT_CHARS = 800       # final truncation limit for every answer\n",
        "MAX_COMPLETION_TOKENS = 256  # hint to the LLM provider, still truncate post-hoc\n",
        "JUDGE_MODEL = \"openai:o3-mini\"  # model used by openevals judges (low-cost judge)\n",
        "LLM_MODEL = \"gpt-4o\"              # fallback LLM if you don't pass an `agent`\n",
        "TEMPERATURE = 0.0\n",
        "MAX_CONCURRENCY = 4\n",
        "# ------------------------------------\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_examples_from_csv(path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load CSV with columns 'question' and 'answer' into LangSmith examples format.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    required = {\"question\", \"answer\"}\n",
        "    if not required.issubset(set(df.columns)):\n",
        "        raise ValueError(f\"CSV must contain columns {required}. Found: {list(df.columns)}\")\n",
        "    examples = []\n",
        "    for _, row in df.iterrows():\n",
        "        examples.append({\n",
        "            \"inputs\": {\"question\": str(row[\"question\"])},\n",
        "            \"outputs\": {\"answer\": str(row[\"answer\"])},\n",
        "            \"metadata\": {}\n",
        "        })\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_or_update_dataset(client: Client, dataset_name: str, examples: List[Dict[str, Any]]):\n",
        "    \"\"\"Create dataset if missing and upload examples (idempotent-ish).\"\"\"\n",
        "    # Create dataset if not present\n",
        "    if not client.has_dataset(dataset_name=dataset_name):\n",
        "        logger.info(\"Creating dataset in LangSmith: %s\", dataset_name)\n",
        "        client.create_dataset(dataset_name=dataset_name, description=\"Auto-created dataset for agent evaluation\")\n",
        "    else:\n",
        "        logger.info(\"Dataset already exists: %s\", dataset_name)\n",
        "\n",
        "    # Upload examples in batches using client.create_examples\n",
        "    logger.info(\"Uploading %d examples to dataset %s\", len(examples), dataset_name)\n",
        "    # client.create_examples accepts a list; adapt to the client signature if needed\n",
        "    client.create_examples(dataset_name=dataset_name, examples=examples)\n",
        "    logger.info(\"Upload done.\")\n",
        "\n",
        "\n",
        "def truncate_text(s: str, max_chars: int = MAX_OUTPUT_CHARS) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    if len(s) <= max_chars:\n",
        "        return s\n",
        "    return s[:max_chars].rstrip() + \"\\n\\n...[TRUNCATED]\"\n",
        "\n",
        "\n",
        "def make_target(agent=None, retriever=None, llm_fallback: Optional[ChatOpenAI] = None,\n",
        "                max_completion_tokens: int = MAX_COMPLETION_TOKENS,\n",
        "                max_output_chars: int = MAX_OUTPUT_CHARS):\n",
        "    \"\"\"\n",
        "    Returns a function target(inputs: dict) -> dict suitable for client.evaluate().\n",
        "    - If `agent` is provided, the agent will be used (supports `.run(question)` or `.predict(question)`).\n",
        "    - If not, `retriever` + `llm_fallback` produce a RAG-style answer.\n",
        "    The returned dict MUST include keys that your evaluators need, e.g. 'answer' and 'contexts'.\n",
        "    \"\"\"\n",
        "    if llm_fallback is None:\n",
        "        llm_fallback = ChatOpenAI(model=LLM_MODEL, temperature=TEMPERATURE)\n",
        "\n",
        "    def target(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        question = inputs.get(\"question\") or inputs.get(\"prompt\") or \"\"\n",
        "        if question is None or question == \"\":\n",
        "            return {\"answer\": \"\", \"contexts\": []}\n",
        "\n",
        "        # Fetch contexts (if retriever available)\n",
        "        contexts = []\n",
        "        try:\n",
        "            if retriever:\n",
        "                docs: List[Document] = retriever.get_relevant_documents(question)\n",
        "                contexts = [d.page_content for d in docs]\n",
        "        except Exception as e:\n",
        "            logger.warning(\"Retriever call failed, continuing w/out contexts: %s\", e)\n",
        "            contexts = []\n",
        "\n",
        "        # Use agent if provided; attempt `.run()` then `.predict()`\n",
        "        answer = \"\"\n",
        "        if agent:\n",
        "            try:\n",
        "                # many LangChain agents expose .run()\n",
        "                answer = agent.run(question)\n",
        "            except Exception:\n",
        "                try:\n",
        "                    answer = agent.predict(question)\n",
        "                except Exception:\n",
        "                    logger.exception(\"Agent call failed, falling back to LLM-only.\")\n",
        "                    answer = \"\"\n",
        "\n",
        "        # Fallback: simple instruction prompt + contexts -> llm\n",
        "        if not answer:\n",
        "            # Build concise RAG prompt that instructs the model to be brief\n",
        "            prompt_parts = [\n",
        "                \"You are a concise, factual assistant. Base your answer only on the contexts if they exist.\",\n",
        "                f\"Question: {question}\",\n",
        "            ]\n",
        "            if contexts:\n",
        "                prompt_parts.append(\"CONTEXTS:\\n\" + \"\\n\\n---\\n\".join(contexts))\n",
        "                prompt_parts.append(\"Answer in 1-3 short paragraphs and be concise.\")\n",
        "            else:\n",
        "                prompt_parts.append(\"No external contexts provided. Answer concisely.\")\n",
        "\n",
        "            prompt = \"\\n\\n\".join(prompt_parts)\n",
        "            # Ask ChatOpenAI to produce the answer; pass token limit hint (providers may respect)\n",
        "            try:\n",
        "                answer = llm_fallback.predict(prompt, max_tokens=max_completion_tokens)\n",
        "            except TypeError:\n",
        "                # some LangChain versions use different kwargs naming; try without kwargs\n",
        "                answer = llm_fallback.predict(prompt)\n",
        "\n",
        "        # Enforce truncation to avoid super-long outputs in LangSmith\n",
        "        answer = truncate_text(answer, max_chars=max_output_chars)\n",
        "        return {\"answer\": answer, \"contexts\": contexts}\n",
        "\n",
        "    return target\n",
        "\n",
        "\n",
        "# ----------------- Evaluators -----------------\n",
        "# We'll create LLM-as-judge evaluators using OpenEvals, then wrap them into functions with the\n",
        "# signature expected by LangSmith's client.evaluate (run, example) -> dict\n",
        "\n",
        "def make_openevals_judges(judge_model: str = JUDGE_MODEL):\n",
        "    \"\"\"Return a small dict of judge callables from openevals\"\"\"\n",
        "    correctness_judge = create_llm_as_judge(\n",
        "        prompt=CORRECTNESS_PROMPT,\n",
        "        feedback_key=\"correctness\",\n",
        "        model=judge_model,\n",
        "    )\n",
        "    hallucination_judge = create_llm_as_judge(\n",
        "        prompt=HALLUCINATION_PROMPT,\n",
        "        feedback_key=\"hallucination\",\n",
        "        model=judge_model,\n",
        "    )\n",
        "    conciseness_judge = create_llm_as_judge(\n",
        "        prompt=CONCISENESS_PROMPT,\n",
        "        feedback_key=\"conciseness\",\n",
        "        model=judge_model,\n",
        "    )\n",
        "    return {\n",
        "        \"correctness\": correctness_judge,\n",
        "        \"hallucination\": hallucination_judge,\n",
        "        \"conciseness\": conciseness_judge\n",
        "    }\n",
        "\n",
        "\n",
        "def wrap_correctness(judge_callable):\n",
        "    \"\"\"Wrap openevals correctness judge into (run, example) signature.\"\"\"\n",
        "    def evaluator(run, example):\n",
        "        inputs = example.inputs.get(\"question\", \"\")\n",
        "        outputs = run.outputs.get(\"answer\", \"\")\n",
        "        reference_outputs = example.outputs.get(\"answer\", \"\")\n",
        "        try:\n",
        "            res = judge_callable(inputs=inputs, outputs=outputs, reference_outputs=reference_outputs)\n",
        "            # openevals returns dict like {'key':..., 'score': bool/float, 'comment': '...'}\n",
        "            return {\"score\": bool(res.get(\"score\")), \"comment\": res.get(\"comment\")}\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Correctness judge failed: %s\", e)\n",
        "            # Fallback heuristics: exact/near-exact string match\n",
        "            pred = (outputs or \"\").strip().lower()\n",
        "            ref = (reference_outputs or \"\").strip().lower()\n",
        "            simple_score = 1.0 if pred == ref and pred != \"\" else 0.0\n",
        "            return {\"score\": simple_score, \"comment\": f\"fallback_simple_match={simple_score}\"}\n",
        "    return evaluator\n",
        "\n",
        "\n",
        "def wrap_hallucination(judge_callable):\n",
        "    \"\"\"Judge whether answer contains hallucinated facts not supported by contexts.\"\"\"\n",
        "    def evaluator(run, example):\n",
        "        inputs = example.inputs.get(\"question\", \"\")\n",
        "        outputs = run.outputs.get(\"answer\", \"\")\n",
        "        # Contexts can be stored either in example.outputs or run.outputs; prefer run.outputs\n",
        "        contexts = run.outputs.get(\"contexts\") or example.outputs.get(\"contexts\") or []\n",
        "        context_text = \"\\n\\n---\\n\".join(contexts) if isinstance(contexts, (list, tuple)) else str(contexts)\n",
        "        try:\n",
        "            res = judge_callable(inputs=inputs, outputs=outputs, context=context_text)\n",
        "            return {\"score\": bool(res.get(\"score\")), \"comment\": res.get(\"comment\")}\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Hallucination judge failed: %s\", e)\n",
        "            # fallback: simple heuristic - check if many unique tokens in outputs are missing from context\n",
        "            out_tokens = set(str(outputs).lower().split())\n",
        "            ctx_tokens = set(str(context_text).lower().split())\n",
        "            missing = out_tokens - ctx_tokens\n",
        "            # proportion of tokens missing (naive)\n",
        "            prop_missing = len(missing) / max(1, len(out_tokens))\n",
        "            score = 0.0 if prop_missing > 0.5 else 1.0\n",
        "            return {\"score\": bool(score), \"comment\": f\"fallback_prop_missing={prop_missing:.2f}\"}\n",
        "    return evaluator\n",
        "\n",
        "\n",
        "def wrap_conciseness(judge_callable):\n",
        "    def evaluator(run, example):\n",
        "        inputs = example.inputs.get(\"question\", \"\")\n",
        "        outputs = run.outputs.get(\"answer\", \"\")\n",
        "        try:\n",
        "            res = judge_callable(inputs=inputs, outputs=outputs)\n",
        "            return {\"score\": bool(res.get(\"score\")), \"comment\": res.get(\"comment\")}\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Conciseness judge failed: %s\", e)\n",
        "            # fallback: prefer shorter outputs; pass if output <= MAX_OUTPUT_CHARS\n",
        "            is_concise = len((outputs or \"\")) <= MAX_OUTPUT_CHARS\n",
        "            return {\"score\": bool(is_concise), \"comment\": \"fallback_length_check\"}\n",
        "    return evaluator\n",
        "\n",
        "\n",
        "# ----------------- Runner / Main -----------------\n",
        "\n",
        "def run_evaluation(\n",
        "    client: Client,\n",
        "    target_callable,\n",
        "    dataset_name: str,\n",
        "    judges: Dict[str, Any],\n",
        "    evaluators_to_use: Optional[List[str]] = None,\n",
        "    blocking: bool = True,\n",
        "    max_concurrency: int = MAX_CONCURRENCY,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run client.evaluate with the wrapped evaluators.\n",
        "    `judges` should be the dict returned by make_openevals_judges()\n",
        "    \"\"\"\n",
        "    if evaluators_to_use is None:\n",
        "        evaluators_to_use = [\"correctness\", \"hallucination\", \"conciseness\"]\n",
        "\n",
        "    # Wrap judges into row-evaluators compatible with client.evaluate\n",
        "    row_evaluators = []\n",
        "    mapping = {\n",
        "        \"correctness\": wrap_correctness,\n",
        "        \"hallucination\": wrap_hallucination,\n",
        "        \"conciseness\": wrap_conciseness\n",
        "    }\n",
        "\n",
        "    for name in evaluators_to_use:\n",
        "        if name not in judges:\n",
        "            raise ValueError(f\"Unknown evaluator requested: {name}\")\n",
        "        wrapped = mapping[name](judges[name])\n",
        "        row_evaluators.append(wrapped)\n",
        "\n",
        "    logger.info(\"Launching evaluation on dataset: %s\", dataset_name)\n",
        "    experiment_results = client.evaluate(\n",
        "        target_callable,\n",
        "        data=dataset_name,\n",
        "        evaluators=row_evaluators,\n",
        "        description=\"Automated evaluation: correctness / hallucination / conciseness\",\n",
        "        max_concurrency=max_concurrency,\n",
        "        blocking=blocking,\n",
        "    )\n",
        "    return experiment_results\n",
        "\n",
        "\n",
        "# ------------- Example script usage -------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) Initialize LangSmith client\n",
        "    client = Client()  # reads LANGSMITH_API_KEY from env\n",
        "\n",
        "    # 2) Load examples (either from CSV / JSONL or supply examples_list manually)\n",
        "    if os.path.exists(DATA_PATH):\n",
        "        examples_list = load_examples_from_csv(DATA_PATH)\n",
        "    else:\n",
        "        # If you already have examples in memory, set them here\n",
        "        # Example format:\n",
        "        examples_list = [\n",
        "            {\"inputs\": {\"question\": \"What is an LLM?\"}, \"outputs\": {\"answer\": \"A large language model is...\"}},\n",
        "            # ...\n",
        "        ]\n",
        "        logger.warning(\"Using embedded examples_list - no DATA_PATH found at %s\", DATA_PATH)\n",
        "\n",
        "    # 3) Create/upload dataset\n",
        "    create_or_update_dataset(client, DATASET_NAME, examples_list)\n",
        "\n",
        "    # 4) Prepare target function:\n",
        "    # If you already have an `agent` variable in your notebook, import it and pass here.\n",
        "    # Example: from my_agent_module import agent, retriever\n",
        "    # Then call make_target(agent=agent, retriever=retriever)\n",
        "    # For this script we default to fallback LLM only:\n",
        "    llm_fallback = ChatOpenAI(model=LLM_MODEL, temperature=TEMPERATURE)\n",
        "    target = make_target(agent=None, retriever=None, llm_fallback=llm_fallback)\n",
        "\n",
        "    # 5) Create judges\n",
        "    judges = make_openevals_judges(judge_model=JUDGE_MODEL)\n",
        "\n",
        "    # 6) Run evaluation (blocking)\n",
        "    results = run_evaluation(\n",
        "        client=client,\n",
        "        target_callable=target,\n",
        "        dataset_name=DATASET_NAME,\n",
        "        judges=judges,\n",
        "        evaluators_to_use=[\"correctness\", \"hallucination\", \"conciseness\"],\n",
        "        blocking=True,\n",
        "        max_concurrency=MAX_CONCURRENCY\n",
        "    )\n",
        "\n",
        "    # 7) Print experiment details and where to view the UI\n",
        "    try:\n",
        "        print(\"Experiment results URL:\", results.url)\n",
        "    except Exception:\n",
        "        print(\"Experiment name:\", results.experiment_name)\n",
        "\n",
        "    # Optionally fetch row-level test results (pandas)\n",
        "    try:\n",
        "        df = client.get_test_results(project_name=None)  # or use project_name if you set one\n",
        "        print(\"Summary of test results (head):\")\n",
        "        print(df.head().to_string())\n",
        "    except Exception as e:\n",
        "        logger.info(\"Could not fetch test results via client.get_test_results(): %s\", e)\n",
        "\n",
        "    print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "jvL2cxfOIRMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "330rxwX2IRJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ty1w59tIRGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxrJ4nCvIQvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"❌ No OpenAI API key found in Colab secrets!\")"
      ],
      "metadata": {
        "id": "QrpKMqSwfFsS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "import os\n",
        "\n",
        "# LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")      #uncomment and run either one according to notebook if not already\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "client = Client(api_key=LANGCHAIN_API_KEY)\n"
      ],
      "metadata": {
        "id": "OkiBkxWlYKUL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" in globals() and OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    logging.info(\"✅ OPENAI_API_KEY environment variable set in cell 3GJ1BNnKv822.\")\n",
        "else:\n",
        "    logging.error(\"❌ OPENAI_API_KEY not found in globals or is empty.\")\n",
        "\n",
        "\n",
        "# --- Async helper to retrieve documents ---\n",
        "async def get_relevant_documents(query: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Retrieves relevant documents using the retriever from chatbot_backend.py.\n",
        "    \"\"\"\n",
        "    if retriever is None:\n",
        "        raise ValueError(\"Retriever is None. Ensure chatbot_backend.py initializes it correctly.\")\n",
        "    return await retriever.ainvoke(query)\n",
        "\n",
        "# --- Load evaluation prompts from LangChain Hub ---\n",
        "grade_prompt_hallucinations = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
        "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
        "grade_prompt_answer = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
        "\n",
        "# --- Custom Evaluator Functions ---\n",
        "\n",
        "\n",
        "\n",
        "async def answer_evaluator(run, example) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of the generated answer against reference answer.\n",
        "    \"\"\"\n",
        "    input_question = example.inputs.get(\"question\", \"\")\n",
        "    reference_answer = example.outputs.get(\"answer\", \"\")\n",
        "    prediction = run.outputs.get(\"answer\", \"\")\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "    answer_grader = grade_prompt_answer | llm\n",
        "\n",
        "    try:\n",
        "        # Correct input keys for rag-answer-vs-reference\n",
        "        score_response = await answer_grader.ainvoke({\n",
        "            \"question\": input_question,\n",
        "            \"reference_answer\": reference_answer,\n",
        "            \"predicted_answer\": prediction\n",
        "        })\n",
        "        # Parse output as string\n",
        "        score_text = str(score_response).strip().lower()\n",
        "        score = 1.0 if score_text in [\"yes\", \"y\", \"true\", \"t\"] else 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error grading answer accuracy: {e}\", exc_info=True)\n",
        "        score = 0.0\n",
        "\n",
        "    print(f'question=\"{input_question}\", accuracy_score={score}')\n",
        "    return {\"key\": \"answer_accuracy\", \"score\": score}\n",
        "\n",
        "\n",
        "async def answer_hallucination_evaluator(run, example) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates if the generated answer contains hallucinated information.\n",
        "    \"\"\"\n",
        "    input_question = example.inputs.get(\"question\", \"\")\n",
        "    prediction = run.outputs.get(\"answer\", \"\")\n",
        "    contexts = run.outputs.get(\"contexts\", [])\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "    hallucination_grader = grade_prompt_hallucinations | llm\n",
        "\n",
        "    try:\n",
        "        # Correct input keys for rag-answer-hallucination\n",
        "        score_response = await hallucination_grader.ainvoke({\n",
        "            \"answer\": prediction,\n",
        "            \"sources\": contexts\n",
        "        })\n",
        "        score_text = str(score_response).strip().lower()\n",
        "        score = 0.0 if score_text in [\"yes\", \"y\", \"true\", \"t\"] else 1.0\n",
        "        # Here, 1 = no hallucination, 0 = hallucination detected\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error grading hallucination: {e}\", exc_info=True)\n",
        "        score = 0.0\n",
        "\n",
        "    print(f'question=\"{input_question}\", hallucination_score={score}')\n",
        "    return {\"key\": \"answer_hallucination\", \"score\": score}\n",
        "\n",
        "\n",
        "async def docs_relevance_evaluator(run, example) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates if the retrieved documents are relevant to the input question.\n",
        "    \"\"\"\n",
        "    input_question = example.inputs.get(\"question\", \"\")\n",
        "    contexts = run.outputs.get(\"contexts\", [])\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o mini\", temperature=0)\n",
        "    relevance_grader = grade_prompt_doc_relevance | llm\n",
        "\n",
        "    try:\n",
        "        # Correct input keys for rag-document-relevance\n",
        "        score_response = await relevance_grader.ainvoke({\n",
        "            \"question\": input_question,\n",
        "            \"documents\": contexts\n",
        "        })\n",
        "        score_text = str(score_response).strip().lower()\n",
        "        score = 1.0 if score_text in [\"yes\", \"y\", \"true\", \"t\"] else 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error grading document relevance: {e}\", exc_info=True)\n",
        "        score = 0.0\n",
        "\n",
        "    print(f'question=\"{input_question}\", doc_relevance_score={score}')\n",
        "    return {\"key\": \"document_relevance\", \"score\": score}\n",
        "\n",
        "\n",
        "\n",
        "# --- Async Prediction Function ---\n",
        "async def predict_agent_run_for_eval(example: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Uses agent to answer a question and provide retrieved contexts.\n",
        "    \"\"\"\n",
        "    question = example.get(\"question\")\n",
        "    if not question:\n",
        "        logging.error(\"Question key missing from example inputs.\")\n",
        "        return {\"answer\": \"Error: Question missing.\", \"contexts\": []}\n",
        "\n",
        "    logging.info(f\"Evaluating question with agent: {question}\")\n",
        "\n",
        "    # Retrieve documents asynchronously\n",
        "    retrieved_docs_strings = []\n",
        "    try:\n",
        "        retrieved_docs = await get_relevant_documents(question)\n",
        "        retrieved_docs_strings = [doc.page_content for doc in retrieved_docs]\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Failed to retrieve documents: {e}\")\n",
        "\n",
        "    # Generate answer asynchronously\n",
        "    try:\n",
        "        response = await agent.ainvoke({\"input\": question})\n",
        "        final_answer = response.get(\"output\", \"No answer generated.\")\n",
        "        return {\"answer\": final_answer, \"contexts\": retrieved_docs_strings}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during agent execution: {e}\", exc_info=True)\n",
        "        return {\"answer\": f\"Error during agent execution: {e}\", \"contexts\": retrieved_docs_strings}\n",
        "\n",
        "\n",
        "# --- Run Evaluation ---\n",
        "dataset_name = DATASET_NAME  # Ensure this matches the dataset created above\n",
        "\n",
        "async def main():\n",
        "    experiment_results = await aevaluate(\n",
        "        predict_agent_run_for_eval,\n",
        "        data=dataset_name,\n",
        "        evaluators=[\n",
        "            answer_evaluator,\n",
        "            answer_hallucination_evaluator,\n",
        "            docs_relevance_evaluator\n",
        "        ],\n",
        "        experiment_prefix=f\"RAG_AI_Content_Eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        metadata={\n",
        "            \"variant\": \"Evaluation with specific questions from LangSmith dataset\",\n",
        "            \"source_dataset_name\": dataset_name\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Evaluation complete! ---\")\n",
        "    try:\n",
        "        print(f\"View results on LangSmith: {experiment_results.url}\")\n",
        "    except AttributeError:\n",
        "        print(f\"Experiment name: {experiment_results.experiment_name}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "54bf33e90f474048ad2e931024fb8805",
            "7a234a76201741419ce5ad5b8b6e2167",
            "ecb4491804c944759a027bf6760723a2",
            "f1155a36c27a4ba3a7a3e8919034fd04",
            "1d32985b0d5e40299bdd31b13c48aa6a",
            "cd385d4764cd4ffaa3bd4c43fd9749c7",
            "d6f093d12590487faa41525de9e57c44",
            "55e4dc09ec4140dfab630c304c814d58",
            "5675ade3cc564a4facc7387614f480ce",
            "e20fc304438646b6979f37cc1fab386a",
            "83413b41ea534693af392dac84b8e38f"
          ]
        },
        "id": "laFjrgYIeSpo",
        "outputId": "682dd462-472c-4c04-ef7a-c47a6a4c811e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'RAG_AI_Content_Eval_20250919_075218-7d81983d' at:\n",
            "https://smith.langchain.com/o/e11e364c-ad8b-4e26-9bf3-bf04a01e332f/datasets/0c368c5b-f89b-47f6-b951-0836d5a806f3/compare?selectedSessions=72a83842-d5eb-4b9a-9639-902d497cc7e4\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54bf33e90f474048ad2e931024fb8805"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools for image generation in 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'S2TAa4P2IuY'}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'source': 'S2TAa4P2IuY', 'chunk_index': 0}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'source': 'S2TAa4P2IuY', 'chunk_index': 0}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'chunk_index': 0, 'source': 'S2TAa4P2IuY'}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\")]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, several AI tools can assist in image generation, including DALL-E 3, which allows users to create images from text prompts and offers API access for automation. Microsoft's Bing Image Creator also utilizes DALL-E 3 and allows for unlimited image creation for free. Midjourney is known for its high-quality and vibrant images, though it operates on a subscription basis. Stable Diffusion is another popular option for generating images.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools can assist in image generation in 2025?\", accuracy_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", hallucination_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Useful AI tools for social media post creation include Hootsuite for managing and scheduling posts, Canva for designing visuals, Copy.ai for generating engaging captions and content ideas, and Buffer for social media management and analytics.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools are useful for social media post creation?\", accuracy_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", hallucination_score=0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question=\"What AI tools are useful for social media post creation?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools for summarizing long documents or PDFs\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"For summarizing long documents or PDFs, the AI Summarizer of WORDVICE.AI is highly recommended. It effectively condenses lengthy content into relevant summaries that highlight important points while maintaining thematic coherence. Users can adjust the length, mode, language, and document type of the summaries. Additionally, WORDVICE.AI offers a human edit option, enhancing credibility with editors who hold advanced degrees. This tool is especially useful for those who frequently work with detailed articles and need concise, accurate summaries.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", accuracy_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", hallucination_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"Which AI tools can help improve SEO for blogs in 2025?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, several AI tools can help improve SEO for blogs. The top tools include Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools assist content marketers in creating SEO-rich content, optimizing for search engines, and enhancing overall content quality.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", accuracy_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", hallucination_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Suggest me AI tools for content creation in 2025\", accuracy_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", hallucination_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools for image generation in 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'source': 'S2TAa4P2IuY', 'chunk_index': 0}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'source': 'S2TAa4P2IuY', 'chunk_index': 0}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'chunk_index': 0, 'source': 'S2TAa4P2IuY'}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'source': 'S2TAa4P2IuY', 'chunk_index': 0}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\")]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, several AI tools can assist in image generation. Notable options include DALL-E 3, which allows users to create images from text prompts and offers API access for automation. Microsoft's Bing Image Creator also utilizes DALL-E 3 and allows for unlimited image creation for free. Midjourney is known for its high-quality, vibrant images, although it operates on a subscription basis. Stable Diffusion is another popular option for generating images.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools can assist in image generation in 2025?\", accuracy_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", hallucination_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Useful AI tools for social media post creation include Hootsuite for managing and scheduling posts, Canva for designing visuals, Copy.ai for generating engaging captions and content ideas, and Buffer for social media management and analytics.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools are useful for social media post creation?\", accuracy_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", hallucination_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"Which AI tools are good at summarizing long documents or PDFs?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The AI Summarizer of WORDVICE.AI is highly effective for summarizing long documents and PDFs. It provides concise summaries that highlight important points while maintaining thematic coherence. Users can adjust the length, mode, language, and document type of the summaries. Additionally, it offers a human edit option for enhanced credibility, making it suitable for those who need accurate and reliable summaries.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", accuracy_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", hallucination_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools to improve SEO for blogs in 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, several AI tools can help improve SEO for blogs. The top tools include Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools assist content marketers in creating SEO-rich content, optimizing for search engines, and enhancing overall content quality.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", accuracy_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", hallucination_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Suggest me AI tools for content creation in 2025\", accuracy_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", hallucination_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, notable AI tools for image generation include DALL-E 3, which allows users to create images from text prompts, Midjourney for high-quality images, and Stable Diffusion for versatile image generation. Microsoft's Bing Image Creator also utilizes DALL-E 3 technology.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools can assist in image generation in 2025?\", accuracy_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", hallucination_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Useful AI tools for social media post creation include Hootsuite for managing and scheduling posts, Canva for designing visuals, Copy.ai for generating engaging captions and content ideas, and Buffer for social media management and analytics.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools are useful for social media post creation?\", accuracy_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", hallucination_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"Which AI tools are good at summarizing long documents or PDFs?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The AI Summarizer of WORDVICE.AI is highly effective for summarizing long documents and PDFs. It provides concise summaries that highlight important points while maintaining thematic coherence. Users can adjust the length, mode, language, and document type of the summaries. Additionally, it offers a human edit option for enhanced credibility, making it suitable for those who need accurate and reliable summaries.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", accuracy_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", hallucination_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools to improve SEO for blogs in 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, the following AI tools can help improve SEO for blogs: Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools assist content marketers in creating SEO-rich content, optimizing for search engines, and enhancing overall content quality.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", accuracy_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", hallucination_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Suggest me AI tools for content creation in 2025\", accuracy_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", hallucination_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools for image generation in 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'S2TAa4P2IuY'}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'chunk_index': 0, 'source': 'S2TAa4P2IuY'}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'source': 'S2TAa4P2IuY', 'chunk_index': 0}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\"), Document(metadata={'source': 'S2TAa4P2IuY', 'chunk_index': 0}, page_content=\"AI has revolutionized image creation. You can now type in a text prompt, like design a storefront for a cookie store, and get captivating visuals back. There are lots of different AI image generators out there, both free and also paid, and you've probably used some yourself. In this video, in partnership with Zapier, we'll look at the top seven AI image generators from Dalle 3 to Midjourney and more. Let's dive in. First up, we have Dalle 3, which is developed by OpenAI, the maker of ChatGPT. You can access this directly from chatgpt.com with a free account, although you only get two images per day. With a plus account, on the other hand, you can create more images, but plans start at $20 per month. As an alternative, you can go to Microsoft's Bing Image Creator, which allows you to create any number of images for free, and it uses Dalle 3 on the backend, the same as what you get with ChatGPT. So, what do I like or dislike about it? Well, overall, you get highly detailed and realistic images. It integrates directly with ChatGPT. If you're already having a conversation there, you can generate an image directly from that conversation. You also have some limited editing tools. For example, I can highlight an area of the image and have Dalle 3 regenerate that area. One of my favorite features, Dalle 3 offers API access, something that none of the other AI image generators offer. What this means is that you can automate the process of generating images based on specific triggers, like, for example, receiving a new email or filling out a form, or even just adding a row to a spreadsheet. Here's some example Zaps that you can set up using Dalle 3. For example, if I receive feedback at, let's say, the Kevin Cookie Company via Google Forms, I can use Zapier to automatically generate an image using Dalle that represents that feedback, and then I can automatically save that image to Google Drive. Honestly, this is so much quicker to understand feedback than to read the actual text. Zapier is free to use. However, you will need API access through OpenAI to make this all work. You can click on the link in the description to learn more. In terms of downsides for Dalle 3, I find that the images are somewhat less realistic, and there are sometimes issues generating texts that's spelled correctly. Second is Midjourney. Head to midjourney.com to sign in. In the past, Midjourney operated primarily through Discord, although recently they rolled out a web interface that makes it more accessible. So what do I like? Midjourney is renowned for its high quality, vibrant, and cohesive images. It's user-friendly and you get back very impressive results. You also get lots of control over the image generation settings. The downside though, it's subscription-based with no free tier available, and plans start at $10 per month. Third, we have Stable Diffusion. This\")]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, several AI tools can assist in image generation, including DALL-E 3, which allows users to create images from text prompts and offers API access for automation. Microsoft's Bing Image Creator also utilizes DALL-E 3 and allows for unlimited image creation for free. Midjourney is known for its high-quality, vibrant images and offers a user-friendly experience, although it operates on a subscription basis. Stable Diffusion is another popular option for generating images.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools can assist in image generation in 2025?\", accuracy_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", hallucination_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Useful AI tools for social media post creation include Hootsuite for managing and scheduling posts, Canva for designing visuals, Copy.ai for generating engaging captions and content ideas, and Buffer for social media management and analytics.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools are useful for social media post creation?\", accuracy_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", hallucination_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools for summarizing long documents or PDFs\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The AI Summarizer of WORDVICE.AI is highly effective for summarizing long documents and PDFs. It provides concise summaries that highlight important points while maintaining thematic coherence. Users can adjust the length, mode, language, and document type of the summaries. Additionally, it offers a human edit option for enhanced credibility, making it suitable for those who need accurate and reliable summaries.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", accuracy_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", hallucination_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"Which AI tools can help improve SEO for blogs in 2025?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, some of the best AI tools for improving SEO for blogs include Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools assist content marketers in creating SEO-rich content, optimizing for search engines, and enhancing overall content quality.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", accuracy_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", hallucination_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Suggest me AI tools for content creation in 2025\", accuracy_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", hallucination_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, notable AI tools for image generation include DALL-E 3, which allows users to create images from text prompts, Midjourney for high-quality images, and Stable Diffusion for versatile image generation. Microsoft's Bing Image Creator also utilizes DALL-E 3 technology.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools can assist in image generation in 2025?\", accuracy_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", hallucination_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Useful AI tools for social media post creation include Hootsuite for managing and scheduling posts, Canva for designing visuals, Copy.ai for generating engaging captions and content ideas, and Buffer for social media management and analytics.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools are useful for social media post creation?\", accuracy_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", hallucination_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"Which AI tools are good at summarizing long documents or PDFs?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 10, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 10}, page_content='languages to choose from Let’s see how accurate this tool is. Now, I’m going to use a French passage. Here’s what I got. Now it’s time to try out the AI Summarizer tool. I read a lot of online content, especially long and detailed articles. Having a short yet relevant summary that highlights all the important points can save me a lot of time. But most tools provide vague summaries that don’t suit my interests. The AI Summarizer of WORDVICE.AI exceeded my expectations. You can see how well it has summarized the passage I provided. It included only the most important points while maintaining an overall thematic view. Plus, I can adjust the length of the summary, the mode, the language, and the document type. What I liked best is that WORDVICE.AI offers a human edit option beyond the AI Writing Assistant. The editors hold master’s and PhD degrees, which add to the credibility of their work. Choosing a plan depends on how much you write. The free plan is fine for light use. If you’re working solo, the Premium plan at $9.95 per month gives you a million words and advanced revision tools. But if you have a team, the Team plan at $8.45 per month makes more sense. Here are the factors I based my review of the above AI tools on: The most important thing for me is how well the AI generates content and whether it’s factually correct. This is a non-negotiable for SEO. I prefer tools with an intuitive interface. I shouldn’t have to go through lengthy tutorials just to use a basic grammar checker. Plus, customization options like adjusting the tone, creativity level, and formatting are a must. I work across multiple platforms. So I need an AI writing tool that integrates smoothly with different applications. I write a lot, so I find premium plans more useful. I always look for a balance between affordability and features. You might find SEO writing to be hard (which it is), but trust me, AI can make it easier. The best part is you can access all these tools right from the comfort of your home. I’ve tested multiple AI tools for writing SEO-rich blog content, and I can confidently say that the right ones can save you time, improve your creativity, and even help you create high-quality content consistently. Isn’t that awesome? Does that mean you should completely rely on AI? Nope. AI tools are simply your assistants. No AI can replace your unique voice. It’s up to you to use these tools to improve your writing. But believe me when I say that there’s an AI tool out there that fits your every need. So which tool is best for your SEO needs? I hope this breakdown of the six best AI tools for writing SEO-rich blog content helps you find the one that works best for you. So what are you waiting for? Start using these AI SEO-writing tools today to make your content')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The AI Summarizer of WORDVICE.AI is highly effective for summarizing long documents and PDFs. It provides concise summaries that highlight important points while maintaining thematic coherence. Users can adjust the length, mode, language, and document type of the summaries. Additionally, it offers a human edit option for enhanced credibility, making it suitable for those who need accurate and reliable summaries.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", accuracy_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", hallucination_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"Which AI tools can help improve SEO for blogs in 2025?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, the following AI tools can help improve SEO for blogs: Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools assist content marketers in creating SEO-rich content, optimizing for search engines, and enhancing overall content quality.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", accuracy_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", hallucination_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Suggest me AI tools for content creation in 2025\", accuracy_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", hallucination_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, notable AI tools for image generation include DALL-E 3, which allows users to create images from text prompts, Midjourney for high-quality images, and Stable Diffusion for versatile image generation. Microsoft's Bing Image Creator also utilizes DALL-E 3 technology.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools can assist in image generation in 2025?\", accuracy_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", hallucination_score=0.0\n",
            "question=\"What AI tools can assist in image generation in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Useful AI tools for social media post creation include Hootsuite for managing and scheduling posts, Canva for designing visuals, Copy.ai for generating engaging captions and content ideas, and Buffer for social media management and analytics.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"What AI tools are useful for social media post creation?\", accuracy_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", hallucination_score=0.0\n",
            "question=\"What AI tools are useful for social media post creation?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Some effective AI tools for summarizing long documents or PDFs include: 1. WORDVICE.AI - Provides concise summaries and allows users to adjust length and format. 2. SMMRY - A web-based tool that condenses text. 3. QuillBot - Offers summarization features alongside paraphrasing. 4. Resoomer - Specifically designed for summarizing articles and documents. 5. Scholarcy - Summarizes research papers and academic articles.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", accuracy_score=0.0\n",
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", hallucination_score=0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question=\"Which AI tools are good at summarizing long documents or PDFs?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools to improve SEO for blogs in 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, the following AI tools can help improve SEO for blogs: Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools assist content marketers in creating SEO-rich content, optimizing for search engines, and enhancing overall content quality.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", accuracy_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", hallucination_score=0.0\n",
            "question=\"Which AI tools can help improve SEO for blogs in 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"Suggest me AI tools for content creation in 2025\", accuracy_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", hallucination_score=0.0\n",
            "question=\"Suggest me AI tools for content creation in 2025\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI assistance for brainstorming creative content topics, campaign ideas, or trending formats for 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_'}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_', 'chunk_index': 0}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_', 'chunk_index': 0}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_', 'chunk_index': 0}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can assist you in brainstorming creative content topics, campaign ideas, and trending formats by acting as a strategic co-pilot. AI tools can help generate better ideas, accelerate your writing, and create stunning visuals. They can be used for generating content ideas, drafting sections of blog posts, creating short-form copy for landing pages, and writing video scripts. This approach allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", accuracy_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", hallucination_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can improve the SEO of your blog posts by providing tools that assist with writing SEO-rich content. Some of the best AI tools for this purpose include Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools help content marketers create optimized content, conduct keyword research, and enhance overall content quality, which can lead to higher rankings in Google searches.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", accuracy_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", hallucination_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To create video content using AI, consider the following tools: 1. **InVideo**: Great for scriptwriting and video creation with templates and stock footage. 2. **Synthesia**: Ideal for generating AI-driven videos with avatars and voiceovers. 3. **Murf**: Excellent for creating human-like voiceovers for your videos. 4. **Descript**: Useful for video editing, allowing you to edit video by editing text. 5. **Canva**: For creating engaging thumbnails and social media graphics. **Workflow**: Start by brainstorming ideas and writing scripts using InVideo or ChatGPT. Then, create videos using Synthesia or InVideo. Add voiceovers with Murf, and finally, edit the video using Descript. Use Canva for promotional graphics to share on social media.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", accuracy_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", hallucination_score=0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can help you repurpose long blog articles into short-form content by using tools like Copy.ai to generate concise summaries and engaging captions, Canva for creating visually appealing graphics, and InVideo for transforming written content into short videos. You can extract key points from your articles using AI summarization tools, then adapt these points into formats suitable for each platform, such as infographics for Instagram, short clips for TikTok, and concise posts for LinkedIn.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", accuracy_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", hallucination_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"essential AI tools for content creation in 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'www_getblend_com_blog_10-best-ai-tools-to-use-for-content-creation_'}, page_content='Title: Best AI Tools for Content Creation in 2025 | BLEND Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations There’s been a lot of talk about the rise of artificial intelligence over the past couple of years. One area where AI has come into its own is the field of content creation. For digital marketers and content creators looking to work smart, they’re a useful tool for streamlining workflows and generating ideas. For businesses, they can prove incredibly cost-effective, offering scalable content solutions for multi-channel marketing. More than 75% of marketers admit to using AI tools to some degree. What’s more, around 19% of businesses use AI tools to generate content. Even if you’re yet to be convinced about the effectiveness of AI-generated content, you’ll need to think seriously about adopting them if you want to remain competitive. To reap the full benefits of AI, you need to be using the right tools. Below, we’ll explore the advantages of this exciting technology and how to utilize it effectively. We’ve also picked out 10 of the best AI content creation tools currently on the market. Are you an AI skeptic? Below are just some of the reasons why you should be considering making AI tools a staple of your content planning and creation. Looking to get content online more quickly? According to one survey, a typical 500-word blog post takes around 4 hours to complete. This doesn’t even take into account compiling a briefing, proofreading the first draft, and waiting on any revision requests to be completed. With a content creation tool, you can streamline the process considerably. Whether you outsource writing assignments or handle content creation in-house, an AI content generator can boost productivity. These powerful tools make quick work of blog ideation, while social media posts and compelling CTAs can be produced in seconds. Looking to slash costs? Outsourcing content to freelancers is relatively inexpensive. However, expect to pay upwards of $175 for a 1,500-word article. You can use AI writing assistant tools to control your outgoings. These tools can help refine the ideation and briefing process. Alternatively, you can leverage the potential of artificial intelligence to make content creation alleviate pressures on internal teams. If you want to create and distribute content at scale, you’ll need to invest a significant amount of time and money. Alternatively, use AI tools to automate processes for a more streamlined approach to content creation. Long gone are the days when you had to rely on external support for SEO services. With the right AI tools at your disposal, you can carry out exhaustive keyword research and access optimization insights in seconds. Look for keyword trends, generate fresh and competitive content ideas, and play around with on-page elements to ensure your content is as optimized as it can be. Although current AI content creation tools require the human touch to produce'), Document(metadata={'source': 'www_getblend_com_blog_10-best-ai-tools-to-use-for-content-creation_', 'chunk_index': 0}, page_content='Title: Best AI Tools for Content Creation in 2025 | BLEND Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations There’s been a lot of talk about the rise of artificial intelligence over the past couple of years. One area where AI has come into its own is the field of content creation. For digital marketers and content creators looking to work smart, they’re a useful tool for streamlining workflows and generating ideas. For businesses, they can prove incredibly cost-effective, offering scalable content solutions for multi-channel marketing. More than 75% of marketers admit to using AI tools to some degree. What’s more, around 19% of businesses use AI tools to generate content. Even if you’re yet to be convinced about the effectiveness of AI-generated content, you’ll need to think seriously about adopting them if you want to remain competitive. To reap the full benefits of AI, you need to be using the right tools. Below, we’ll explore the advantages of this exciting technology and how to utilize it effectively. We’ve also picked out 10 of the best AI content creation tools currently on the market. Are you an AI skeptic? Below are just some of the reasons why you should be considering making AI tools a staple of your content planning and creation. Looking to get content online more quickly? According to one survey, a typical 500-word blog post takes around 4 hours to complete. This doesn’t even take into account compiling a briefing, proofreading the first draft, and waiting on any revision requests to be completed. With a content creation tool, you can streamline the process considerably. Whether you outsource writing assignments or handle content creation in-house, an AI content generator can boost productivity. These powerful tools make quick work of blog ideation, while social media posts and compelling CTAs can be produced in seconds. Looking to slash costs? Outsourcing content to freelancers is relatively inexpensive. However, expect to pay upwards of $175 for a 1,500-word article. You can use AI writing assistant tools to control your outgoings. These tools can help refine the ideation and briefing process. Alternatively, you can leverage the potential of artificial intelligence to make content creation alleviate pressures on internal teams. If you want to create and distribute content at scale, you’ll need to invest a significant amount of time and money. Alternatively, use AI tools to automate processes for a more streamlined approach to content creation. Long gone are the days when you had to rely on external support for SEO services. With the right AI tools at your disposal, you can carry out exhaustive keyword research and access optimization insights in seconds. Look for keyword trends, generate fresh and competitive content ideas, and play around with on-page elements to ensure your content is as optimized as it can be. Although current AI content creation tools require the human touch to produce'), Document(metadata={'chunk_index': 0, 'source': 'www_getblend_com_blog_10-best-ai-tools-to-use-for-content-creation_'}, page_content='Title: Best AI Tools for Content Creation in 2025 | BLEND Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations There’s been a lot of talk about the rise of artificial intelligence over the past couple of years. One area where AI has come into its own is the field of content creation. For digital marketers and content creators looking to work smart, they’re a useful tool for streamlining workflows and generating ideas. For businesses, they can prove incredibly cost-effective, offering scalable content solutions for multi-channel marketing. More than 75% of marketers admit to using AI tools to some degree. What’s more, around 19% of businesses use AI tools to generate content. Even if you’re yet to be convinced about the effectiveness of AI-generated content, you’ll need to think seriously about adopting them if you want to remain competitive. To reap the full benefits of AI, you need to be using the right tools. Below, we’ll explore the advantages of this exciting technology and how to utilize it effectively. We’ve also picked out 10 of the best AI content creation tools currently on the market. Are you an AI skeptic? Below are just some of the reasons why you should be considering making AI tools a staple of your content planning and creation. Looking to get content online more quickly? According to one survey, a typical 500-word blog post takes around 4 hours to complete. This doesn’t even take into account compiling a briefing, proofreading the first draft, and waiting on any revision requests to be completed. With a content creation tool, you can streamline the process considerably. Whether you outsource writing assignments or handle content creation in-house, an AI content generator can boost productivity. These powerful tools make quick work of blog ideation, while social media posts and compelling CTAs can be produced in seconds. Looking to slash costs? Outsourcing content to freelancers is relatively inexpensive. However, expect to pay upwards of $175 for a 1,500-word article. You can use AI writing assistant tools to control your outgoings. These tools can help refine the ideation and briefing process. Alternatively, you can leverage the potential of artificial intelligence to make content creation alleviate pressures on internal teams. If you want to create and distribute content at scale, you’ll need to invest a significant amount of time and money. Alternatively, use AI tools to automate processes for a more streamlined approach to content creation. Long gone are the days when you had to rely on external support for SEO services. With the right AI tools at your disposal, you can carry out exhaustive keyword research and access optimization insights in seconds. Look for keyword trends, generate fresh and competitive content ideas, and play around with on-page elements to ensure your content is as optimized as it can be. Although current AI content creation tools require the human touch to produce'), Document(metadata={'source': 'www_getblend_com_blog_10-best-ai-tools-to-use-for-content-creation_', 'chunk_index': 0}, page_content='Title: Best AI Tools for Content Creation in 2025 | BLEND Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations Click here to Get FAST Translations There’s been a lot of talk about the rise of artificial intelligence over the past couple of years. One area where AI has come into its own is the field of content creation. For digital marketers and content creators looking to work smart, they’re a useful tool for streamlining workflows and generating ideas. For businesses, they can prove incredibly cost-effective, offering scalable content solutions for multi-channel marketing. More than 75% of marketers admit to using AI tools to some degree. What’s more, around 19% of businesses use AI tools to generate content. Even if you’re yet to be convinced about the effectiveness of AI-generated content, you’ll need to think seriously about adopting them if you want to remain competitive. To reap the full benefits of AI, you need to be using the right tools. Below, we’ll explore the advantages of this exciting technology and how to utilize it effectively. We’ve also picked out 10 of the best AI content creation tools currently on the market. Are you an AI skeptic? Below are just some of the reasons why you should be considering making AI tools a staple of your content planning and creation. Looking to get content online more quickly? According to one survey, a typical 500-word blog post takes around 4 hours to complete. This doesn’t even take into account compiling a briefing, proofreading the first draft, and waiting on any revision requests to be completed. With a content creation tool, you can streamline the process considerably. Whether you outsource writing assignments or handle content creation in-house, an AI content generator can boost productivity. These powerful tools make quick work of blog ideation, while social media posts and compelling CTAs can be produced in seconds. Looking to slash costs? Outsourcing content to freelancers is relatively inexpensive. However, expect to pay upwards of $175 for a 1,500-word article. You can use AI writing assistant tools to control your outgoings. These tools can help refine the ideation and briefing process. Alternatively, you can leverage the potential of artificial intelligence to make content creation alleviate pressures on internal teams. If you want to create and distribute content at scale, you’ll need to invest a significant amount of time and money. Alternatively, use AI tools to automate processes for a more streamlined approach to content creation. Long gone are the days when you had to rely on external support for SEO services. With the right AI tools at your disposal, you can carry out exhaustive keyword research and access optimization insights in seconds. Look for keyword trends, generate fresh and competitive content ideas, and play around with on-page elements to ensure your content is as optimized as it can be. Although current AI content creation tools require the human touch to produce')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. ChatGPT - Great for generating ideas and drafting content. 2. Jasper - Excellent for creating marketing copy and blog posts. 3. Copy.ai - Useful for brainstorming and generating social media content. 4. Canva - Ideal for designing visuals and graphics. 5. InVideo - Best for creating video content from scripts. 6. Murf - Provides voiceovers for videos. 7. Descript - Useful for editing video and audio content. 8. Surfer SEO - Helps optimize content for search engines. 9. Frase - Assists in creating SEO-friendly content by analyzing competitors. 10. SEMrush - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", accuracy_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", hallucination_score=0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI tools for brainstorming creative content topics, campaign ideas, or trending formats for 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'www_gwi_com_blog_free-ai-tools-for-content-creation'}, page_content=\"Title: 15 Free AI Tools For Content Creation | GWI SHARE You’ve got five briefs due, a campaign launching tomorrow, and your designer is on leave. Stressful, right? That’s where AI tools can step in - to make the chaos more manageable. Marketers, strategists, and creatives are no longer just looking for tools to help them write faster, but are in desperate need of solutions that can do everything from analyzing audience behavior to creating visuals, optimizing content marketing, and more. That’s why we’ve pulled together 15 of the most useful free AI tools on the web. These tools don’t just churn out text, they help you understand your audience, streamline workflows, and make sure your content cuts through the noise. We like to think of AI as not replacing human creativity but rather supercharging it. Today’s tools can draft copy, generate imagery, transform blogs into videos in a few clicks, and even tell you exactly what your audience cares about. Say you’ve got three client campaigns going live in a week. AI tools can help you go from staring at your screen to drafting social posts, promo graphics, and SEO snippets in an afternoon, all without having to skip your lunchtime mental health walk. That kind of support is a game changer. But, as of yet, no single tool does it all. Some are built to help you be more efficient, others to get your ideas out of your mind and onto paper. The best content teams know what to use and when - that’s where this list comes in. Everything here is either completely free (yay) or comes with a generous free tier (still yay). And we’ve made sure there’s something for every stage of the content lifecycle, from idea to insight, creation to distribution. Each tool solves a specific challenge, so you’ll find a clear explanation of what it does, what it’s great at, and who it’s best for. Whether you're a freelance social manager juggling four brands, or the content lead in a fast-scaling startup, there’s a smart way to use AI to your advantage. We’re kicking things off with GWI Spark, and for good reason. While most AI tools are focused on producing content, GWI Spark helps you create the right content by starting with what your audience actually wants. Before you even type a single word, GWI Spark gives you the lowdown on who you’re talking to and what matters most to them. It’s our AI assistant designed for marketers, strategists, and anyone who wants to build smarter content using real, reliable data. Rather than pulling from the open web, GWI Spark taps into GWI’s massive global survey data of nearly a million real consumers across 50+ markets. Just ask a question in plain language and get instant, insight-packed answers you can pop straight into your campaign or pitch. With an easy chat interface and a free access tier, GWI Spark is ideal for non-experts who need fast, credible insights. Use it to shape\"), Document(metadata={'chunk_index': 0, 'source': 'www_gwi_com_blog_free-ai-tools-for-content-creation'}, page_content=\"Title: 15 Free AI Tools For Content Creation | GWI SHARE You’ve got five briefs due, a campaign launching tomorrow, and your designer is on leave. Stressful, right? That’s where AI tools can step in - to make the chaos more manageable. Marketers, strategists, and creatives are no longer just looking for tools to help them write faster, but are in desperate need of solutions that can do everything from analyzing audience behavior to creating visuals, optimizing content marketing, and more. That’s why we’ve pulled together 15 of the most useful free AI tools on the web. These tools don’t just churn out text, they help you understand your audience, streamline workflows, and make sure your content cuts through the noise. We like to think of AI as not replacing human creativity but rather supercharging it. Today’s tools can draft copy, generate imagery, transform blogs into videos in a few clicks, and even tell you exactly what your audience cares about. Say you’ve got three client campaigns going live in a week. AI tools can help you go from staring at your screen to drafting social posts, promo graphics, and SEO snippets in an afternoon, all without having to skip your lunchtime mental health walk. That kind of support is a game changer. But, as of yet, no single tool does it all. Some are built to help you be more efficient, others to get your ideas out of your mind and onto paper. The best content teams know what to use and when - that’s where this list comes in. Everything here is either completely free (yay) or comes with a generous free tier (still yay). And we’ve made sure there’s something for every stage of the content lifecycle, from idea to insight, creation to distribution. Each tool solves a specific challenge, so you’ll find a clear explanation of what it does, what it’s great at, and who it’s best for. Whether you're a freelance social manager juggling four brands, or the content lead in a fast-scaling startup, there’s a smart way to use AI to your advantage. We’re kicking things off with GWI Spark, and for good reason. While most AI tools are focused on producing content, GWI Spark helps you create the right content by starting with what your audience actually wants. Before you even type a single word, GWI Spark gives you the lowdown on who you’re talking to and what matters most to them. It’s our AI assistant designed for marketers, strategists, and anyone who wants to build smarter content using real, reliable data. Rather than pulling from the open web, GWI Spark taps into GWI’s massive global survey data of nearly a million real consumers across 50+ markets. Just ask a question in plain language and get instant, insight-packed answers you can pop straight into your campaign or pitch. With an easy chat interface and a free access tier, GWI Spark is ideal for non-experts who need fast, credible insights. Use it to shape\"), Document(metadata={'source': 'www_gwi_com_blog_free-ai-tools-for-content-creation', 'chunk_index': 0}, page_content=\"Title: 15 Free AI Tools For Content Creation | GWI SHARE You’ve got five briefs due, a campaign launching tomorrow, and your designer is on leave. Stressful, right? That’s where AI tools can step in - to make the chaos more manageable. Marketers, strategists, and creatives are no longer just looking for tools to help them write faster, but are in desperate need of solutions that can do everything from analyzing audience behavior to creating visuals, optimizing content marketing, and more. That’s why we’ve pulled together 15 of the most useful free AI tools on the web. These tools don’t just churn out text, they help you understand your audience, streamline workflows, and make sure your content cuts through the noise. We like to think of AI as not replacing human creativity but rather supercharging it. Today’s tools can draft copy, generate imagery, transform blogs into videos in a few clicks, and even tell you exactly what your audience cares about. Say you’ve got three client campaigns going live in a week. AI tools can help you go from staring at your screen to drafting social posts, promo graphics, and SEO snippets in an afternoon, all without having to skip your lunchtime mental health walk. That kind of support is a game changer. But, as of yet, no single tool does it all. Some are built to help you be more efficient, others to get your ideas out of your mind and onto paper. The best content teams know what to use and when - that’s where this list comes in. Everything here is either completely free (yay) or comes with a generous free tier (still yay). And we’ve made sure there’s something for every stage of the content lifecycle, from idea to insight, creation to distribution. Each tool solves a specific challenge, so you’ll find a clear explanation of what it does, what it’s great at, and who it’s best for. Whether you're a freelance social manager juggling four brands, or the content lead in a fast-scaling startup, there’s a smart way to use AI to your advantage. We’re kicking things off with GWI Spark, and for good reason. While most AI tools are focused on producing content, GWI Spark helps you create the right content by starting with what your audience actually wants. Before you even type a single word, GWI Spark gives you the lowdown on who you’re talking to and what matters most to them. It’s our AI assistant designed for marketers, strategists, and anyone who wants to build smarter content using real, reliable data. Rather than pulling from the open web, GWI Spark taps into GWI’s massive global survey data of nearly a million real consumers across 50+ markets. Just ask a question in plain language and get instant, insight-packed answers you can pop straight into your campaign or pitch. With an easy chat interface and a free access tier, GWI Spark is ideal for non-experts who need fast, credible insights. Use it to shape\"), Document(metadata={'source': 'www_gwi_com_blog_free-ai-tools-for-content-creation', 'chunk_index': 0}, page_content=\"Title: 15 Free AI Tools For Content Creation | GWI SHARE You’ve got five briefs due, a campaign launching tomorrow, and your designer is on leave. Stressful, right? That’s where AI tools can step in - to make the chaos more manageable. Marketers, strategists, and creatives are no longer just looking for tools to help them write faster, but are in desperate need of solutions that can do everything from analyzing audience behavior to creating visuals, optimizing content marketing, and more. That’s why we’ve pulled together 15 of the most useful free AI tools on the web. These tools don’t just churn out text, they help you understand your audience, streamline workflows, and make sure your content cuts through the noise. We like to think of AI as not replacing human creativity but rather supercharging it. Today’s tools can draft copy, generate imagery, transform blogs into videos in a few clicks, and even tell you exactly what your audience cares about. Say you’ve got three client campaigns going live in a week. AI tools can help you go from staring at your screen to drafting social posts, promo graphics, and SEO snippets in an afternoon, all without having to skip your lunchtime mental health walk. That kind of support is a game changer. But, as of yet, no single tool does it all. Some are built to help you be more efficient, others to get your ideas out of your mind and onto paper. The best content teams know what to use and when - that’s where this list comes in. Everything here is either completely free (yay) or comes with a generous free tier (still yay). And we’ve made sure there’s something for every stage of the content lifecycle, from idea to insight, creation to distribution. Each tool solves a specific challenge, so you’ll find a clear explanation of what it does, what it’s great at, and who it’s best for. Whether you're a freelance social manager juggling four brands, or the content lead in a fast-scaling startup, there’s a smart way to use AI to your advantage. We’re kicking things off with GWI Spark, and for good reason. While most AI tools are focused on producing content, GWI Spark helps you create the right content by starting with what your audience actually wants. Before you even type a single word, GWI Spark gives you the lowdown on who you’re talking to and what matters most to them. It’s our AI assistant designed for marketers, strategists, and anyone who wants to build smarter content using real, reliable data. Rather than pulling from the open web, GWI Spark taps into GWI’s massive global survey data of nearly a million real consumers across 50+ markets. Just ask a question in plain language and get instant, insight-packed answers you can pop straight into your campaign or pitch. With an easy chat interface and a free access tier, GWI Spark is ideal for non-experts who need fast, credible insights. Use it to shape\")]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can assist you in brainstorming creative content topics, campaign ideas, and trending formats by providing tools that analyze audience behavior and generate insights. These tools help you understand what your audience cares about, streamline workflows, and create content that resonates. They can draft copy, generate visuals, and even transform long-form content into engaging formats suitable for platforms like Instagram, TikTok, and LinkedIn. By leveraging these capabilities, you can efficiently produce social media posts, promotional graphics, and SEO snippets, enhancing your content strategy.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", accuracy_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", hallucination_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can improve the SEO of your blog posts by providing tools that assist with keyword research, content optimization, and readability. Specific tools include: 1. **Surfer SEO** - Analyzes top-ranking pages and provides guidelines for optimizing your content. 2. **Clearscope** - Offers keyword insights and content optimization suggestions. 3. **Frase** - Helps create SEO-friendly content by analyzing competitors and suggesting relevant topics. 4. **SEMrush** - Provides comprehensive SEO tools for keyword research, site audits, and tracking performance. 5. **Ahrefs** - Offers tools for backlink analysis and content gap analysis, helping you identify opportunities for improvement.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", accuracy_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", hallucination_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To create video content using AI, consider the following tools: 1. **InVideo**: Great for scriptwriting and video creation with templates and stock footage. 2. **Synthesia**: Ideal for generating AI-driven videos with avatars and voiceovers. 3. **Murf**: Excellent for creating human-like voiceovers for your videos. 4. **Descript**: Useful for video editing, allowing you to edit video by editing text. 5. **Canva**: For creating engaging thumbnails and social media graphics. **Workflow**: Start by brainstorming ideas and writing scripts using InVideo or ChatGPT. Then, create videos using Synthesia or InVideo. Add voiceovers with Murf, and finally, edit the video using Descript. Use Canva for promotional graphics to share on social media.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", accuracy_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", hallucination_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can help you repurpose long blog articles into short-form content by using tools like Copy.ai to generate concise summaries and engaging captions, Canva for creating visually appealing graphics, and InVideo for transforming written content into short videos. You can extract key points from your articles using AI summarization tools, then adapt these points into formats suitable for each platform, such as infographics for Instagram, short clips for TikTok, and concise posts for LinkedIn.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", accuracy_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", hallucination_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", accuracy_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", hallucination_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI assistance in brainstorming creative content topics, campaign ideas, or trending formats for 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_', 'chunk_index': 0}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_', 'chunk_index': 0}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'chunk_index': 0, 'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_'}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_', 'chunk_index': 0}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can assist you in brainstorming creative content topics, campaign ideas, and trending formats by acting as a strategic co-pilot. AI tools can help generate better ideas, accelerate your writing, and create stunning visuals. They can be used for generating content ideas, drafting sections of blog posts, creating short-form copy for landing pages, and writing video scripts. This approach allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", accuracy_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", hallucination_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can improve the SEO of your blog posts by providing tools that assist with writing SEO-rich content. Some of the best AI tools for this purpose include Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools help content marketers create optimized content, conduct keyword research, and enhance overall content quality, which can lead to higher rankings in Google searches.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", accuracy_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", hallucination_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To create video content using AI, consider the following tools: 1. **InVideo**: Great for scriptwriting and video creation with templates and stock footage. 2. **Synthesia**: Ideal for generating AI-driven videos with avatars and voiceovers. 3. **Murf**: Excellent for creating human-like voiceovers for your videos. 4. **Descript**: Useful for video editing, allowing you to edit video by editing text. 5. **Canva**: For creating engaging thumbnails and social media graphics. **Workflow**: Start by brainstorming ideas and writing scripts using InVideo or ChatGPT. Then, create videos using Synthesia or InVideo. Add voiceovers with Murf, and finally, edit the video using Descript. Use Canva for promotional graphics to share on social media.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", accuracy_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", hallucination_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can help you repurpose long blog articles into short-form content by using tools like Copy.ai to generate concise summaries and engaging captions, Canva for creating visually appealing graphics, and InVideo for transforming written content into short videos. You can extract key points from your articles using AI summarization tools, then adapt these points into formats suitable for each platform, such as infographics for Instagram, short clips for TikTok, and concise posts for LinkedIn.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", accuracy_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", hallucination_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", accuracy_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", hallucination_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"brainstorming creative content topics, campaign ideas, or trending formats for 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'source': 'www_impactplus_com_blog_ai-tools-for-content-creation', 'chunk_index': 1}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\"), Document(metadata={'source': 'www_impactplus_com_blog_ai-tools-for-content-creation', 'chunk_index': 1}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\"), Document(metadata={'chunk_index': 1, 'source': 'www_impactplus_com_blog_ai-tools-for-content-creation'}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\"), Document(metadata={'chunk_index': 1, 'source': 'www_impactplus_com_blog_ai-tools-for-content-creation'}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\")]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can assist you in brainstorming creative content topics, campaign ideas, and trending formats by enabling you to draft and summarize blog content for social media. Tools can help repurpose one piece of content into many formats, such as converting written content into animated videos or extracting viral clips from webinars and podcasts. For example, platforms like Lumen5 can convert your written content into animated videos, while tools like Descript and Recast Studio can help turn webinars, podcasts, and blogs into short-form videos, audio quotes, and social posts. Additionally, AI tools like Jasper and ChatGPT can help you generate fast drafts with brand voice support, making it easier to create consistent, high-quality content.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", accuracy_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", hallucination_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'chunk_index': 0, 'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c'}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your'), Document(metadata={'source': 'medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c', 'chunk_index': 0}, page_content='Title: I Tried 7 AI Writers. Here are the Best AI Tools for Writing SEO-Rich Blog Content. | by Anangsha Alammyan | Freelancer’s Hub | Medium Sign up Sign in Sign up Sign in We house your stories from finding clients, managing time and finances, to taking care of yourself as a freelancer. If you’ve set up a successful side hustle or built your own business, we’d LOVE to hear your stories! Featured -- 16 Listen Share My top 7 picks for the best AI for writing SEO rich blog content tools – 👉Koala AI 👉WriteHuman 👉Copymatic 👉Jenni AI 👉Rytr 👉GravityWrite 👉WORDVICE.AI · Introduction: Why AI writers?· 7 Best AI Tool for Writing SEO-rich Blog Content Every Content Marketer Needs in 2025· 1. Koala AI· 2. WriteHuman· 3. Copymatic· 4. Jenni AI· 5. Rytr· 6. GravityWrite· 7. WORDVICE.AI· How to Choose the Best AI Writing Tool Creating SEO-Rich Content· Final Thoughts: Which AI Tool is the Best for Writing SEO-Rich Blogs?· FAQs If you’re a content marketer, you know how time-consuming writing can be. You need to manage a team of writers, make sure every piece is up to the mark, and ensure your client gets the best output. Creating quality content that drives results isn’t easy. And when you’re juggling multiple platforms, your workload exponentially increases. I mean, doing competitor analysis, hunting for high-performing keywords, publishing quality posts, and working with a writing and SEO team at once is challenging. But it’s 2025. AI is completely changing the way we create and consume content. But let me be clear. AI isn’t here to replace writers. It’s only here to make our lives easier. It’s like having an assistant who speeds up the process, gives you fresh ideas, keeps track of your content calendar, and… well… does most of the grunt work for you. That way, you can focus on more important things, like scaling your business. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your writers with AI writing assistants. They can be more productive, do better SEO research, and give your client what they desire the most: website traffic and conversions. I’ve used AI for writing in so many ways. Do I blindly rely on it? Nope. When I need a quick draft, it helps me get my thoughts on paper without overthinking. When I struggle with sentence structure, it offers suggestions to make my writing smoother. When I want my content to rank better on Google, AI gives me keyword insights I might have missed. But not all AI writing assistants are the same. How do you know which one is right for you? In this article, I’ll break down the 7 best AI tools for writing SEO rich blog content, with detailed review of each one of them, so your content marketing journey becomes easier. As a content marketer, you don’t need to replace your copywriting team with AI. But it’s smart to empower your')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can improve the SEO of your blog posts by providing tools that assist with writing SEO-rich content. Some of the best AI tools for this purpose include Koala AI, WriteHuman, Copymatic, Jenni AI, Rytr, GravityWrite, and WORDVICE.AI. These tools help content marketers create optimized content, conduct keyword research, and enhance overall content quality, which can lead to higher rankings in Google searches.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", accuracy_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", hallucination_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To create video content using AI, consider the following tools: 1. **InVideo**: Great for scriptwriting and video creation with templates and stock footage. 2. **Synthesia**: Ideal for generating AI-driven videos with avatars and voiceovers. 3. **Murf**: Excellent for creating human-like voiceovers for your videos. 4. **Descript**: Useful for video editing, allowing you to edit video by editing text. 5. **Canva**: For creating engaging thumbnails and social media graphics. **Workflow**: Start by brainstorming ideas and writing scripts using InVideo or ChatGPT. Then, create videos using Synthesia or InVideo. Add voiceovers with Murf, and finally, edit the video using Descript. Use Canva for promotional graphics to share on social media.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", accuracy_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", hallucination_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can help you repurpose long blog articles into short-form content by using tools like Copy.ai to generate concise summaries and engaging captions, Canva for creating visually appealing graphics, and InVideo for transforming written content into short videos. You can extract key points from your articles using AI summarization tools, then adapt these points into formats suitable for each platform, such as infographics for Instagram, short clips for TikTok, and concise posts for LinkedIn.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", accuracy_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", hallucination_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", accuracy_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", hallucination_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"AI assistance in brainstorming creative content topics, campaign ideas, or trending formats for 2025.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 0, 'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_'}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'chunk_index': 0, 'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_'}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'chunk_index': 0, 'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_'}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent'), Document(metadata={'chunk_index': 0, 'source': 'www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_'}, page_content='Title: How to Use AI for Content Creation Categories Categories Tags Join over 100,000 business owners who receive monthly tips about maintaining a successful online presence. [sibwp_form id=1] HostPapa will use the information you provide to send you our monthly newsletter, and the latest HostPapa content and offers. You can unsubscribe at any time. View our Privacy Policy. Home / Marketing / How to Use AI for Content Creation [2025 Guide] Last modified on: July 8th, 2025 The digital world is insatiably hungry. You’re battling to be heard over the noise of 7.5 million blog posts published every single day. You need to create more, engage better, and show a real return on your efforts—all while your time and resources are stretched thin. If that sounds familiar, you’re not alone. But what if you could change the game entirely? It’s time to meet your new strategic co-pilot for 2025: Artificial Intelligence. And no, we’re not talking about robots looking to take over your job. AI is now an excellent content creation tool, and AI content creation tools are becoming more prevalent. AI content creation is the ultimate answer to the challenge of scaling your output without sacrificing quality. It’s about automating the repetitive tasks so you can focus on the big picture—strategy, creativity, and connection. This guide provides the practical workflow you need to do just that. By the end, you will know exactly how to leverage AI tools like Google’s Gemini and Midjourney to generate better ideas, accelerate your writing, and create stunning visuals, all while amplifying your unique brand voice. Ready to turn content creation from a challenge into your greatest strength? Let’s start. Before you can master how to use AI for content creation, you need to understand your toolkit. The artificial intelligence powering these content creation tools has matured significantly. We’re no longer talking about clunky chatbots but nimble AI tools that help in every way possible. Today’s Large Language Models (LLMs) are sophisticated engines built on advanced machine learning and natural language processing, capable of understanding nuance, context, and even your unique brand voice. The modern content creation process isn’t about using one magic tool. It’s about building a stack of specialized AI tools that handle different parts of your workflow. This approach to AI content creation allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity. Let’s break down the basic categories of AI tools for content creation that every marketer should know. These foundational AI tools, like Google Gemini and OpenAI’s ChatGPT, are your partners for brainstorming and drafting. AI writing tools are also neat for assessing and managing your piece’s tone. Use these conversational AI assistants to generate content ideas, draft sections of blog posts, create short-form copy for landing pages, and write video scripts. These content generation tools help massively with writer’s block, but remember the human touch: their output is a starting point that requires your expert editing to ensure accuracy and a consistent')]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can assist you in brainstorming creative content topics, campaign ideas, and trending formats by acting as a strategic co-pilot. AI tools can help generate better ideas, accelerate your writing, and create stunning visuals. They can be used for generating content ideas, drafting sections of blog posts, creating short-form copy for landing pages, and writing video scripts. This approach allows you to move faster, eliminate time-consuming tasks, and focus your energy on strategy and creativity.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", accuracy_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", hallucination_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can improve the SEO of your blog posts by providing tools that assist with keyword research, content optimization, and readability. Specific tools include: 1. **Surfer SEO** - Analyzes top-ranking pages and provides guidelines for optimizing your content. 2. **Clearscope** - Offers keyword insights and content optimization suggestions. 3. **Frase** - Helps create SEO-friendly content by analyzing competitors and suggesting relevant topics. 4. **SEMrush** - Provides comprehensive SEO tools for keyword research, site audits, and tracking performance. 5. **Ahrefs** - Offers tools for backlink analysis and content gap analysis, helping you identify opportunities for improvement.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", accuracy_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", hallucination_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To create video content using AI, consider the following tools: 1. **InVideo**: Great for scriptwriting and video creation with templates and stock footage. 2. **Synthesia**: Ideal for generating AI-driven videos with avatars and voiceovers. 3. **Murf**: Excellent for creating human-like voiceovers for your videos. 4. **Descript**: Useful for video editing, allowing you to edit video by editing text. 5. **Canva**: For creating engaging thumbnails and social media graphics. **Workflow**: Start by brainstorming ideas and writing scripts using InVideo or ChatGPT. Then, create videos using Synthesia or InVideo. Add voiceovers with Murf, and finally, edit the video using Descript. Use Canva for promotional graphics to share on social media.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", accuracy_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", hallucination_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can help you repurpose long blog articles into short-form content by using tools like Copy.ai to generate concise summaries and engaging captions, Canva for creating visually appealing graphics, and InVideo for transforming written content into short videos. You can extract key points from your articles using AI summarization tools, then adapt these points into formats suitable for each platform, such as infographics for Instagram, short clips for TikTok, and concise posts for LinkedIn.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", accuracy_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", hallucination_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", accuracy_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", hallucination_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Chroma Retriever\",\n",
            "    \"action_input\": \"brainstorming creative content topics, campaign ideas, or trending formats for 2025\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m[Document(metadata={'chunk_index': 1, 'source': 'www_impactplus_com_blog_ai-tools-for-content-creation'}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\"), Document(metadata={'chunk_index': 1, 'source': 'www_impactplus_com_blog_ai-tools-for-content-creation'}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\"), Document(metadata={'chunk_index': 1, 'source': 'www_impactplus_com_blog_ai-tools-for-content-creation'}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\"), Document(metadata={'chunk_index': 1, 'source': 'www_impactplus_com_blog_ai-tools-for-content-creation'}, page_content=\"drafts, summarize blog content for social media, develop subject lines and preview text, repurpose one piece of content into many (with the content remix) and act as a helping hand and create the right report. Use cases: Don’t have a video team? No problem. Lumen5 converts your written content into animated video in minutes, ideal for LinkedIn or email embeds. Use cases: Pull viral clips from webinars and podcasts automatically. Recast is perfect if you’re trying to make more from what you’ve already recorded. Use cases: BrandWell turns content marketing into a self-sustaining engine. It combines research, writing, optimization, and linking into one platform, saving hours of manual work while boosting visibility and authority. Ideal for teams who need consistent, high-quality content without the heavy lift. Use cases: What is the best AI tool for blog writing in 2025? Jasper and ChatGPT remain top choices for blog writing, offering fast drafts with brand voice support. Claude is ideal for long-form and sensitive content. What AI tools help with content repurposing? Descript, Recast Studio, and Lumen5 help turn webinars, podcasts, and blogs into short-form video, audio quotes, and social posts. You can have the best AI tools in the world, but without structure, process, buy-in and human oversight, you’re just publishing faster, not better. Want your content strategy to work smarter, not just faster? At IMPACT, we coach businesses just like yours in the Endless Customers System™–a proven approach that puts content at the heart of your sales and marketing. We’ll help you build a respected presence that educates buyers, builds trust, and drives real revenue. If you're ready to stop guessing and start growing your business, let’s talk. Order Your Copy of Marcus Sheridan's New Book — Endless Customers! Share: Subscribe now and get the latest podcast releases delivered straight to your inbox. Each week have the latest episode delivered directly to your inbox with the latest insights, tactics, and success stories. 125 Commerce Court, Suite 9 Cheshire, CT, 06410 (203) 265-4377 © 2025 IMPACT, All Rights Reserved.\")]\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can assist you in brainstorming creative content topics, campaign ideas, and trending formats by helping you draft and summarize blog content for social media. Tools can repurpose one piece of content into many formats, such as turning written articles into animated videos or extracting viral clips from webinars and podcasts. For example, platforms can convert your written content into animated videos, while others can help turn webinars, podcasts, and blogs into short-form videos, audio quotes, and social posts. This allows you to engage with your audience across different platforms effectively.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", accuracy_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", hallucination_score=0.0\n",
            "question=\"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can improve the SEO of your blog posts by providing tools that assist with keyword research, content optimization, and readability. Specific tools include: 1. **Surfer SEO** - Analyzes top-ranking pages and provides guidelines for optimizing your content. 2. **Clearscope** - Offers keyword insights and content optimization suggestions. 3. **Frase** - Helps create SEO-friendly content by analyzing competitors and suggesting relevant topics. 4. **SEMrush** - Provides comprehensive SEO tools for keyword research, site audits, and tracking performance. 5. **Ahrefs** - Offers tools for backlink analysis and content gap analysis, helping you identify opportunities for improvement.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", accuracy_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", hallucination_score=0.0\n",
            "question=\"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"To create video content using AI, consider the following tools: 1. **InVideo**: Great for scriptwriting and video creation with templates and stock footage. 2. **Synthesia**: Ideal for generating AI-driven videos with avatars and voiceovers. 3. **Murf**: Excellent for creating human-like voiceovers for your videos. 4. **Descript**: Useful for video editing, allowing you to edit video by editing text. 5. **Canva**: For creating engaging thumbnails and social media graphics. **Workflow**: Start by brainstorming ideas and writing scripts using InVideo or ChatGPT. Then, create videos using Synthesia or InVideo. Add voiceovers with Murf, and finally, edit the video using Descript. Use Canva for promotional graphics to share on social media.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", accuracy_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", hallucination_score=0.0\n",
            "question=\"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"AI can help you repurpose long blog articles into short-form content by using tools like Copy.ai to generate concise summaries and engaging captions, Canva for creating visually appealing graphics, and InVideo for transforming written content into short videos. You can extract key points from your articles using AI summarization tools, then adapt these points into formats suitable for each platform, such as infographics for Instagram, short clips for TikTok, and concise posts for LinkedIn.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", accuracy_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", hallucination_score=0.0\n",
            "question=\"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\", doc_relevance_score=0.0\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error grading answer accuracy: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 41, in answer_evaluator\n",
            "    score_response = await answer_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'correct_answer'}.  Expected: ['correct_answer', 'question', 'student_answer'] Received: ['question', 'reference_answer', 'predicted_answer']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading hallucination: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 71, in answer_hallucination_evaluator\n",
            "    score_response = await hallucination_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 240, in ainvoke\n",
            "    return await self._acall_with_config(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2143, in _acall_with_config\n",
            "    output: Output = await coro_with_context(coro, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 193, in _aformat_prompt_with_error_handling\n",
            "    inner_input_ = self._validate_input(inner_input)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/prompts/base.py\", line 181, in _validate_input\n",
            "    raise KeyError(\n",
            "KeyError: \"Input to StructuredPrompt is missing variables {'student_answer', 'documents'}.  Expected: ['documents', 'student_answer'] Received: ['answer', 'sources']\\nNote: if you intended {student_answer} to be part of the string and not a variable, please escape it with double curly braces like: '{{student_answer}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
            "ERROR:root:Error grading document relevance: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3675071000.py\", line 99, in docs_relevance_evaluator\n",
            "    score_response = await relevance_grader.ainvoke({\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3290, in ainvoke\n",
            "    input_ = await coro_with_context(part(), context, create_task=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
            "    return await self.bound.ainvoke(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
            "    llm_result = await self.agenerate_prompt(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1430, in _agenerate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1398, in _agenerate\n",
            "    _handle_openai_bad_request(e)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 1393, in _agenerate\n",
            "    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\", line 381, in wrapped\n",
            "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1619, in parse\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1794, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1594, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"In 2025, essential AI tools for content creation include: 1. **ChatGPT** - Great for generating ideas and drafting content. 2. **Jasper** - Excellent for creating marketing copy and blog posts. 3. **Copy.ai** - Useful for brainstorming and generating social media content. 4. **Canva** - Ideal for designing visuals and graphics. 5. **InVideo** - Best for creating video content from scripts. 6. **Murf** - Provides voiceovers for videos. 7. **Descript** - Useful for editing video and audio content. 8. **Surfer SEO** - Helps optimize content for search engines. 9. **Frase** - Assists in creating SEO-friendly content by analyzing competitors. 10. **SEMrush** - Comprehensive tool for keyword research and SEO analysis.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", accuracy_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", hallucination_score=0.0\n",
            "question=\"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\", doc_relevance_score=0.0\n",
            "\n",
            "--- Evaluation complete! ---\n",
            "Experiment name: RAG_AI_Content_Eval_20250919_075218-7d81983d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j81q7g-peSmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDkW2PoxeSje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2633eb67"
      },
      "source": [
        "import asyncio\n",
        "from langsmith import Client\n",
        "import os\n",
        "from datetime import datetime\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.tools import Tool\n",
        "from typing import Dict, Any\n",
        "import random\n",
        "\n",
        "# Assuming 'client', 'dataset', 'tools', 'llm', 'memory', 'DATASET_NAME', and 'OPENAI_API_KEY'\n",
        "# are defined in previously executed cells.\n",
        "\n",
        "# --- Define Agent Factory ---\n",
        "def create_agent():\n",
        "    \"\"\"\n",
        "    Factory function that initializes and returns the agent.\n",
        "    Assumes 'tools', 'llm', and 'memory' are available in the global scope.\n",
        "    \"\"\"\n",
        "    # Ensure required components are available\n",
        "    if 'tools' not in globals():\n",
        "        raise ValueError(\"Global 'tools' list not found.\")\n",
        "    if 'llm' not in globals():\n",
        "         raise ValueError(\"Global 'llm' object not found.\")\n",
        "    # The memory object is created within the factory, so no need to check global scope\n",
        "\n",
        "    # Re-initialize the agent within the factory\n",
        "    agent_instance = initialize_agent(\n",
        "        tools=tools,\n",
        "        llm=llm,\n",
        "        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "        memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True), # Initialize memory here if needed per run\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True\n",
        "    )\n",
        "    return agent_instance\n",
        "\n",
        "# --- Input Mapper for Dataset ---\n",
        "def map_inputs(example_inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Maps dataset example inputs ('question') to the agent's expected input format ('input').\"\"\"\n",
        "    return {\"input\": example_inputs.get(\"question\", \"\")}\n",
        "\n",
        "\n",
        "# --- Run Agent on Dataset ---\n",
        "async def run_evaluation_dataset():\n",
        "    \"\"\"\n",
        "    Runs the agent on the specified dataset and logs results to LangSmith.\n",
        "    \"\"\"\n",
        "    if 'client' not in globals() or client is None:\n",
        "         raise ValueError(\"LangSmith client not found or is None. Ensure it's initialized.\")\n",
        "    if 'dataset' not in globals() or dataset is None:\n",
        "         raise ValueError(\"Dataset object not found or is None. Ensure it's created in a previous cell.\")\n",
        "    if 'DATASET_NAME' not in globals():\n",
        "         raise ValueError(\"DATASET_NAME is not defined.\")\n",
        "\n",
        "\n",
        "    print(f\"Running agent on dataset '{dataset.name}'...\")\n",
        "\n",
        "    # Use client.run_on_dataset with the factory function and input_mapper\n",
        "    # Make the experiment prefix more unique\n",
        "    experiment_prefix=f\"RunOnDataset_AI_Content_Eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}\"\n",
        "    project_name=os.environ.get(\"LANGCHAIN_PROJECT\", \"default\")\n",
        "\n",
        "    results = await client.run_on_dataset(\n",
        "        dataset_name=DATASET_NAME,\n",
        "        llm_or_chain_factory=create_agent, # Pass the factory function\n",
        "        input_mapper=map_inputs, # Pass the input mapper function\n",
        "        experiment_prefix=experiment_prefix,\n",
        "        project_name=project_name,\n",
        "        concurrency=1, # Start with concurrency 1 to avoid potential memory issues with ConversationBufferMemory per run\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Evaluation run complete! ---\")\n",
        "    try:\n",
        "        if hasattr(results, 'url'):\n",
        "             print(f\"View results on LangSmith: {results.url}\")\n",
        "        elif isinstance(results, list) and len(results) > 0 and hasattr(results[0], 'url'):\n",
        "             # Fallback for older versions or different structures\n",
        "             print(f\"View results on LangSmith (first run URL): {results[0].url.rsplit('/', 1)[0]}\") # Get experiment URL from run URL\n",
        "        else:\n",
        "             print(\"Results logged to LangSmith. Check your project dashboard.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate LangSmith URL: {e}\")\n",
        "        print(\"Results logged to LangSmith. Check your project dashboard.\")\n",
        "\n",
        "# Run the async function\n",
        "await run_evaluation_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Module Docstring ---\n",
        "# \"\"\"\n",
        "# This script creates a question-answer dataset for AI content creation in LangSmith,\n",
        "# fetches the API key from Colab secrets, sets environment variables, defines specific\n",
        "# Q&A pairs, uploads them to LangSmith, and evaluates agent responses using multiple\n",
        "# custom evaluators (accuracy, hallucination, document relevance).\n",
        "# \"\"\"\n",
        "\n",
        "# # --- Standard library imports ---\n",
        "# import logging\n",
        "# import os\n",
        "# from typing import List, Dict, Any\n",
        "# from datetime import datetime\n",
        "# import asyncio\n",
        "\n",
        "# # --- Third-party imports ---\n",
        "# from langsmith import Client\n",
        "# from langsmith.evaluation import aevaluate\n",
        "# from langchain import hub\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# from langchain_core.documents import Document\n",
        "# from dotenv import load_dotenv\n",
        "# from langchain_core.runnables import RunnableLambda # Import RunnableLambda\n",
        "\n",
        "# # --- Colab secret handling ---\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # --- Load environment variables from .env file (optional for local dev) ---\n",
        "# load_dotenv()\n",
        "\n",
        "# # --- Configure logging ---\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# # --- Load LangSmith API key from Colab secrets ---\n",
        "# langsmith_api_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "# if not langsmith_api_key:\n",
        "#     raise ValueError(\"❌ No LangSmith API key found in Colab secrets!\")\n",
        "\n",
        "# # --- Set environment variables for LangSmith ---\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# logging.info(\"✅ LangSmith API key loaded and environment configured.\")\n",
        "\n",
        "# # --- Initialize LangSmith Client ---\n",
        "# client = Client()\n",
        "\n",
        "# # --- Dataset Creation with Conflict Handling ---\n",
        "# DATASET_NAME = \"rag_eval_dataset\"\n",
        "\n",
        "# try:\n",
        "#     dataset = client.read_dataset(dataset_name=DATASET_NAME)\n",
        "#     logging.info(f\"⚠️ Dataset '{DATASET_NAME}' already exists. Using existing dataset.\")\n",
        "# except Exception as e:\n",
        "#     logging.info(f\"Dataset '{DATASET_NAME}' not found. Creating a new one...\")\n",
        "#     dataset = client.create_dataset(\n",
        "#         dataset_name=DATASET_NAME,\n",
        "#         description=\"RAG evaluation dataset for AI content creation\"\n",
        "#     )\n",
        "#     logging.info(f\"✅ Dataset '{DATASET_NAME}' created successfully.\")\n",
        "\n",
        "\n",
        "# --- Import chatbot backend components ---\n",
        "# try:\n",
        "#     from chatbot_backend import agent, retriever\n",
        "#     logging.info(\"Successfully imported agent and retriever from chatbot_backend.py\")\n",
        "# except ImportError as e:\n",
        "#     logging.error(f\"Failed to import components from chatbot_backend.py: {e}\")\n",
        "#     exit(\"Exiting: Could not load core chatbot components. Check chatbot_backend.py.\")\n",
        "\n",
        "# Assuming agent and retriever are defined in previous cells in this notebook\n",
        "# Ensure the agent object is named 'agent' and the retriever is named 'retriever'\n",
        "\n",
        "# --- Async helper to retrieve documents ---\n",
        "async def get_relevant_documents(query: str, retriever_instance) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Retrieves relevant documents using the provided retriever instance.\n",
        "    \"\"\"\n",
        "    if retriever_instance is None:\n",
        "         raise ValueError(\"Retriever instance is None.\")\n",
        "    return await retriever_instance.ainvoke(query)\n",
        "\n",
        "# --- Load evaluation prompts from LangChain Hub ---\n",
        "grade_prompt_hallucinations = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
        "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
        "grade_prompt_answer = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
        "\n",
        "# --- Custom Evaluator Functions ---\n",
        "\n",
        "def answer_evaluator(run, example) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of the generated answer against reference answer.\n",
        "    \"\"\"\n",
        "    input_question = example.inputs.get(\"question\", \"\")\n",
        "    correct_answer = example.outputs.get(\"answer\", \"\")\n",
        "    prediction = run.outputs.get(\"answer\", \"\")\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "    answer_grader = grade_prompt_answer | llm\n",
        "\n",
        "    try:\n",
        "        # LangChain Expression Language (LCEL) invoke\n",
        "        score_response = answer_grader.invoke({\n",
        "            \"question\": input_question,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"student_answer\": prediction\n",
        "        })\n",
        "        # Extracting the score from the response structure\n",
        "        # Assuming the response has a 'Score' key with 'YES' or 'NO'\n",
        "        raw_score_text = score_response.content.strip().lower() # Access content attribute for ChatMessage\n",
        "        score = 1.0 if raw_score_text.startswith(\"yes\") else 0.0 # Check if it starts with 'yes'\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error grading answer accuracy: {e}\", exc_info=True)\n",
        "        score = 0.0\n",
        "\n",
        "    print(f'question=\"{input_question}\", score={score}')\n",
        "    return {\"key\": \"answer_accuracy\", \"score\": score}\n",
        "\n",
        "\n",
        "def answer_hallucination_evaluator(run, example) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates if the generated answer contains hallucinated information.\n",
        "    \"\"\"\n",
        "    input_question = example.inputs.get(\"question\", \"\")\n",
        "    # Ensure 'contexts' is correctly extracted from the run outputs\n",
        "    # The agent run might return outputs in a different structure\n",
        "    # Based on the agent definition, the output might be in 'output' key, not 'contexts'\n",
        "    # If contexts are needed, the agent run structure needs to provide them\n",
        "    # For this evaluator, we need the retrieved documents which might not be in agent.run output directly\n",
        "    # A better approach might be to retrieve contexts separately or modify the agent output\n",
        "    # For now, let's assume the agent run output contains 'contexts' key from the retriever tool's output\n",
        "    # Correcting to access contexts from the custom prediction function's output\n",
        "    contexts = run.outputs.get(\"contexts\", [])\n",
        "\n",
        "    prediction = run.outputs.get(\"answer\", \"\") # Assuming final answer is in 'answer' key\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "    answer_grader = grade_prompt_hallucinations | llm\n",
        "\n",
        "    try:\n",
        "        score_response = answer_grader.invoke({\n",
        "            \"documents\": contexts,\n",
        "            \"student_answer\": prediction\n",
        "        })\n",
        "        raw_score_text = score_response.content.strip().lower() # Access content attribute for ChatMessage\n",
        "        score = 1.0 if raw_score_text.startswith(\"yes\") else 0.0 # Check if it starts with 'yes'\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error grading hallucination: {e}\", exc_info=True)\n",
        "        score = 0.0\n",
        "\n",
        "    print(f'question=\"{input_question}\", hallucination_score={score}')\n",
        "    return {\"key\": \"answer_hallucination\", \"score\": score}\n",
        "\n",
        "\n",
        "def docs_relevance_evaluator(run, example) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates if the retrieved documents are relevant to the input question.\n",
        "    \"\"\"\n",
        "    input_question = example.inputs.get(\"question\", \"\")\n",
        "    # Similar assumption as above: 'contexts' is in the agent run output\n",
        "    # Correcting to access contexts from the custom prediction function's output\n",
        "    contexts = run.outputs.get(\"contexts\", [])\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "    answer_grader = grade_prompt_doc_relevance | llm\n",
        "\n",
        "    try:\n",
        "        score_response = answer_grader.invoke({\n",
        "            \"question\": input_question,\n",
        "            \"documents\": contexts\n",
        "        })\n",
        "        raw_score_text = score_response.content.strip().lower() # Access content attribute for ChatMessage\n",
        "        score = 1.0 if raw_score_text.startswith(\"yes\") else 0.0 # Check if it starts with 'yes'\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error grading document relevance: {e}\", exc_info=True)\n",
        "        score = 0.0\n",
        "\n",
        "    print(f'question=\"{input_question}\", doc_relevance_score={score}')\n",
        "    return {\"key\": \"document_relevance\", \"score\": score}\n",
        "\n",
        "\n",
        "# --- Async Prediction Function ---\n",
        "async def predict_agent_run_for_eval(inputs: Dict[str, Any], agent_instance, retriever_instance) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Uses provided agent and retriever instances to answer a question and provide retrieved contexts.\n",
        "    Takes inputs dictionary directly from example.inputs.\n",
        "    \"\"\"\n",
        "    question = inputs.get(\"question\")\n",
        "    if not question:\n",
        "        logging.error(\"Question key missing from inputs dictionary.\")\n",
        "        return {\"answer\": \"Error: Question missing.\", \"contexts\": []}\n",
        "\n",
        "    logging.info(f\"Evaluating question with agent: {question}\")\n",
        "\n",
        "    # Retrieve documents asynchronously\n",
        "    retrieved_docs_strings = []\n",
        "    try:\n",
        "        if retriever_instance is None:\n",
        "             raise ValueError(\"Retriever instance is None.\")\n",
        "        # Ensure retriever_instance is a runnable with ainvoke\n",
        "        if not hasattr(retriever_instance, 'ainvoke'):\n",
        "             logging.error(\"Retriever instance does not have an 'ainvoke' method.\")\n",
        "             retrieved_docs = []\n",
        "        else:\n",
        "            retrieved_docs = await retriever_instance.ainvoke(question)\n",
        "        retrieved_docs_strings = [doc.page_content for doc in retrieved_docs]\n",
        "    except Exception as e: # Catch broader exceptions during retrieval\n",
        "        logging.warning(f\"Failed to retrieve documents for evaluation: {e}\")\n",
        "\n",
        "\n",
        "    # Generate answer asynchronously using the provided agent instance\n",
        "    try:\n",
        "        if agent_instance is None:\n",
        "             raise ValueError(\"Agent instance is None.\")\n",
        "        # Ensure agent_instance is a runnable with ainvoke\n",
        "        if not hasattr(agent_instance, 'ainvoke'):\n",
        "             logging.error(\"Agent instance does not have an 'ainvoke' method.\")\n",
        "             final_answer = \"Error: Agent instance does not have 'ainvoke'.\"\n",
        "        else:\n",
        "            # Invoke the agent with the question\n",
        "            # The agent expects {\"input\": question}\n",
        "            response = await agent_instance.ainvoke({\"input\": question})\n",
        "            final_answer = response.get(\"output\", \"No answer generated.\") # Assuming final answer is in 'output' key\n",
        "\n",
        "        # Return both the answer and the separately retrieved contexts for evaluators\n",
        "        return {\"answer\": final_answer, \"contexts\": retrieved_docs_strings}\n",
        "\n",
        "    except Exception as e: # Catch broader exceptions during agent execution\n",
        "        logging.error(f\"Error during agent execution for evaluation: {e}\", exc_info=True)\n",
        "        return {\"answer\": f\"Error during agent execution: {e}\", \"contexts\": retrieved_docs_strings}\n",
        "\n",
        "\n",
        "# Wrap the async prediction function in a RunnableLambda\n",
        "# This runnable will be the target for aevaluate\n",
        "runnable_predict = RunnableLambda(lambda inputs: predict_agent_run_for_eval(inputs, agent, retriever))\n",
        "\n",
        "\n",
        "# --- Run Evaluation ---\n",
        "dataset_name = DATASET_NAME  # Ensure this matches the dataset created above\n",
        "\n",
        "async def main():\n",
        "    # Ensure the 'dataset' object is accessible in this scope\n",
        "    if 'dataset' not in globals() or dataset is None:\n",
        "         raise ValueError(\"Dataset object not found or is None. Ensure it's created in a previous cell.\")\n",
        "    # Ensure 'agent' and 'retriever' objects are accessible\n",
        "    if 'agent' not in globals() or agent is None:\n",
        "        raise ValueError(\"Agent object not found or is None. Ensure it's initialized.\")\n",
        "    if 'retriever' not in globals() or retriever is None:\n",
        "        raise ValueError(\"Retriever object not found or is None. Ensure it's initialized.\")\n",
        "\n",
        "\n",
        "    experiment_results = await aevaluate(\n",
        "        # Pass the RunnableLambda as the target\n",
        "        runnable_predict,\n",
        "        data=dataset.id, # Pass the dataset ID for evaluation\n",
        "        evaluators=[\n",
        "            answer_evaluator,\n",
        "            answer_hallucination_evaluator,\n",
        "            docs_relevance_evaluator\n",
        "        ],\n",
        "        experiment_prefix=f\"RAG_AI_Content_Eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        metadata={\n",
        "            \"variant\": \"Evaluation with specific questions from LangSmith dataset (RunnableLambda target)\",\n",
        "            \"source_dataset_name\": dataset_name\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Evaluation complete! ---\")\n",
        "    try:\n",
        "        print(f\"View results on LangSmith: {experiment_results.url}\")\n",
        "    except AttributeError:\n",
        "        print(f\"Experiment name: {experiment_results.experiment_name}\")\n",
        "\n",
        "\n",
        "# Directly await main() since Colab has a running event loop\n",
        "await main()"
      ],
      "metadata": {
        "id": "MEsGnEnYVoqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a388781e"
      },
      "source": [
        "import asyncio\n",
        "from langsmith import Client\n",
        "import os\n",
        "from datetime import datetime\n",
        "from langchain.agents import initialize_agent, AgentType # Assuming these are needed for the factory\n",
        "from langchain_openai import ChatOpenAI # Assuming this is needed for the factory\n",
        "from langchain.memory import ConversationBufferMemory # Assuming this is needed for the factory\n",
        "from langchain.tools import Tool # Assuming this is needed for the factory\n",
        "from typing import Dict, Any # Import Dict and Any for type hinting\n",
        "\n",
        "# Assuming 'client', 'dataset', 'tools', 'llm', and 'memory' objects are available from previous cells\n",
        "\n",
        "# --- Define Agent Factory ---\n",
        "def create_agent():\n",
        "    \"\"\"\n",
        "    Factory function that initializes and returns the agent.\n",
        "    Assumes 'tools', 'llm', and 'memory' are available in the global scope.\n",
        "    \"\"\"\n",
        "    # Ensure required components are available\n",
        "    if 'tools' not in globals():\n",
        "        raise ValueError(\"Global 'tools' list not found.\")\n",
        "    if 'llm' not in globals():\n",
        "         raise ValueError(\"Global 'llm' object not found.\")\n",
        "    # The memory object is created within the factory, so no need to check global scope\n",
        "\n",
        "    # Re-initialize the agent within the factory\n",
        "    # Note: Memory might behave differently when initialized inside a factory\n",
        "    # for each run. Consider how you want memory to be handled across examples.\n",
        "    # For simple evaluation on independent examples, a new memory per run is fine.\n",
        "    agent_instance = initialize_agent(\n",
        "        tools=tools,\n",
        "        llm=llm,\n",
        "        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "        memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True), # Initialize memory here if needed per run\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True\n",
        "    )\n",
        "    return agent_instance\n",
        "\n",
        "# --- Input Mapper for Dataset ---\n",
        "def map_inputs(example_inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Maps dataset example inputs ('question') to the agent's expected input format ('input').\"\"\"\n",
        "    # The agent expects 'input' for the current query and 'chat_history'.\n",
        "    # run_on_dataset handles the 'chat_history' for conversational agents.\n",
        "    # We just need to map the dataset's 'question' key to the agent's 'input' key.\n",
        "    return {\"input\": example_inputs.get(\"question\", \"\")}\n",
        "\n",
        "\n",
        "# --- Run Agent on Dataset ---\n",
        "async def run_evaluation_dataset():\n",
        "    \"\"\"\n",
        "    Runs the agent on the specified dataset and logs results to LangSmith.\n",
        "    \"\"\"\n",
        "    if 'client' not in globals() or client is None:\n",
        "         raise ValueError(\"LangSmith client not found or is None. Ensure it's initialized.\")\n",
        "    if 'dataset' not in globals() or dataset is None:\n",
        "         raise ValueError(\"Dataset object not found or is None. Ensure it's created in a previous cell.\")\n",
        "\n",
        "    print(f\"Running agent on dataset '{dataset.name}'...\")\n",
        "\n",
        "    # Use client.run_on_dataset with the factory function and input_mapper\n",
        "    results = await client.run_on_dataset(\n",
        "        dataset_name=dataset.name,\n",
        "        llm_or_chain_factory=create_agent, # Pass the factory function\n",
        "        input_mapper=map_inputs, # Pass the input mapper function\n",
        "        experiment_prefix=f\"RunOnDataset_AI_Content_Eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        project_name=os.environ.get(\"LANGCHAIN_PROJECT\", \"default\"), # Use the project name from environment\n",
        "        concurrency=1, # Start with concurrency 1 to avoid potential memory issues with ConversationBufferMemory per run\n",
        "        verbose=True,\n",
        "        # This argument might also be required depending on langsmith version/setup\n",
        "        # project_kwargs={} # Add any project specific kwargs here if needed\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Evaluation run complete! ---\")\n",
        "    try:\n",
        "        # Print the URL to view the experiment results in LangSmith\n",
        "        # The structure of 'results' might vary slightly based on langsmith version\n",
        "        # Attempt to find a URL in the response\n",
        "        if hasattr(results, 'url'):\n",
        "             print(f\"View results on LangSmith: {results.url}\")\n",
        "        elif isinstance(results, list) and len(results) > 0 and hasattr(results[0], 'url'):\n",
        "             # Fallback for older versions or different structures\n",
        "             print(f\"View results on LangSmith (first run URL): {results[0].url.rsplit('/', 1)[0]}\") # Get experiment URL from run URL\n",
        "        else:\n",
        "             print(\"Results logged to LangSmith. Check your project dashboard.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate LangSmith URL: {e}\")\n",
        "        print(\"Results logged to LangSmith. Check your project dashboard.\")\n",
        "\n",
        "# Run the async function\n",
        "await run_evaluation_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06db6095"
      },
      "source": [
        "After the execution completes, click the link in the output to view the traces in LangSmith. You can then manually inspect each run to see the agent's steps, the retrieved documents, and the final answer, and compare them to the expected output in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bt-FvTZZVony"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8HN1q30VolQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sYBOSQKdVojC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ayc_K2OKVoge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDifddgIVodv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YynPQ_A6Voau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import logging\n",
        "import os\n",
        "import asyncio\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_core.documents import Document\n",
        "from langsmith.evaluation import aevaluate\n",
        "\n",
        "# --- Import chatbot backend components ---\n",
        "try:\n",
        "    from chatbot_backend import agent, retriever\n",
        "    logging.info(\"✅ Successfully imported agent and retriever\")\n",
        "except ImportError as e:\n",
        "    logging.error(f\"❌ Failed to import from chatbot_backend.py: {e}\")\n",
        "    raise SystemExit(\"Exiting: Could not load core chatbot components.\")\n",
        "\n",
        "# --- Ensure API key is set ---\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
        "    raise ValueError(\"❌ OPENAI_API_KEY not set. Please load it into environment variables.\")\n",
        "\n",
        "# --- Async helper to retrieve documents ---\n",
        "async def get_relevant_documents(query: str) -> List[Document]:\n",
        "    if retriever is None:\n",
        "        raise ValueError(\"Retriever is None. Ensure chatbot_backend.py initializes it correctly.\")\n",
        "    return await retriever.ainvoke(query)\n",
        "\n",
        "# --- Load evaluation prompts from LangChain Hub ---\n",
        "grade_prompt_hallucinations = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
        "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
        "grade_prompt_answer = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
        "\n",
        "# --- Evaluator functions here (same as your version) ---\n",
        "# answer_evaluator(...)\n",
        "# answer_hallucination_evaluator(...)\n",
        "# docs_relevance_evaluator(...)\n",
        "\n",
        "# --- Prediction function ---\n",
        "async def predict_agent_run_for_eval(example: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    question = example.get(\"question\")\n",
        "    if not question:\n",
        "        logging.error(\"Question key missing from example inputs.\")\n",
        "        return {\"answer\": \"Error: Question missing.\", \"contexts\": []}\n",
        "\n",
        "    logging.info(f\"Evaluating question: {question}\")\n",
        "\n",
        "    retrieved_docs_strings = []\n",
        "    try:\n",
        "        retrieved_docs = await get_relevant_documents(question)\n",
        "        retrieved_docs_strings = [doc.page_content for doc in retrieved_docs]\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Failed to retrieve documents: {e}\")\n",
        "\n",
        "    try:\n",
        "        response = await agent.ainvoke({\"input\": question})\n",
        "        final_answer = response.get(\"output\", \"No answer generated.\")\n",
        "        return {\"answer\": final_answer, \"contexts\": retrieved_docs_strings}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during agent execution: {e}\", exc_info=True)\n",
        "        return {\"answer\": f\"Error during agent execution: {e}\", \"contexts\": retrieved_docs_strings}\n",
        "\n",
        "# --- Run Evaluation ---\n",
        "dataset_name = \"rag_eval_dataset\"  # 👈 update with your dataset name in LangSmith\n",
        "\n",
        "async def main():\n",
        "    experiment_results = await aevaluate(\n",
        "        predict_agent_run_for_eval,\n",
        "        data=dataset_name,\n",
        "        evaluators=[\n",
        "            answer_evaluator,\n",
        "            answer_hallucination_evaluator,\n",
        "            docs_relevance_evaluator,\n",
        "        ],\n",
        "        experiment_prefix=f\"RAG_AI_Content_Eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        metadata={\"variant\": \"Evaluation with specific questions\", \"source_dataset_name\": dataset_name},\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Evaluation complete! ---\")\n",
        "    try:\n",
        "        print(f\"View results on LangSmith: {experiment_results.url}\")\n",
        "    except AttributeError:\n",
        "        print(f\"Experiment name: {experiment_results.experiment_name}\")\n",
        "\n",
        "# 👇 In Jupyter/Colab, call this in a separate cell:\n",
        "# await main()\n"
      ],
      "metadata": {
        "id": "HiGaqf0re6Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ================ BACK UP =================="
      ],
      "metadata": {
        "id": "e2pG0Wj0X81R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Module Docstring ---\n",
        "# \"\"\"\n",
        "# This script creates a question-answer dataset for everything related to AI content creation in LangSmith.\n",
        "# It fetches the API key from Colab secrets, sets the environment variables,\n",
        "# defines a set of specific Q&A pairs, and uploads them to LangSmith.\n",
        "# \"\"\"\n",
        "\n",
        "# # --- Standard library imports ---\n",
        "# import logging\n",
        "# import os\n",
        "# from typing import List, Dict\n",
        "\n",
        "# # --- Colab secret handling ---\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # --- Third-party imports ---\n",
        "# from langsmith import Client\n",
        "\n",
        "# # --- Configure logging ---\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# # --- Load LangSmith API key from Colab secrets ---\n",
        "# langsmith_api_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# if not langsmith_api_key:\n",
        "#     raise ValueError(\"❌ No LangSmith API key found in Colab secrets!\")\n",
        "\n",
        "# # Set environment variables for LangSmith\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "\n",
        "# logging.info(\"✅ LangSmith API key loaded and environment configured.\")\n",
        "\n",
        "# # --- Initialize LangSmith Client ---\n",
        "# client = Client()\n",
        "\n",
        "# # Define dataset name\n",
        "# DATASET_NAME = \"rag_eval_dataset\"\n",
        "\n",
        "# # --- Define Q&A pairs ---\n",
        "# qa_pairs: List[Dict[str, Dict[str, str]]] = [\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I’m starting as an AI content creator in 2025. Can you list the most essential AI tools I should use, and explain what each one is best for?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"As a beginner AI content creator in 2025, here are some must-have tools and their strengths:\\n\"\n",
        "#                 \"- ChatGPT (OpenAI): Best for idea generation, drafting scripts, and long-form blog posts.\\n\"\n",
        "#                 \"- Jasper: Focused on marketing copy — great for ad campaigns, landing pages, and social posts.\\n\"\n",
        "#                 \"- MidJourney: Specialized in generating high-quality AI images, thumbnails, and visual assets.\\n\"\n",
        "#                 \"- Runway: A leader in AI video editing and generative video production.\\n\"\n",
        "#                 \"- Grammarly AI: Ensures writing is clear, polished, and grammatically correct.\\n\"\n",
        "#                 \"- Notion AI: Enhances productivity by organizing ideas, outlines, and workflows.\"\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I write long blog articles. How can AI help me repurpose them into short-form content for social media platforms like Instagram, TikTok, and LinkedIn?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"AI can transform a single blog post into multiple content formats:\\n\"\n",
        "#                 \"- Copy.ai / Jasper: Summarize sections into captions, taglines, or tweet threads.\\n\"\n",
        "#                 \"- ChatGPT: Reframe insights into engaging Q&A carousels or LinkedIn posts.\\n\"\n",
        "#                 \"- Canva AI: Turn text into visually appealing infographics or slides.\\n\"\n",
        "#                 \"- MidJourney: Create supporting visuals (e.g., diagrams, story slides).\\n\"\n",
        "#                 \"- Pictory AI: Convert blogs into video scripts and short-form content for TikTok/Reels.\"\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I want to make video content using AI. Which tools should I use for scriptwriting, video generation, and editing, and how do they fit together in a workflow?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"For a complete AI-powered video workflow in 2025:\\n\"\n",
        "#                 \"1. Scriptwriting → ChatGPT or Jasper for scripts, hooks, and CTAs.\\n\"\n",
        "#                 \"2. Video Generation → Runway Gen-3 for cinematic AI video; Pika Labs for short animations.\\n\"\n",
        "#                 \"3. Avatar & Narration → Synthesia for talking avatars, ElevenLabs for natural voiceovers.\\n\"\n",
        "#                 \"4. Editing → Descript to edit videos by text, auto-generate subtitles, and polish content.\"\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"How can AI improve the SEO of my blog posts and ensure they rank higher in Google searches? Can you name specific tools and how they help?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"AI-powered SEO tools to boost rankings:\\n\"\n",
        "#                 \"- SurferSEO: Recommends keywords, structure, and length based on competitors.\\n\"\n",
        "#                 \"- Clearscope: Uses NLP to include semantically related terms.\\n\"\n",
        "#                 \"- SEMRush AI Assistant: Tracks keyword rankings and suggests updates.\\n\"\n",
        "#                 \"- MarketMuse: Identifies content gaps for untapped opportunities.\\n\"\n",
        "#                 \"- ChatGPT (SEO plugins): Drafts keyword-rich meta descriptions and FAQ sections.\"\n",
        "#             )\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         \"inputs\": {\n",
        "#             \"question\": (\n",
        "#                 \"I often struggle to come up with new content ideas. How can AI assist me in brainstorming creative content topics, campaign ideas, or trending formats for 2025?\"\n",
        "#             )\n",
        "#         },\n",
        "#         \"outputs\": {\n",
        "#             \"answer\": (\n",
        "#                 \"AI can generate fresh, trend-driven ideas:\\n\"\n",
        "#                 \"- ChatGPT: Brainstorms article titles, podcast themes, and outlines.\\n\"\n",
        "#                 \"- Jasper: Creates ad campaign slogans and A/B test variants.\\n\"\n",
        "#                 \"- Copy.ai: Produces multiple versions of headlines and hooks.\\n\"\n",
        "#                 \"- BuzzSumo + AI: Finds trending topics, viral posts, and competitor insights.\\n\"\n",
        "#                 \"- Notion AI: Organizes brainstormed ideas into campaigns, schedules, and task lists.\"\n",
        "#             )\n",
        "#         }\n",
        "#     }\n",
        "# ]"
      ],
      "metadata": {
        "id": "F9KhaabjO3oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check if dataset exists or create it\n",
        "# dataset = client.create_dataset(dataset_name=\"RAG_eval_dataset\", description=\"RAG evaluation dataset for AI content creation\")\n",
        "# logging.info(f\"✅ Dataset '{dataset.name}' is ready in LangSmith.\")"
      ],
      "metadata": {
        "id": "YSksxKZTQVu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Create or load dataset ---\n",
        "# try:\n",
        "#     dataset = client.read_dataset(dataset_name=DATASET_NAME)\n",
        "#     logging.info(\"📂 Dataset '%s' already exists with ID: %s\", DATASET_NAME, dataset.id)\n",
        "# except Exception as err:\n",
        "#     logging.info(\"📂 Dataset '%s' not found. Creating a new one... (Error: %s)\", DATASET_NAME, err)\n",
        "#     dataset = client.create_dataset(\n",
        "#         dataset_name=DATASET_NAME,\n",
        "#         description=\"Dataset for evaluating AI content creation tools and strategies.\"\n",
        "#     )\n",
        "#     # Add examples\n",
        "#     for qa_pair in qa_pairs:\n",
        "#         client.create_example(inputs=qa_pair[\"inputs\"], outputs=qa_pair[\"outputs\"], dataset_id=dataset.id)\n",
        "#     logging.info(\"✅ Added %d examples to dataset '%s'.\", len(qa_pairs), DATASET_NAME)"
      ],
      "metadata": {
        "id": "Yk5n0GG1O3jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.callbacks import LangChainTracer\n",
        "# from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# tracer = LangChainTracer()\n",
        "# agent_with_tracing = initialize_agent(\n",
        "#     tools=tools,\n",
        "#     llm=llm,\n",
        "#     agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, #ZERO_SHOT_REACT_DESCRIPTION\n",
        "#     memory=memory,\n",
        "#     verbose=True,\n",
        "#     handle_parsing_errors=True,\n",
        "#     callbacks=[tracer]   # 👈 now every run is logged to LangSmith\n",
        "# )"
      ],
      "metadata": {
        "id": "V8Jh0Cz8MIAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}