{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "========================= READING THE DATA =========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZZWvMQ9Lk_r",
        "outputId": "c2d955b0-3a0c-49bb-b0fd-6529ed531e33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully read 15 URLs from 'youtube_urls.txt'.\n",
            "\n",
            "Processing 15 YouTube URLs...\n",
            "Successfully read 10 URLs from 'web_urls.txt'.\n",
            "Processing https://www.sprinklr.com/blog/ai-social-media-content-creation/...\n",
            "Successfully scraped and saved to articles\\www_sprinklr_com_blog_ai-social-media-content-creation_.txt\n",
            "Processing https://www.gwi.com/blog/free-ai-tools-for-content-creation...\n",
            "Successfully scraped and saved to articles\\www_gwi_com_blog_free-ai-tools-for-content-creation.txt\n",
            "Processing https://www.getblend.com/blog/10-best-ai-tools-to-use-for-content-creation/...\n",
            "Successfully scraped and saved to articles\\www_getblend_com_blog_10-best-ai-tools-to-use-for-content-creation_.txt\n",
            "Processing https://www.hostpapa.com/blog/marketing/how-to-use-ai-for-content-creation/...\n",
            "Successfully scraped and saved to articles\\www_hostpapa_com_blog_marketing_how-to-use-ai-for-content-creation_.txt\n",
            "Processing https://www.usemotion.com/blog/ai-content-creation...\n",
            "Successfully scraped and saved to articles\\www_usemotion_com_blog_ai-content-creation.txt\n",
            "Processing https://blog.hootsuite.com/ai-content-creation-tools/...\n",
            "Successfully scraped and saved to articles\\blog_hootsuite_com_ai-content-creation-tools_.txt\n",
            "Processing https://medium.com/freelancers-hub/best-ai-writing-tools-d9f064ac2d5c...\n",
            "Successfully scraped and saved to articles\\medium_com_freelancers-hub_best-ai-writing-tools-d9f064ac2d5c.txt\n",
            "Processing https://www.impactplus.com/blog/ai-tools-for-content-creation...\n",
            "Successfully scraped and saved to articles\\www_impactplus_com_blog_ai-tools-for-content-creation.txt\n",
            "Processing https://numerous.ai/blog/generative-ai-content-creation...\n",
            "Successfully scraped and saved to articles\\numerous_ai_blog_generative-ai-content-creation.txt\n",
            "Processing https://www.zemith.com/en/blogs/ai-tools-for-content-creation...\n",
            "Successfully scraped and saved to articles\\www_zemith_com_en_blogs_ai-tools-for-content-creation.txt\n",
            "\n",
            "--- Process Complete ---\n",
            "Successfully processed 10 articles.\n",
            "The content is saved in the 'articles' directory.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STEP 1 : IMPORT LIBRARIES\n",
        "# ============================================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Define a function to read the youtube video urls from a local folder\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def read_local_urls_file(file_name):\n",
        "    \"\"\"\n",
        "    Reads a text file containing URLs, where each URL is on a new line.\n",
        "    Returns a list of URLs or None if the file is not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_name, 'r', encoding='utf-8') as f:\n",
        "            urls = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"Successfully read {len(urls)} URLs from '{file_name}'.\")\n",
        "        return urls\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_name}' was not found. Please make sure it's in the same directory as the script.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file '{file_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Define a function to read the blogs and articles urls from a local folder\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def scrape_web_page(url):\n",
        "    \"\"\"\n",
        "    Fetches the content of a web page related to AI and extracts the main text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Get the title of the article\n",
        "        title = soup.title.string if soup.title else \"Untitled\"\n",
        "\n",
        "        # Find all paragraph tags and combine their text\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        # Basic cleaning: remove extra whitespace and newlines\n",
        "        cleaned_text = ' '.join(text.split())\n",
        "\n",
        "        return title, cleaned_text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing content from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# --- Main script execution ---\n",
        "\n",
        "# Get URLs from youtube_urls.txt and process them.\n",
        "youtube_urls = read_local_urls_file(\"youtube_urls.txt\")\n",
        "if youtube_urls:\n",
        "    print(f\"\\nProcessing {len(youtube_urls)} YouTube URLs...\")\n",
        "    # Add your YouTube scraping logic here\n",
        "\n",
        "# Get URLs from web_urls.txt and process them.\n",
        "web_urls = read_local_urls_file(\"web_urls.txt\")\n",
        "if web_urls:\n",
        "    # Create a directory to store the scraped content\n",
        "    if not os.path.exists(\"articles\"):\n",
        "        os.makedirs(\"articles\")\n",
        "\n",
        "    articles_processed = 0\n",
        "\n",
        "    for url in web_urls:\n",
        "        print(f\"Processing {url}...\")\n",
        "        title, content = scrape_web_page(url)\n",
        "        if content:\n",
        "            # Create a clean filename from the URL to save the content\n",
        "            filename = url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "            output_filename = os.path.join(\"articles\", f\"{filename}.txt\")\n",
        "\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"Title: {title}\\n\\n\")\n",
        "                f.write(content)\n",
        "\n",
        "            articles_processed += 1\n",
        "            print(f\"Successfully scraped and saved to {output_filename}\")\n",
        "\n",
        "        # Be a good web citizen and add a small delay\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(\"\\n--- Process Complete ---\")\n",
        "    print(f\"Successfully processed {articles_processed} articles.\")\n",
        "    print(\"The content is saved in the 'articles' directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "False\n",
            "False\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Check if the youtube videos have the transcript available\n",
        "# ==============================================================\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
        "\n",
        "def has_transcript(video_id: str) -> bool:\n",
        "    try:\n",
        "        api = YouTubeTranscriptApi()\n",
        "        api.fetch(video_id)\n",
        "        return True\n",
        "    except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable):\n",
        "        return False\n",
        "\n",
        "print(has_transcript(\"V5ych2rxtnQ\"))\n",
        "print(has_transcript(\"1MQ5ozIvgzE\"))\n",
        "print(has_transcript(\"tQ84XYcP-nA\"))\n",
        "print(has_transcript(\"S2TAa4P2IuY&t=347s\"))\n",
        "print(has_transcript(\"1MQ5ozIvgzE&t=171s\"))\n",
        "print(has_transcript(\"UKeCWtI_lfA\"))  # Replace with any YouTube video ID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "===================== FETCH THE TRANSCRIPTS FROM THE YOUTUBE VIDEOS ====================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing video ID: tQ84XYcP-nA ...\n",
            "✅ Transcript saved: YT_transcripts\\tQ84XYcP-nA.txt\n",
            "Processing video ID: _SpyH8wTA-4 ...\n",
            "✅ Transcript saved: YT_transcripts\\_SpyH8wTA-4.txt\n",
            "Processing video ID: S2TAa4P2IuY ...\n",
            "✅ Transcript saved: YT_transcripts\\S2TAa4P2IuY.txt\n",
            "Processing video ID: 1MQ5ozIvgzE ...\n",
            "✅ Transcript saved: YT_transcripts\\1MQ5ozIvgzE.txt\n",
            "Processing video ID: UKeCWtI_lfA ...\n",
            "✅ Transcript saved: YT_transcripts\\UKeCWtI_lfA.txt\n",
            "Processing video ID: rM0xpwENa8I ...\n",
            "✅ Transcript saved: YT_transcripts\\rM0xpwENa8I.txt\n",
            "Processing video ID: uxrSyA-VxWs ...\n",
            "✅ Transcript saved: YT_transcripts\\uxrSyA-VxWs.txt\n",
            "Processing video ID: Tw9HButMNu8 ...\n",
            "✅ Transcript saved: YT_transcripts\\Tw9HButMNu8.txt\n",
            "Processing video ID: LEJGFnjIWmQ ...\n",
            "✅ Transcript saved: YT_transcripts\\LEJGFnjIWmQ.txt\n",
            "Processing video ID: V5ych2rxtnQ ...\n",
            "✅ Transcript saved: YT_transcripts\\V5ych2rxtnQ.txt\n",
            "Processing video ID: -lSDKrX01xA ...\n",
            "✅ Transcript saved: YT_transcripts\\-lSDKrX01xA.txt\n",
            "Processing video ID: 1MQ5ozIvgzE ...\n",
            "✅ Transcript saved: YT_transcripts\\1MQ5ozIvgzE.txt\n",
            "Processing video ID: IjF5Uun2jrM ...\n",
            "✅ Transcript saved: YT_transcripts\\IjF5Uun2jrM.txt\n",
            "Processing video ID: V-eez-Irfbc ...\n",
            "✅ Transcript saved: YT_transcripts\\V-eez-Irfbc.txt\n",
            "Processing video ID: iln0fOakWpM ...\n",
            "✅ Transcript saved: YT_transcripts\\iln0fOakWpM.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
        "\n",
        "# --- CONFIG ---\n",
        "input_file = \"youtube_urls.txt\"   # Your file with one YouTube URL per line\n",
        "output_folder = \"YT_transcripts\"   # Assign folder to save transcripts\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# --- FUNCTION: Extract Video ID ---\n",
        "# -----------------------------------------\n",
        "\n",
        "def extract_video_id(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract YouTube video ID from a URL (works with standard and shortened URLs)\n",
        "    \"\"\"\n",
        "    match = re.search(r\"v=([A-Za-z0-9_-]{11})\", url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    match = re.search(r\"youtu\\.be/([A-Za-z0-9_-]{11})\", url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# --- FUNCTION: Fetch Transcript from the video ID ---\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def fetch_transcript(video_id: str):\n",
        "    try:\n",
        "        api = YouTubeTranscriptApi()\n",
        "        transcript = api.fetch(video_id)\n",
        "        # Combine text lines into a single string using dot notation\n",
        "        full_text = \" \".join([entry.text for entry in transcript])\n",
        "        return full_text\n",
        "    except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable):\n",
        "        return None\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# --- SAVE the fetched youtube transcripts ---\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "with open(input_file, \"r\") as f:\n",
        "    urls = [line.strip() for line in f.readlines()]\n",
        "\n",
        "for url in urls:\n",
        "    video_id = extract_video_id(url)\n",
        "    if not video_id:\n",
        "        print(f\"❌ Could not extract video ID from URL: {url}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing video ID: {video_id} ...\")\n",
        "    transcript_text = fetch_transcript(video_id)\n",
        "\n",
        "    if transcript_text:\n",
        "        output_path = os.path.join(output_folder, f\"{video_id}.txt\")\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
        "            out_file.write(transcript_text)\n",
        "        print(f\"✅ Transcript saved: {output_path}\")\n",
        "    else:\n",
        "        print(f\"⚠️ No transcript available for video: {video_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 14 transcripts.\n",
            "Example: tQ84XYcP-nA Every week there's a new AI tool making headlines, and right now there are more AI video generators than ever. But most of them don't work as well as you'd expect. Some generate great videos, but only if you stay within their style limits. If you try to get more creative, like detailed anime or wild fantasy worlds, they often mess up, and most of the time they're too expensive or just confusing to use. I've tested every major tool that's come out recently, and I've seen where they shine and wher\n"
          ]
        }
      ],
      "source": [
        "#  ===================================\n",
        "#  Load the youtube video transcripts\n",
        "#  ===================================\n",
        "\n",
        "import os\n",
        "\n",
        "transcripts_folder = \"YT_transcripts\"\n",
        "transcript_files = [f for f in os.listdir(transcripts_folder) if f.endswith(\".txt\")]\n",
        "\n",
        "YT_transcripts_data = []\n",
        "\n",
        "for file_name in transcript_files:\n",
        "    file_path = os.path.join(transcripts_folder, file_name)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "        YT_transcripts_data.append({\n",
        "            \"video_id\": file_name.replace(\".txt\", \"\"),\n",
        "            \"text\": text\n",
        "        })\n",
        "\n",
        "print(f\"✅ Loaded {len(YT_transcripts_data)} transcripts.\")\n",
        "print(\"Example:\", YT_transcripts_data[0][\"video_id\"], YT_transcripts_data[0][\"text\"][:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Total YouTube chunks created: 96\n",
            "Example chunk: {'video_id': 'tQ84XYcP-nA', 'chunk_index': 0, 'text': \"Every week there's a new AI tool making headlines, and right now there are more AI video generators than ever. But most of them don't work as well as you'd expect. Some generate great videos, but only if you stay within their style limits. If you try to get more creative, like detailed anime or wild fantasy worlds, they often mess up, and most of the time they're too expensive or just confusing to use. I've tested every major tool that's come out recently, and I've seen where they shine and where they completely fall apart. So, in this video, I'm going to show you the AI video tools that actually deliver the kind of quality you'd want to publish. The best way to use them without wasting hours learning clunky software, and the one platform that ties everything together, so you can create full videos without switching between five different sites. Let's break it down. All right, let's start with Seedance 1.0. Seance is by danceance's flagship AI video generator, the same company behind Tik Tok. And it's currently ranked number one on the leading video AI benchmark. And unlike most tools that glitch out when you get specific, Cance was built for precision and it actually follows your complex prompts. If you've tried video generators before and felt limited, this one changes the game. To use it, I'm heading into the text to video tab and selecting seed dance in the drop- down menu. By the way, if you want to know how I ax all of the AI models in just one tool, it's because I use open art. I'll leave a link to it down below so that you can follow along. Seedance supports both image and text generation. But for now, we're starting fresh with a prompt. Now, I'll type in my prompt. For example, a lone cyberpunk courier speeds through a neon lit city on a futuristic motorcycle. Rain falls in slow motion, bouncing off chrome surfaces and glowing street sign. The camera tracks from behind as the bike swerves between flying cars and under flickering holograms. Midway, the courier skids around a corner and removes their helmet, revealing a determined expression. In the distance, a tower explodes in a pulse of electric blue light. Moody, high contrast lighting, reflective puddles, 24 fps, cinematic motion blur. As you can see, I try to be as in-depth as I possibly can. This is very important for all models we are going to cover today. The more specific you are in your prompt, the better the result will come out. But I get that for a beginner, this can be challenging. So, I've come up with a simple solution to help you out. To create a very in-depth prompt, I head to chat GPT and paste in this prompt. Create a super in-depth prompt for an AI generated video using Seance Model. The prompt idea is a lone courier speeding through a neon lit city on\"}\n"
          ]
        }
      ],
      "source": [
        "# ====================================\n",
        "# chunk the youtube video transcripts\n",
        "# ====================================\n",
        "\n",
        "\n",
        "def chunk_text(text, max_length=500):  # max length define the length of each chunk\n",
        "    \"\"\"\n",
        "    Split text into smaller chunks of approximately max_length words.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_length):\n",
        "        chunks.append(\" \".join(words[i:i+max_length]))\n",
        "    return chunks\n",
        "\n",
        "yt_chunks = []\n",
        "\n",
        "for item in YT_transcripts_data:\n",
        "    chunks = chunk_text(item[\"text\"], max_length=500)  # you can adjust length\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        yt_chunks.append({\n",
        "            \"video_id\": item[\"video_id\"],\n",
        "            \"chunk_index\": i,\n",
        "            \"text\": chunk\n",
        "        })\n",
        "\n",
        "print(f\"✅ Total YouTube chunks created: {len(yt_chunks)}\")\n",
        "print(\"Example chunk:\", yt_chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Total YouTube chunks created: 104\n",
            "Example chunk: {'video_id': 'V5ych2rxtnQ', 'chunk_index': 0, 'text': \"If you've ever used Chat GBT to create content, you've probably run into the same issue I have. It sounds okay, but it never really sounds like you. Yeah, this sounds nothing like me. But what if I could tell you that you could visually teach AI how to sound like you? Your tone, the structure, and even your sense of humor, all by just dropping in your old work. The problem isn't that AI can't sound like you. The problem is there's no clear or creative way to show it how. That is, there wasn't until now. And I'm going to show you a tool that has helped me save hours of time in my creative workflow. With just a few inputs from my past work, you're going to see this thing work like magic. So, let me show you. Poppy AI. All right. Here we are in a brand new Poppy board. This is the visual space that you will be working in. And I'm going to start an AI chat window, which is not going to be too unfamiliar if you've been on any AI tool before. But where this gets interesting is when you can just\"}\n"
          ]
        }
      ],
      "source": [
        "#  ====================================================================================\n",
        "# chunk the youtube video transcripts with max_length of 200 characters for each chunk\n",
        "#  ====================================================================================\n",
        "\n",
        "\n",
        "# def chunk_text(text, max_length=500):\n",
        "#     \"\"\"\n",
        "#     Split text into smaller chunks of approximately max_length words.\n",
        "#     \"\"\"\n",
        "#     words = text.split()\n",
        "#     chunks = []\n",
        "#     for i in range(0, len(words), max_length):\n",
        "#         chunks.append(\" \".join(words[i:i+max_length]))\n",
        "#     return chunks\n",
        "\n",
        "# yt_chunks = []\n",
        "\n",
        "# for item in transcripts_data:\n",
        "#     chunks = chunk_text(item[\"text\"], max_length=200)  # you can adjust length\n",
        "#     for i, chunk in enumerate(chunks):\n",
        "#         yt_chunks.append({\n",
        "#             \"video_id\": item[\"video_id\"],\n",
        "#             \"chunk_index\": i,\n",
        "#             \"text\": chunk\n",
        "#         })\n",
        "\n",
        "# print(f\"✅ Total YouTube chunks created: {len(yt_chunks)}\")\n",
        "# print(\"Example chunk:\", yt_chunks[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 96/96 [01:21<00:00,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embeddings generated for 96 chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#  =======================================\n",
        "#  Generate embeddings for each YT chunk\n",
        "#  =======================================\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Load API key from .env\n",
        "load_dotenv()\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "def get_yt_embedding(text):\n",
        "    # Ensure text isn’t too long\n",
        "    if len(text) > 8000:  # token proxy via characters\n",
        "        text = text[:8000]\n",
        "    try:\n",
        "        response = client.embeddings.create(\n",
        "            model=\"text-embedding-3-small\",    # OpenAI embedding model\n",
        "            input=text\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Embedding failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Loop with a pause to respect rate limits\n",
        "for chunk in tqdm(yt_chunks):\n",
        "    emb = get_yt_embedding(chunk['text'])\n",
        "    if emb:\n",
        "        chunk['embedding'] = emb\n",
        "    time.sleep(0.5)  # add small delay\n",
        "\n",
        "\n",
        "print(f\"✅ Embeddings generated for {len(yt_chunks)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedded data saved to embedded_yt_transcripts.json\n"
          ]
        }
      ],
      "source": [
        "#  ===========================================================\n",
        "#  save generated YouTube embeddings locally as .json file)\n",
        "#  ===========================================================\n",
        "\n",
        "import json\n",
        "\n",
        "# Example: storing locally as JSON\n",
        "output_file = \"embedded_yt_transcripts.json\"   # Assign the file name to save the embedded data\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(yt_chunks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Embedded data saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "======================= BLOGS AND ARTICLES ========================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 10 articles.\n",
            "Example: blog_hootsuite_com_ai-content-creation-tools_ Title: 15 Free AI Tools For Content Creation | GWI\n",
            "\n",
            "SHARE You’ve got five briefs due, a campaign launching tomorrow, and your designer is on leave. Stressful, right? That’s where AI tools can step in - to make the chaos more manageable. Marketers, strategists, and creatives are no longer just looking for tools to help them write faster, but are in desperate need of solutions that can do everything from analyzing audience behavior to creating visuals, optimizing content marketing, and more. That’\n"
          ]
        }
      ],
      "source": [
        "#  ===================================\n",
        "#  Load the blogs and articles\n",
        "#  ===================================\n",
        "\n",
        "\n",
        "articles_folder = \"articles\"  # your folder containing article .txt files\n",
        "article_files = [f for f in os.listdir(articles_folder) if f.endswith(\".txt\")]\n",
        "\n",
        "articles_data = []\n",
        "\n",
        "for file_name in article_files:\n",
        "    file_path = os.path.join(articles_folder, file_name)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "        articles_data.append({\n",
        "            \"article_id\": file_name.replace(\".txt\", \"\"),\n",
        "            \"text\": text\n",
        "        })\n",
        "\n",
        "print(f\"✅ Loaded {len(articles_data)} articles.\")\n",
        "print(\"Example:\", articles_data[1][\"article_id\"], articles_data[0][\"text\"][:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Total article chunks created: 68\n",
            "Example chunk: {'article_id': 'www_gwi_com_blog_free-ai-tools-for-content-creation', 'chunk_index': 0, 'text': \"Title: 15 Free AI Tools For Content Creation | GWI SHARE You’ve got five briefs due, a campaign launching tomorrow, and your designer is on leave. Stressful, right? That’s where AI tools can step in - to make the chaos more manageable. Marketers, strategists, and creatives are no longer just looking for tools to help them write faster, but are in desperate need of solutions that can do everything from analyzing audience behavior to creating visuals, optimizing content marketing, and more. That’s why we’ve pulled together 15 of the most useful free AI tools on the web. These tools don’t just churn out text, they help you understand your audience, streamline workflows, and make sure your content cuts through the noise. We like to think of AI as not replacing human creativity but rather supercharging it. Today’s tools can draft copy, generate imagery, transform blogs into videos in a few clicks, and even tell you exactly what your audience cares about. Say you’ve got three client campaigns going live in a week. AI tools can help you go from staring at your screen to drafting social posts, promo graphics, and SEO snippets in an afternoon, all without having to skip your lunchtime mental health walk. That kind of support is a game changer. But, as of yet, no single tool does it all. Some are built to help you be more efficient, others to get your ideas out of your mind and onto paper. The best content teams know what to use and when - that’s where this list comes in. Everything here is either completely free (yay) or comes with a generous free tier (still yay). And we’ve made sure there’s something for every stage of the content lifecycle, from idea to insight, creation to distribution. Each tool solves a specific challenge, so you’ll find a clear explanation of what it does, what it’s great at, and who it’s best for. Whether you're a freelance social manager juggling four brands, or the content lead in a fast-scaling startup, there’s a smart way to use AI to your advantage. We’re kicking things off with GWI Spark, and for good reason. While most AI tools are focused on producing content, GWI Spark helps you create the right content by starting with what your audience actually wants. Before you even type a single word, GWI Spark gives you the lowdown on who you’re talking to and what matters most to them. It’s our AI assistant designed for marketers, strategists, and anyone who wants to build smarter content using real, reliable data. Rather than pulling from the open web, GWI Spark taps into GWI’s massive global survey data of nearly a million real consumers across 50+ markets. Just ask a question in plain language and get instant, insight-packed answers you can pop straight into your campaign or pitch. With an easy chat interface and a free access tier, GWI Spark is ideal for non-experts who need fast, credible insights. Use it to shape\"}\n"
          ]
        }
      ],
      "source": [
        "#  ===================================\n",
        "#  Chunk the blogs and articles\n",
        "#  ===================================\n",
        "\n",
        "def chunk_text(text, max_length=500):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_length):\n",
        "        chunks.append(\" \".join(words[i:i+max_length]))\n",
        "    return chunks\n",
        "\n",
        "article_chunks = []\n",
        "\n",
        "for item in articles_data:\n",
        "    chunks = chunk_text(item[\"text\"], max_length=500)  # adjust size if needed\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        article_chunks.append({\n",
        "            \"article_id\": item[\"article_id\"],\n",
        "            \"chunk_index\": i,\n",
        "            \"text\": chunk\n",
        "        })\n",
        "\n",
        "print(f\"✅ Total article chunks created: {len(article_chunks)}\")\n",
        "print(\"Example chunk:\", article_chunks[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:52<00:00,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embeddings generated for 68 article chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#  ===========================================\n",
        "#  Generate embeddings for each article chunk\n",
        "#  ===========================================\n",
        "\n",
        "def get_art_embedding(text):\n",
        "    \n",
        "    if len(text) > 8000:    # Ensure text isn’t too long\n",
        "        text = text[:8000]\n",
        "    try:\n",
        "        response = client.embeddings.create(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            input=text\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Embedding failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Loop with a pause to respect rate limits\n",
        "for chunk in tqdm(article_chunks):\n",
        "    emb = get_art_embedding(chunk['text'])\n",
        "    if emb:\n",
        "        chunk['embedding'] = emb\n",
        "    time.sleep(0.5)  # add small delay\n",
        "\n",
        "\n",
        "# for i, chunk in enumerate(article_chunks):\n",
        "#     chunk[\"embedding\"] = embeddings[i]\n",
        "\n",
        "print(f\"✅ Embeddings generated for {len(article_chunks)} article chunks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedded article data saved to embedded_articles.json\n"
          ]
        }
      ],
      "source": [
        "#  ===========================================\n",
        "#  Save the article embeddings\n",
        "#  ===========================================\n",
        "\n",
        "import json\n",
        "\n",
        "output_file = \"embedded_articles.json\"     # Assign the file name to save the embedded article data\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(article_chunks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Embedded article data saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the Youtube chunks\n",
        "\n",
        "import json\n",
        "\n",
        "file_path_1 = \"/content/drive/MyDrive/Ironhack_final_project/embedded_yt_transcripts.json\" # CHANGE THE FILE PATH IF YOU ARE USING THE VS CODE\n",
        "\n",
        "with open(file_path_1, \"r\") as f:\n",
        "      yt_embedded_chunks = json.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(yt_embedded_chunks)} youtube transcript chunks\")\n",
        "print(yt_embedded_chunks[0])  # preview first chunk\n",
        "\n",
        "\n",
        "\n",
        "# Importing the article chunks\n",
        "\n",
        "file_path_2 = \"/content/drive/MyDrive/Ironhack_final_project/embedded_articles.json\" # CHANGE THE FILE PATH IF YOU ARE USING THE VS CODE\n",
        "\n",
        "with open(file_path_2, \"r\") as f:\n",
        "      article_embedded_chunks = json.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(article_embedded_chunks)} article chunks\")\n",
        "print(article_embedded_chunks[0])  # preview first chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# =========================================================================\n",
        "# Comnbining the both embedded chunks into one named 'all_embedded_chunks'\n",
        "# =========================================================================\n",
        "\n",
        "all_embedded_chunks = yt_embedded_chunks + article_embedded_chunks\n",
        "\n",
        "print(f\"Total chunks: {len(all_embedded_chunks)}\")\n",
        "print(\"Example chunk:\", all_embedded_chunks[45])\n",
        "\n",
        "# Save to JSON\n",
        "\n",
        "all_embedded_file_path = \"/content/drive/MyDrive/Ironhack_final_project/\"   # assign the folder path to save as JSON file.\n",
        "\n",
        "# make sure the folder exists\n",
        "os.makedirs(all_embedded_file_path, exist_ok=True)\n",
        "\n",
        "# full file path\n",
        "file_path = os.path.join(all_embedded_file_path, \"all_embedded_chunks.json\")\n",
        "\n",
        "# save as json\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_embedded_chunks, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Saved {len(all_embedded_chunks)} chunks to {file_path}\")\n",
        "\n",
        "print(\"✅Saved as all_embedded_chunks.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
