# -*- coding: utf-8 -*-
"""Part3_Agent_Deployment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U5irp-gxk43hUvzJ2JRRALIGJfvxqR6C
"""

!pip install chromadb
!pip install openai
!pip install tqdm
!pip install sounddevice
!pip install whisper
!pip install gtts
!pip install beautifulsoup4
!pip install tavily-python
!pip install lxml
!pip install pyttsx3
!pip install langchain_openai
!pip install langchain-community
!pip install langchain-google-genai
!pip install langsmith
!pip install PyPDF2

# Common dependencies for some of the above packages
# BeautifulSoup often works best with an external parser like lxml
# Tavily and OpenAI clients use an HTTP client like httpx
!pip install lxml
!pip install httpx

!pip install google-search-results

# ============================================
# STEP - 1 : Imports
# ============================================

# Standard Library Imports
import os
import json
import re

# Third-Party Library Imports
import chromadb
import openai
import tqdm
import pyttsx3
import gtts
from google.colab import drive
from bs4 import BeautifulSoup
from tavily import TavilyClient

# LangChain and related framework imports
import langchain
from langchain import hub
from langchain.tools import Tool
from langchain.agents import AgentExecutor, create_react_agent
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import WebBaseLoader
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser # Corrected import path
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

# =========================================
# STEP - 2 : Load API keys (Colab + Env)
# =========================================
import os

# Attempt to fetch from Colab userdata if available
try:
    from google.colab import userdata
    _colab_available = True
except ImportError:
    _colab_available = False

def get_key(key_name: str) -> str:
    """Fetch API key from Colab userdata first, then environment variables."""
    key = None
    if _colab_available:
        key = userdata.get(key_name)
    if not key:
        key = os.environ.get(key_name)
    if not key:
        print(f"‚ùå {key_name} not found! Please add it to Colab secrets or environment variables.")
    else:
        print(f"‚úÖ {key_name} loaded successfully!")
    return key

# Fetch all keys
OPENAI_API_KEY = get_key("OPENAI_API_KEY")
TAVILY_API_KEY = get_key("TAVILY_API_KEY")
SERPAPI_API_KEY = get_key("SERPAPI_API_KEY")
LANGCHAIN_API_KEY = get_key("LANGCHAIN_API_KEY")
# LANGSMITH_API_KEY = get_key("LANGSMITH_API_KEY")
# HF_TOKEN = get_key("HF_TOKEN")
# Optional: Pinecone if you use it
# PINECONE_API_KEY = get_key("PINECONE_API_KEY")

# Set OpenAI key for SDKs
if OPENAI_API_KEY:
    import openai
    openai.api_key = OPENAI_API_KEY

# ================================================
# ---------- MOUNT THE GOOGLE DRIVE -------------
# ================================================

from google.colab import drive
drive.mount('/content/drive')

print(f"‚úÖ Google Drive mounted successfully!")

# ============================================
# STEP - 3 : Importing all embedded chunks
# ============================================

all_embedded_file_path = "/content/drive/MyDrive/Ironhack_final_project/all_embedded_chunks.json"

with open(all_embedded_file_path, "r", encoding="utf-8") as f:
    all_embedded_chunks = json.load(f)

print(f"‚úÖ Loaded {len(all_embedded_chunks)} chunks")
print(all_embedded_chunks[0])  # preview first chunk

# ============================================
# STEP - 4 : RAG Pipeline with Memory
# ============================================

import os
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory

# --------------------------
# 1Ô∏è Initialize OpenAI client
# --------------------------
api_key = userdata.get("OPENAI_API_KEY")  # Load API key from Colab secrets
# client = OpenAI(api_key=api_key) # No need to re-initialize client here


# --------------------------
# 2Ô∏è Initialize OpenAIEmbeddings
# --------------------------
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=api_key
)

# --------------------------
# 3Ô∏è Create/Load vector store
# --------------------------

persist_dir = "/content/drive/MyDrive/Ironhack_final_project/chromadb_store" # file path to the vector database

vectorstore = Chroma(
    persist_directory=persist_dir,  # folder where your vector DB is stored
    embedding_function=embeddings
)


# ----------------------------------------------------------
# 4Ô∏è Add embedded chunked data to Chromadb_store (vector DB or store)
# ----------------------------------------------------------
texts = [chunk["text"] for chunk in all_embedded_chunks]
metadatas = [
    {
        "source": chunk.get("video_id") or chunk.get("article_id"),
        "chunk_index": chunk["chunk_index"]
    }
    for chunk in all_embedded_chunks
]

# Add documents with automatic embedding generation
vectorstore.add_texts(texts=texts, metadatas=metadatas)

print(f"üéâ Done! Chroma vector store is loaded")


# --------------------------
# 5Ô∏è Create retriever
# --------------------------
retriever = vectorstore.as_retriever(search_kwargs={"k": 8})  # higher k for better results


# --------------------------
# 6Ô∏è LLMChain prompt and chain
# --------------------------

prompt = PromptTemplate(
    input_variables=["context", "question", "chat_history"],
    template="""
You are a helpful AI assistant for content creators.
Always answer concisely, clearly, and in a structured format.
Prefer bullet points or numbered steps (3‚Äì6 items).
Each bullet should be 1‚Äì2 sentences max.
Do not repeat information.
If the context is incomplete, use conversation history or your own knowledge.

Conversation history:
{chat_history}

Context:
{context}

Question: {question}

Answer (concise and structured):
"""
)


# prompt = PromptTemplate(
#     input_variables=["context", "question", "chat_history"],
#     template="""
# You are a helpful AI assistant. Use the context below and the chat history to answer the question.
# If the context is incomplete, rely on conversation history or your knowledge to give the best answer.

# Conversation history:
# {chat_history}

# Context:
# {context}

# Question: {question}

# Answer:
# """
# )

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3, openai_api_key=api_key) # Pass the api_key here


# ----------------------
# Add memory here
# ----------------------
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


qa_chain = LLMChain(llm=llm, prompt=prompt)


# ----------------------------------
# 7Ô∏è Full RAG function with memory
# ----------------------------------

def ask_rag(query: str) -> str:
    """
    Full RAG pipeline: retrieves relevant chunks, uses memory, and generates answer.
    """
    # --- Retrieve relevant chunks ---
    results = retriever.get_relevant_documents(query)

    if not results:
        return "‚ùå No relevant documents found."

    # --- Combine retrieved chunks ---
    context = "\n\n".join([doc.page_content for doc in results])

    print("\nüìù Retrieved context preview:\n", context[:1000])

    # --- Load memory ---
    chat_history = memory.load_memory_variables({}).get("chat_history", [])

    # --- Generate answer ---
    answer = qa_chain.run({
        "context": context,
        "question": query,
        "chat_history": chat_history
    })

    # --- Save interaction into memory ---
    memory.save_context({"input": query}, {"output": answer})

    return answer

# ‚úÖ Run a memory test
print("\n--- Memory Test ---\n")

# First interaction
response1 = ask_rag("My name is Abhi.")
print("User: My name is Abhi.")
print("Bot:", response1)

# Second interaction (checks memory)
response2 = ask_rag("What is my name?")
print("\nUser: What is my name?")
print("Bot:", response2)

# Third interaction (longer context test)
response3 = ask_rag("Remember that I like working with RAG pipelines.")
print("\nUser: Remember that I like working with RAG pipelines.")
print("Bot:", response3)

response4 = ask_rag("What do I like working with?")
print("\nUser: What do I like working with?")
print("Bot:", response4)

# =============================================================================
# STEP - 5 :  Testing my RAG retrieval and its memory with multiple questions
# =============================================================================

# First question: sets some context
print(ask_rag("What is an AI image generator?"))

# Second question: relies on memory
print(ask_rag("And which one is considered the best?"))

# Third question: continues from memory
print(ask_rag("Why is it considered the best?"))


# # -----------------------
# # 8Ô∏è‚É£ Testing my RAG
# # -----------------------
# if __name__ == "__main__":
#     user_query = input("‚ùì Enter your question: ")
#     answer = ask_rag(user_query)
#     print("\nüí° Answer:\n", answer)


# # -----------------------------------------------------------
# # 8Ô∏è‚É£ Testing RAG with memory with continuous questioning
# # -----------------------------------------------------------

# if __name__ == "__main__":
#     print("üìù You can ask multiple questions. Type 'exit' to quit.\n")

#     while True:
#         user_query = input("‚ùì Enter your question: ")
#         if user_query.lower() in ["exit", "quit"]:
#             print("üëã Exiting chat. Goodbye!")
#             break

#         answer = ask_rag(user_query)
#         print("\nüí° Answer:\n", answer)
#         print("-" * 50)  # separator for readability

"""**==================>>> T O O L S <<<========================**"""

# ============================================
# STEP - 6 : Retriever tool (Chroma retriever) with metadata
# ============================================

from langchain.tools import Tool

retriever_tool = Tool(
    name="Chroma Retriever",
    func=vectorstore.as_retriever().get_relevant_documents,
    description=(
        "This is the PRIMARY tool to use first. "
        "Always try this before using any other tool. "
        "It retrieves relevant document chunks from the Chroma database "
        "to answer questions when the context exists."
    )
)

print("‚úÖ Retriever tool is loaded")

# ============================================
# STEP - 7 : Google Search tool (Tavily)
# ============================================


# from langchain.tools import Tool


tavily_api_key = userdata.get('TAVILY_API_KEY')
if not tavily_api_key:
    raise ValueError("‚ùå No Tavily API key found! Please add it in Colab secrets.")

from tavily import TavilyClient
tavily = TavilyClient(api_key=tavily_api_key)

def search_tavily(query: str):
    """Perform a web search using Tavily and return top results as a list."""
    results = tavily.search(query, max_results=3)
    # Return a list of result content
    return [r["content"] for r in results["results"]]

search_tool = Tool(
    name="Google Search (Tavily)",
    func=search_tavily,
    description="Use this when the question cannot be answered from the context. Returns top 3 web search results."
)

print("‚úÖ Search tool is loaded")

# ------------------------------------
# Testing the search tool (Tavily)
# ------------------------------------

results = search_tavily("AI image generators")
print(f"üîç Tavily Search Results:")
for i, r in enumerate(results, 1):
    print(f" {i}. {r}")

# ========================================================
# STEP 8 - W H I S P E R  T O O L - for Speech-to-Text
# ========================================================

# import whisper # Removed this import
# import sounddevice as sd # Keep this import if local recording is desired, but address PortAudio issue separately
from scipy.io.wavfile import write
from langchain.tools import Tool
from openai import OpenAI # Ensure OpenAI client is imported

# Load Whisper model once # Removed this line

client = OpenAI(api_key=OPENAI_API_KEY)


def record_audio(filename="input.wav", duration=5, fs=16000):
    print(f"üé§ Recording for {duration} seconds...")
    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)
    sd.wait()
    write(filename, fs, audio)
    print("‚úÖ Recording finished")
    return filename

def transcribe_audio_openai(audio_file_path):
    """Transcribes audio using OpenAI's Whisper model."""
    with open(audio_file_path, "rb") as audio_file:
        # Use the existing 'client' object from cell oz4C828MYuR5
        # Make sure the client is initialized in a preceding cell
        transcription = client.audio.transcriptions.create(
            model="whisper-1", # OpenAI's Whisper model
            file=audio_file
        )
    return transcription.text

# --- Wrap in a Tool ---
def whisper_speech_to_text(audio_file_path):
    """Transcribes a given audio file into text using OpenAI's Whisper."""
    # Note: This now expects a file path as input, not live recording due to PortAudio issue
    # If local recording is resolved, you can add logic here to record first
    if not os.path.exists(audio_file_path):
         return f"‚ùå Error: Audio file not found at {audio_file_path}"

    text = transcribe_audio_openai(audio_file_path)
    return text

whisper_tool = Tool(
    name="Whisper Speech-to-Text",
    func=whisper_speech_to_text,
    description=(
        "Transcribes an audio file (provide file path) into text using OpenAI's Whisper model. "
        # Removed the part about recording live audio due to environment limitations
        "Useful for converting spoken content from a file into text for analysis or response generation."
    )
)

print("‚úÖ Whisper tool is loaded (using OpenAI)")

# =====================================================
# STEP - 9 : TTS (Text-to-Speech) tool with OpenAI
# =====================================================

from openai import OpenAI
import os


# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def text_to_speech(text, filename="response.mp3"):
    """
    Convert agent's text response to natural speech using OpenAI TTS
    """
    with client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",   # You can also try "gpt-4o-tts" for higher quality
        voice="alloy",             # Options: alloy, verse, sage, etc.
        input=text,
    ) as response:
        response.stream_to_file(filename)

    print(f"üîä Saved speech to {filename}")
    return filename

# Wrap text-to-speech function as a Tool
tts_tool = Tool(
    name="Text-to-Speech",
    func=text_to_speech,
    description="Converts text into natural-sounding speech using OpenAI TTS and saves as MP3."
)

print("‚úÖ Text-to-Speech tool is loaded")

# =================================================
# STEP - 10 : N E W S  T O O L - Powered by Google
# =================================================

import os
from langchain.utilities import SerpAPIWrapper
from langchain.agents import Tool

# --------------------------
# 1Ô∏è‚É£ Load SerpAPI key
# --------------------------
# For Colab, you can use userdata.get() if available
try:
    from google.colab import userdata
    serpapi_api_key = userdata.get('SERPAPI_API_KEY')
except ImportError:
    serpapi_api_key = os.getenv('SERPAPI_API_KEY')

if not serpapi_api_key:
    raise ValueError("‚ùå No SerpAPI API key found! Please set it in environment variables or Colab secrets.")

os.environ['SERPAPI_API_KEY'] = serpapi_api_key

# --------------------------
# 2Ô∏è‚É£ Initialize SerpAPI wrapper
# --------------------------
search = SerpAPIWrapper()  # automatically uses SERPAPI_API_KEY

# --------------------------
# 3Ô∏è‚É£ Safe search function
# --------------------------
def safe_news_search(query: str) -> str:
    """
    Searches the web using SerpAPI and returns a concise summary of the top results.

    Args:
        query (str): The search query. Must not be empty.

    Returns:
        str: Concise summary of the search results or an error message if input is invalid.
    """
    if not query or query.strip() == "":
        return "‚ùå Cannot perform search: the query is empty."

    # Get top 5 results for brevity
    results = search.results(query)
    organic = results.get("organic_results", [])

    if not organic:
        return "No results found for your query."

    summary_lines = []
    for i, r in enumerate(organic[:5], 1):
        title = r.get("title", "No title")
        link = r.get("link", "No link")
        snippet = r.get("snippet", "")
        summary_lines.append(f"{i}. {title}\n   {snippet}\n   üîó {link}")

    return "\n\n".join(summary_lines)

# --------------------------
# 4Ô∏è‚É£ Wrap as a LangChain tool
# --------------------------
news_tool = Tool(
    name="Latest AI News",
    func=safe_news_search,
    description=(
        "Use this tool to search the web for factual information, news, or updates on a specific topic. "
        "Only call it when the user asks a factual question or requests the latest news. "
        "Do NOT use it for greetings, casual conversation, or personal opinions. "
        "The input must be a valid search query; empty queries will return an error."
    )
)

print("‚úÖ Latest AI News tool is loaded")

!pip install fpdf

# ============================================
# STEP - 11 : SAVE CHAT AS PDF TOOL
# ============================================

from fpdf import FPDF

def save_chat_as_pdf(chat_text: str, filename="chat.pdf"):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, chat_text)
    pdf.output(filename)
    return f"Chat saved to {filename}"

from langchain.agents import Tool

save_pdf_tool = Tool(
    name="SaveChatPDF",
    func=save_chat_as_pdf,
    description="Saves the current chat as a PDF file."
)

# ============================================
# STEP - 11 : SUMMARY TOOL
# ============================================

import requests
from bs4 import BeautifulSoup

def summarize_url(url: str):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    text = soup.get_text()
    # Use your LLM to summarize
    summary = llm.predict(f"Summarize this text:\n{text}")
    return summary

url_summary_tool = Tool(
    name="URLSummary",
    func=summarize_url,
    description="Reads a URL and returns a summarized version of its content."
)

# ============================================
# STEP - 11 : GREETING TOOL
# ============================================
import random


def greet_user(user_input: str = None) -> str:
    greetings_general = [
        "Hello! üëã I‚Äôm your AI Content Coach. How can I assist you today?",
        "Hi there! I‚Äôm here to help you create amazing content. What would you like to work on?",
        "Hey! Ready to improve your content? Let‚Äôs get started.",
        "Hello! I can guide you through content creation, AI tools and tips. What‚Äôs first?"
    ]

    if user_input:
        user_input_lower = user_input.lower()
        if any(word in user_input_lower for word in ["morning", "afternoon", "evening"]):
            time_word = next((w for w in ["morning", "afternoon", "evening"] if w in user_input_lower), "day")
            return f"Good {time_word.capitalize()}! I‚Äôm your AI Content Coach. How can I help you today?"
        if any(word in user_input_lower for word in ["hi", "hello", "hey"]):
            return random.choice(greetings_general)

    return random.choice(greetings_general)

greeting_tool = Tool(
    name="Greeting",
    func=greet_user,
    description="Responds naturally to greetings like 'hi', 'hello', or 'good morning'."
)


print("‚úÖ Greeting tool is loaded")

"""**=================== > > > AGENT < < < ======================**"""

# ============================================
# STEP - 12 : Conversational A G E N T
# ============================================

from langchain.agents import initialize_agent, AgentType
from langchain.tools import Tool
from gtts import gTTS
from IPython.display import Audio, display
import openai
# import sounddevice as sd
import numpy as np
import tempfile
import wave


# ----------------------------------------
# Integrating list of tools to the Agent
# ----------------------------------------

tools = [greeting_tool, retriever_tool, search_tool, whisper_tool, news_tool, tts_tool, save_pdf_tool, url_summary_tool]

# ------------------------------------
# 3Ô∏è‚É£ Initialize Agent with Memory
# ------------------------------------

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=True,
    handle_parsing_errors=True
)

print("ü§ñ AI Content Coach is ready! Type 'exit' to quit.")

# --------------------------
# 4Ô∏è‚É£ Run Conversation Loop
# --------------------------

voice_enabled = False  # üîá Default OFF

while True:
    user_input = input("\nYou: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break

    # Toggle voice manually
    if user_input.lower() == "voice on":
        voice_enabled = True
        print("üîä Voice enabled.")
        continue
    elif user_input.lower() == "voice off":
        voice_enabled = False
        print("üîá Voice disabled.")
        continue

    # Agent responds
    response = agent.run(user_input)
    print("\nAI:", response)

    # Play audio only if enabled
    if voice_enabled:
        text_to_speech(response)

"""**=================== > > > AGENT DEPLOYMENT < < < ======================**"""

!pip install PyPDF2

import gradio as gr
import os
import inspect, asyncio, traceback, tempfile, logging, types
import markdown

# Path for logs
LOG_FILE = "agent_gradio.log"
logging.basicConfig(filename=LOG_FILE,
                    level=logging.INFO,
                    format="%(asctime)s %(levelname)s %(message)s")

# ----------------- Globals -----------------
voice_enabled = False


def should_use_retriever(query: str) -> bool:
    """Decide if query needs external knowledge (RAG) or just memory."""
    conversational_keywords = ["my name", "what did i say", "remember", "earlier", "last time"]
    return not any(keyword in query.lower() for keyword in conversational_keywords)


# ----------------- Core RAG Agent Logic -----------------

def chat_with_agent(message, history):
    user_query = message.strip()

    # Convert chat_history (list of tuples) into OpenAI message format
    messages = [{"role": "system", "content": "You are a helpful AI assistant. "
            "Always keep responses short, concise, and informative. "
            "Limit answers to 2‚Äì5 sentences or bullet points."}]
    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_msg})

    # Add the latest user query
    messages.append({"role": "user", "content": user_query})

    try:
        if should_use_retriever(user_query):
            # ‚úÖ Use RAG pipeline (retriever + memory)
            response = ask_rag(user_query)
        else:
            # ‚úÖ Use memory only (skip retriever)
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=250
            )
            response = completion.choices[0].message.content

    except Exception as e:
        response = f"‚ö†Ô∏è Agent Error: {str(e)}"

    return response



# ----------------- Feature Functions -----------------
def text_to_speech(text):
    """Placeholder function to simulate text-to-speech."""
    print(f"Simulating TTS for: '{text}'")
    # In a real app, you would use a library like gTTS or pyttsx3.
    # from gtts import gTTS
    # tts = gTTS(text=text, lang='en')
    # tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    # tts.save(tmp.name)
    # return tmp.name
    return None # Returns None to indicate no audio file is produced in this example

def download_transcript(history):
    """Placeholder function to download chat transcript."""
    path = tempfile.NamedTemporaryFile(delete=False, suffix=".txt").name
    try:
        with open(path, "w", encoding="utf-8") as f:
            for user, assistant in history:
                f.write("User: " + str(user) + "\n")
                f.write("Assistant: " + str(assistant) + "\n")
                f.write("-" * 60 + "\n")
        return path
    except Exception as e:
        logging.exception("Failed to write transcript")
        raise

# -------------------------------------------------------------------------------------


import os

def summarize_file(file):
    """Read and summarize an uploaded file (PDF or TXT)."""
    if file is None:
        return "‚ö†Ô∏è Please upload a file."

    try:
        text_content = ""

        # Handle .txt files
        if file.name.lower().endswith(".txt"):
            with open(file.name, "r", encoding="utf-8", errors="ignore") as f:
                text_content = f.read()

        # Handle .pdf files
        elif file.name.lower().endswith(".pdf"):
            from PyPDF2 import PdfReader
            reader = PdfReader(file.name)
            for page in reader.pages[:5]:  # Limit to first 5 pages
                text_content += page.extract_text() or ""

        else:
            return "‚ö†Ô∏è Unsupported file format. Please upload a PDF or TXT file."

        if not text_content.strip():
            return "‚ö†Ô∏è Could not extract meaningful content from the file."

        # Summarize with AI
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Summarize the following document in 3‚Äì4 concise sentences."},
                {"role": "user", "content": text_content[:4000]}  # Truncate for efficiency
            ],
            max_tokens=250  #lower number prevents overly long completions. In this case 250 is low which is good if you want to keep concise
        )
        return completion.choices[0].message.content

    except Exception as e:
        return f"‚ö†Ô∏è Error summarizing file: {str(e)}"



# def summarize_file(file):
#     """Placeholder function to summarize an uploaded file."""
#     if file is None:
#         return "Please upload a file."

#     # You would add your file reading and summarization logic here.
#     return f"This is a placeholder summary of the file at: {file.name}"


# -------------------------------------------------------------------------------------


import requests
from bs4 import BeautifulSoup



# -------------------------------------------------------------------------------------


def summarize_url(url):
    """Fetch and summarize the main text content of a URL."""
    if not url or not url.startswith(("http://", "https://")):
        return "‚ö†Ô∏è Please enter a valid URL (must start with http:// or https://)."

    try:
        # Fetch webpage
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        # Parse HTML
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = [p.get_text(strip=True) for p in soup.find_all("p")]
        text_content = " ".join(paragraphs[:10])  # Limit to first 10 paragraphs for efficiency

        if not text_content.strip():
            return "‚ö†Ô∏è Could not extract meaningful content from the page."

        # Summarize with AI
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Summarize the webpage content in 3‚Äì4 concise sentences."},
                {"role": "user", "content": text_content}
            ],
            max_tokens=150
        )
        return completion.choices[0].message.content

    except requests.exceptions.RequestException as e:
        return f"‚ö†Ô∏è Failed to fetch URL: {str(e)}"
    except Exception as e:
        return f"‚ö†Ô∏è Error summarizing URL: {str(e)}"


# def summarize_url(url):
#     """Placeholder function to summarize a URL's content."""
#     if not url or not url.startswith(("http://", "https://")):
#         return "Please enter a valid URL."

    # You would add your web scraping and summarization logic here.
    return f"This is a placeholder summary of the content from the URL: {url}"

# -------------------------------------------------------------------------------------

# ----------------------------------------------------------
# ----------------- Gradio App Logic -----------------
# ----------------------------------------------------------


def respond(user_input, audio_filepath, history, url_input=None, file_input=None):
    global voice_enabled

    # Handle summarization requests first
    if url_input:
        summary = summarize_url(url_input)
        history.append((f"Summarize URL: {url_input}", summary))
        return history, "", None, None

    if file_input:
        summary = summarize_file(file_input)
        history.append((f"Summarize File: {file_input.name}", summary))
        return history, "", None, None

    # Handle chat messages
    if not user_input and not audio_filepath:
        return history, "", None, None

    message = user_input
    if audio_filepath:
        # NOTE: You need to implement your own transcription logic here.
        message = "Transcribing audio... (This feature needs a real transcription tool)"

    # Handle voice toggle commands
    if message.lower() == "voice on":
        voice_enabled = True
        response = "üîä Voice enabled."
    elif message.lower() == "voice off":
        voice_enabled = False
        response = "üîá Voice disabled."
    else:
        try:
            response = chat_with_agent(message, history)
        except Exception as e:
            tb = traceback.format_exc()
            logging.exception("Error while calling agent")
            response = f"‚ö†Ô∏è Agent Error: {str(e)}\n\n```\n{tb}\n```"

    audio_output = None
    if voice_enabled and not response.startswith("‚ö†Ô∏è"):
        try:
            audio_output_file = text_to_speech(response)
            if audio_output_file and os.path.exists(audio_output_file):
                audio_output = audio_output_file
        except Exception as e:
            logging.exception("TTS generation failed")
            audio_output = None

    if audio_output:
        history.append((message, (response, audio_output)))
    else:
        history.append((message, response))

    return history, "", None, None

# -------------------------------------------------------------------------------------


def respond_with_tts(user_text, mic_file, history, url_input, file_upload):
    if history is None:
        history = []

    # 1Ô∏è‚É£ Append user message immediately
    history.append((user_text, ""))

    # 2Ô∏è‚É£ Generate bot response
    bot_response = respond(user_text, mic_file, history, url_input, file_upload)[0][-1][1]

    # 3Ô∏è‚É£ Update last entry with bot response
    history[1] = (user_text, bot_response)

    # 4Ô∏è‚É£ Convert bot response to speech
    audio_file = tts_tool.run(bot_response)

    return history, None, None, None, audio_file



# ------------------------------------------------------------------
# ----------------- Gradio UI -----------------
# ------------------------------------------------------------------

with gr.Blocks(css="""
    .gr-chat-message.user {background-color: #E0F7FA; border-radius: 10px; padding: 5px;}
    .gr-chat-message.bot {background-color: #FFF3E0; border-radius: 10px; padding: 5px;}
    .gr-button {background-color: #29B6F6; color: white;}
""") as demo:

    gr.Markdown("## ü§ñ AI Content Coach")
    gr.Markdown("Type your question or speak it. Use the sidebar for additional features.")

    chatbot = gr.Chatbot(label="Chat History", elem_id="chatbot", type="tuples", height=400)
    history = gr.State([])   # ‚úÖ persistent chat history

    # Audio output for TTS
    audio_output = gr.Audio(label="Speech Output", type="filepath")

    with gr.Row():
        with gr.Column(scale=3):
            msg = gr.Textbox(label="Your Question", placeholder="Type here...", lines=1)
            mic = gr.Microphone(label="üé§ Speak", type="filepath", sources="microphone")
            clear_button = gr.Button("Clear Chat", variant="primary")

        with gr.Column(scale=1):
            gr.Markdown("### Features")
            url_input = gr.Textbox(label="Summarize URL", placeholder="Paste a URL here")
            file_upload = gr.File(label="Summarize File", file_types=["pdf", "txt"])
            download_button = gr.Button("Download Chat Transcript")
            file_output = gr.File(label="Download File")

    # ----------------- Event listeners -----------------
    msg.submit(
        fn=respond_with_tts,
        inputs=[msg, gr.State(None), history, url_input, file_upload],
        outputs=[chatbot, msg, url_input, file_upload, audio_output]
    )

    mic.change(
        fn=respond_with_tts,
        inputs=[gr.State(None), mic, history, url_input, file_upload],
        outputs=[chatbot, msg, url_input, file_upload, audio_output]
    )

    url_input.submit(
        fn=respond_with_tts,
        inputs=[gr.State(None), gr.State(None), history, url_input, file_upload],
        outputs=[chatbot, msg, url_input, file_upload, audio_output]
    )

    file_upload.change(
        fn=respond_with_tts,
        inputs=[gr.State(None), gr.State(None), history, url_input, file_upload],
        outputs=[chatbot, msg, url_input, file_upload, audio_output]
    )

    clear_button.click(lambda: ([], None, None, None, None), inputs=None, outputs=[chatbot, msg, url_input, file_upload, audio_output])

    download_button.click(
        fn=download_transcript,
        inputs=[history],
        outputs=[file_output]
    )

# Launch the app
demo.launch()