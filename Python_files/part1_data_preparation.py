# -*- coding: utf-8 -*-
"""Part1_Data_Preparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f9GPV-6Z79s8NG6BEfeP0XSEji-SAqqb

========================= READING THE DATA ==========================
"""

# ============================================
# STEP 1 : IMPORT LIBRARIES
# ============================================

import requests
from bs4 import BeautifulSoup
import os
import time


# ----------------------------------------------------------------------
# Define a function to read the youtube video urls from a local folder
# ----------------------------------------------------------------------

def read_local_urls_file(file_name):
    """
    Reads a text file containing URLs, where each URL is on a new line.
    Returns a list of URLs or None if the file is not found.
    """
    try:
        with open(file_name, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        print(f"Successfully read {len(urls)} URLs from '{file_name}'.")
        return urls
    except FileNotFoundError:
        print(f"Error: The file '{file_name}' was not found. Please make sure it's in the same directory as the script.")
        return None
    except Exception as e:
        print(f"An error occurred while reading the file '{file_name}': {e}")
        return None


# ----------------------------------------------------------------------
# Define a function to read the blogs and articles urls from a local folder
# ----------------------------------------------------------------------

def scrape_web_page(url):
    """
    Fetches the content of a web page related to AI and extracts the main text.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status() # Raise an exception for bad status codes

        soup = BeautifulSoup(response.content, 'html.parser')

        # Get the title of the article
        title = soup.title.string if soup.title else "Untitled"

        # Find all paragraph tags and combine their text
        paragraphs = soup.find_all('p')
        text = ' '.join([p.get_text() for p in paragraphs])

        # Basic cleaning: remove extra whitespace and newlines
        cleaned_text = ' '.join(text.split())

        return title, cleaned_text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching URL {url}: {e}")
        return None, None
    except Exception as e:
        print(f"Error parsing content from {url}: {e}")
        return None, None

# --- Main script execution ---

# Get URLs from youtube_urls.txt and process them.
youtube_urls = read_local_urls_file("youtube_urls.txt")
if youtube_urls:
    print(f"\nProcessing {len(youtube_urls)} YouTube URLs...")
    # Add your YouTube scraping logic here

# Get URLs from web_urls.txt and process them.
web_urls = read_local_urls_file("web_urls.txt")
if web_urls:
    # Create a directory to store the scraped content
    if not os.path.exists("articles"):
        os.makedirs("articles")

    articles_processed = 0

    for url in web_urls:
        print(f"Processing {url}...")
        title, content = scrape_web_page(url)
        if content:
            # Create a clean filename from the URL to save the content
            filename = url.replace("https://", "").replace("http://", "").replace("/", "_").replace(".", "_")
            output_filename = os.path.join("articles", f"{filename}.txt")

            with open(output_filename, "w", encoding="utf-8") as f:
                f.write(f"Title: {title}\n\n")
                f.write(content)

            articles_processed += 1
            print(f"Successfully scraped and saved to {output_filename}")

        # Be a good web citizen and add a small delay
        time.sleep(1)

    print("\n--- Process Complete ---")
    print(f"Successfully processed {articles_processed} articles.")
    print("The content is saved in the 'articles' directory.")

# ==============================================================
# Check if the youtube videos have the transcript available
# ==============================================================

from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import NoTranscriptFound, TranscriptsDisabled, VideoUnavailable

def has_transcript(video_id: str) -> bool:
    try:
        api = YouTubeTranscriptApi()
        api.fetch(video_id)
        return True
    except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable):
        return False

print(has_transcript("V5ych2rxtnQ"))
print(has_transcript("1MQ5ozIvgzE"))
print(has_transcript("tQ84XYcP-nA"))
print(has_transcript("S2TAa4P2IuY&t=347s"))
print(has_transcript("1MQ5ozIvgzE&t=171s"))
print(has_transcript("UKeCWtI_lfA"))  # Replace with any YouTube video ID

"""===================== FETCH THE TRANSCRIPTS FROM THE YOUTUBE VIDEOS ====================="""

import os
import re
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import NoTranscriptFound, TranscriptsDisabled, VideoUnavailable

# --- CONFIG ---
input_file = "youtube_urls.txt"   # Your file with one YouTube URL per line
output_folder = "YT_transcripts"   # Assign folder to save transcripts
os.makedirs(output_folder, exist_ok=True)


# -----------------------------------------
# --- FUNCTION: Extract Video ID ---
# -----------------------------------------

def extract_video_id(url: str) -> str:
    """
    Extract YouTube video ID from a URL (works with standard and shortened URLs)
    """
    match = re.search(r"v=([A-Za-z0-9_-]{11})", url)
    if match:
        return match.group(1)
    match = re.search(r"youtu\.be/([A-Za-z0-9_-]{11})", url)
    if match:
        return match.group(1)
    return None


# ---------------------------------------------------------
# --- FUNCTION: Fetch Transcript from the video ID ---
# ---------------------------------------------------------

def fetch_transcript(video_id: str):
    try:
        api = YouTubeTranscriptApi()
        transcript = api.fetch(video_id)
        # Combine text lines into a single string using dot notation
        full_text = " ".join([entry.text for entry in transcript])
        return full_text
    except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable):
        return None

# ---------------------------------------------------------
# --- SAVE the fetched youtube transcripts ---
# ---------------------------------------------------------

with open(input_file, "r") as f:
    urls = [line.strip() for line in f.readlines()]

for url in urls:
    video_id = extract_video_id(url)
    if not video_id:
        print(f"❌ Could not extract video ID from URL: {url}")
        continue

    print(f"Processing video ID: {video_id} ...")
    transcript_text = fetch_transcript(video_id)

    if transcript_text:
        output_path = os.path.join(output_folder, f"{video_id}.txt")
        with open(output_path, "w", encoding="utf-8") as out_file:
            out_file.write(transcript_text)
        print(f"✅ Transcript saved: {output_path}")
    else:
        print(f"⚠️ No transcript available for video: {video_id}")

#  ===================================
#  Load the youtube video transcripts
#  ===================================

import os

transcripts_folder = "YT_transcripts"
transcript_files = [f for f in os.listdir(transcripts_folder) if f.endswith(".txt")]

YT_transcripts_data = []

for file_name in transcript_files:
    file_path = os.path.join(transcripts_folder, file_name)
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
        YT_transcripts_data.append({
            "video_id": file_name.replace(".txt", ""),
            "text": text
        })

print(f"✅ Loaded {len(YT_transcripts_data)} transcripts.")
print("Example:", YT_transcripts_data[0]["video_id"], YT_transcripts_data[0]["text"][:500])

# ====================================
# chunk the youtube video transcripts
# ====================================


def chunk_text(text, max_length=500):  # max length define the length of each chunk
    """
    Split text into smaller chunks of approximately max_length words.
    """
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_length):
        chunks.append(" ".join(words[i:i+max_length]))
    return chunks

yt_chunks = []

for item in YT_transcripts_data:
    chunks = chunk_text(item["text"], max_length=500)  # you can adjust length
    for i, chunk in enumerate(chunks):
        yt_chunks.append({
            "video_id": item["video_id"],
            "chunk_index": i,
            "text": chunk
        })

print(f"✅ Total YouTube chunks created: {len(yt_chunks)}")
print("Example chunk:", yt_chunks[0])

#  ====================================================================================
# chunk the youtube video transcripts with max_length of 200 characters for each chunk
#  ====================================================================================


# def chunk_text(text, max_length=500):
#     """
#     Split text into smaller chunks of approximately max_length words.
#     """
#     words = text.split()
#     chunks = []
#     for i in range(0, len(words), max_length):
#         chunks.append(" ".join(words[i:i+max_length]))
#     return chunks

# yt_chunks = []

# for item in transcripts_data:
#     chunks = chunk_text(item["text"], max_length=200)  # you can adjust length
#     for i, chunk in enumerate(chunks):
#         yt_chunks.append({
#             "video_id": item["video_id"],
#             "chunk_index": i,
#             "text": chunk
#         })

# print(f"✅ Total YouTube chunks created: {len(yt_chunks)}")
# print("Example chunk:", yt_chunks[0])

#  =======================================
#  Generate embeddings for each YT chunk
#  =======================================

import os
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm
import time

# Load API key from .env
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def get_yt_embedding(text):
    # Ensure text isn’t too long
    if len(text) > 8000:  # token proxy via characters
        text = text[:8000]
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",    # OpenAI embedding model
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"❌ Embedding failed: {e}")
        return None

# Loop with a pause to respect rate limits
for chunk in tqdm(yt_chunks):
    emb = get_yt_embedding(chunk['text'])
    if emb:
        chunk['embedding'] = emb
    time.sleep(0.5)  # add small delay


print(f"✅ Embeddings generated for {len(yt_chunks)} chunks.")

#  ===========================================================
#  save generated YouTube embeddings locally as .json file)
#  ===========================================================

import json

# Example: storing locally as JSON
output_file = "embedded_yt_transcripts.json"   # Assign the file name to save the embedded data

with open(output_file, "w", encoding="utf-8") as f:
    json.dump(yt_chunks, f, ensure_ascii=False, indent=2)

print(f"✅ Embedded data saved to {output_file}")

"""======================= BLOGS AND ARTICLES ========================"""

#  ===================================
#  Load the blogs and articles
#  ===================================


articles_folder = "articles"  # your folder containing article .txt files
article_files = [f for f in os.listdir(articles_folder) if f.endswith(".txt")]

articles_data = []

for file_name in article_files:
    file_path = os.path.join(articles_folder, file_name)
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
        articles_data.append({
            "article_id": file_name.replace(".txt", ""),
            "text": text
        })

print(f"✅ Loaded {len(articles_data)} articles.")
print("Example:", articles_data[1]["article_id"], articles_data[0]["text"][:500])

#  ===================================
#  Chunk the blogs and articles
#  ===================================

def chunk_text(text, max_length=500):
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_length):
        chunks.append(" ".join(words[i:i+max_length]))
    return chunks

article_chunks = []

for item in articles_data:
    chunks = chunk_text(item["text"], max_length=500)  # adjust size if needed
    for i, chunk in enumerate(chunks):
        article_chunks.append({
            "article_id": item["article_id"],
            "chunk_index": i,
            "text": chunk
        })

print(f"✅ Total article chunks created: {len(article_chunks)}")
print("Example chunk:", article_chunks[0])

#  ===========================================
#  Generate embeddings for each article chunk
#  ===========================================

def get_art_embedding(text):

    if len(text) > 8000:    # Ensure text isn’t too long
        text = text[:8000]
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"❌ Embedding failed: {e}")
        return None

# Loop with a pause to respect rate limits
for chunk in tqdm(article_chunks):
    emb = get_art_embedding(chunk['text'])
    if emb:
        chunk['embedding'] = emb
    time.sleep(0.5)  # add small delay


# for i, chunk in enumerate(article_chunks):
#     chunk["embedding"] = embeddings[i]

print(f"✅ Embeddings generated for {len(article_chunks)} article chunks.")

#  ===========================================
#  Save the article embeddings
#  ===========================================

import json

output_file = "embedded_articles.json"     # Assign the file name to save the embedded article data

with open(output_file, "w", encoding="utf-8") as f:
    json.dump(article_chunks, f, ensure_ascii=False, indent=2)

print(f"✅ Embedded article data saved to {output_file}")

# Importing the Youtube chunks

import json

file_path_1 = "/content/drive/MyDrive/Ironhack_final_project/embedded_yt_transcripts.json" # CHANGE THE FILE PATH IF YOU ARE USING THE VS CODE

with open(file_path_1, "r") as f:
      yt_embedded_chunks = json.load(f)

print(f"✅ Loaded {len(yt_embedded_chunks)} youtube transcript chunks")
print(yt_embedded_chunks[0])  # preview first chunk



# Importing the article chunks

file_path_2 = "/content/drive/MyDrive/Ironhack_final_project/embedded_articles.json" # CHANGE THE FILE PATH IF YOU ARE USING THE VS CODE

with open(file_path_2, "r") as f:
      article_embedded_chunks = json.load(f)

print(f"✅ Loaded {len(article_embedded_chunks)} article chunks")
print(article_embedded_chunks[0])  # preview first chunk

import os
# =========================================================================
# Comnbining the both embedded chunks into one named 'all_embedded_chunks'
# =========================================================================

all_embedded_chunks = yt_embedded_chunks + article_embedded_chunks

print(f"Total chunks: {len(all_embedded_chunks)}")
print("Example chunk:", all_embedded_chunks[45])

# Save to JSON

all_embedded_file_path = "/content/drive/MyDrive/Ironhack_final_project/"   # assign the folder path to save as JSON file.

# make sure the folder exists
os.makedirs(all_embedded_file_path, exist_ok=True)

# full file path
file_path = os.path.join(all_embedded_file_path, "all_embedded_chunks.json")

# save as json
with open(file_path, "w", encoding="utf-8") as f:
    json.dump(all_embedded_chunks, f, ensure_ascii=False, indent=4)

print(f"Saved {len(all_embedded_chunks)} chunks to {file_path}")

print("✅Saved as all_embedded_chunks.json")